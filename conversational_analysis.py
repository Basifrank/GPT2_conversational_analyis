# -*- coding: utf-8 -*-
"""conversational_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z9USq-MDuaSvdSTMgFc_Y_xn2Rx2Y_1Q
"""

from google.colab import drive

drive.mount('/content/drive')

#importing libraries
import os
import json
import random
import re
import pandas as pd
from numpy import int64

#cloning git
!git clone https://github.com/tenoke/gpt-2

cd gpt-2

#installing necessary requirements
!pip3 install -r requirements.txt

# Run this cell and select the kaggle.json file downloaded
# from the Kaggle account settings page.
from google.colab import files
files.upload()

# Let's make sure the kaggle.json file is present.
!ls -lha kaggle.json

# Next, install the Kaggle API client.
!pip install -q kaggle

# The Kaggle API client expects this file to be in ~/.kaggle,
# so move it there.
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

# This permissions change avoids a warning on Kaggle tool startup.
!chmod 600 ~/.kaggle/kaggle.json
#downloading data
!kaggle datasets download -d rtatman/ubuntu-dialogue-corpus



!unzip ubuntu-dialogue-corpus.zip

#Reading data
FIRST_DATA = 'Ubuntu-dialogue-corpus/dialogueText.csv'
SECOND_DATA = 'Ubuntu-dialogue-corpus/dialogueText_196.csv'
THIRD_DATA = 'Ubuntu-dialogue-corpus/dialogueText_301.csv'

DF1 = pd.read_csv(FIRST_DATA, parse_dates=['date'], chunksize=1200000)
DF2 = pd.read_csv(SECOND_DATA, parse_dates=['date'], chunksize=1200000)
DF3 = pd.read_csv(THIRD_DATA, parse_dates=['date'], chunksize=1200000)





#downloading model
!python download_model.py 345M

!mkdir ubuntu-data ubuntu-npz

#Cleaning function
def clean(dataset, file_name):
  i = 1
  for data in dataset:    
    text_corpus = ''
    current = None
    for msg in data.itertuples():
      if msg.dialogueID != current:
        current = msg.dialogueID
        text_corpus += '\n\n'
      try: 
        text_corpus += f"({msg.date}) {msg._4}: {msg.text}\n"
      except KeyError:
        pass
    
    
    with open(f'ubuntu-data/{file_name}_{i}.txt', 'w') as f:
      f.write(text_corpus)  
    del(text_corpus)
    i += 1

#cleaning data
clean(DF1, 'cleaned_d1_df')
clean(DF2, 'cleaned_d2_df')
clean(DF3, 'cleaned_d3_df')

#encoding
!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d1_df_1.txt ubuntu-npz/cleaned_d1_df_1.txt.npz --model_name 345M
!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d2_df_1.txt ubuntu-npz/cleaned_d2_df_1.txt.npz --model_name 345M
!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d2_df_2.txt ubuntu-npz/cleaned_d2_df_2.txt.npz --model_name 345M
!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d2_df_3.txt ubuntu-npz/cleaned_d2_df_3.txt.npz --model_name 345M
!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d2_df_4.txt ubuntu-npz/cleaned_d2_df_4.txt.npz --model_name 345M
!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d2_df_5.txt ubuntu-npz/cleaned_d2_df_5.txt.npz --model_name 345M
!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d2_df_6.txt ubuntu-npz/cleaned_d2_df_6.txt.npz --model_name 345M
!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d2_df_7.txt ubuntu-npz/cleaned_d2_df_7.txt.npz --model_name 345M
!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d2_df_8.txt ubuntu-npz/cleaned_d2_df_8.txt.npz --model_name 345M
!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d3_df_1.txt ubuntu-npz/cleaned_d3_df_1.txt.npz --model_name 345M
!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d3_df_2.txt ubuntu-npz/cleaned_d3_df_2.txt.npz --model_name 345M
!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d3_df_3.txt ubuntu-npz/cleaned_d3_df_3.txt.npz --model_name 345M
!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d3_df_4.txt ubuntu-npz/cleaned_d3_df_4.txt.npz --model_name 345M
!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d3_df_5.txt ubuntu-npz/cleaned_d3_df_5.txt.npz --model_name 345M
!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d3_df_6.txt ubuntu-npz/cleaned_d3_df_6.txt.npz --model_name 345M
!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d3_df_7.txt ubuntu-npz/cleaned_d3_df_7.txt.npz --model_name 345M
!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d3_df_8.txt ubuntu-npz/cleaned_d3_df_8.txt.npz --model_name 345M
!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d3_df_9.txt ubuntu-npz/cleaned_d3_df_9.txt.npz --model_name 345M
!PYTHONPATH=src ./encode.py ubuntu-data/cleaned_d3_df_10.txt ubuntu-npz/cleaned_d3_df_10.txt.npz --model_name 345M

#Training data
!PYTHONPATH=src ./train.py --dataset ubuntu-npz/ --sample_every 250 --stop_after 1450 --model_name 345M

!PYTHONPATH=src ./train.py --dataset ubuntu-npz/ --sample_every 250 --learning_rate 0.0001 --stop_after 3501 --model_name 345M

!PYTHONPATH=src ./train.py --dataset ubuntu-npz/ --sample_every 250 --learning_rate 0.000001 --stop_after 5001 --model_name 345M

!PYTHONPATH=src ./train.py --dataset ubuntu-npz/ --sample_every=250 --learning_rate=0.01 --stop_after=1251 --model_name 345M

#copy the new trainned weights


!cp -r /content/gpt-2/checkpoint/run1/* /content/gpt-2/models/345M/







#generate conversations on its own 
#generate sample articles on its own base on the trained data.
!python3 src/generate_unconditional_samples.py --top_k 40 --temperature 0.95 --model_name 345M



#generate article on its own after finetuning model temperature and top_k
!python3 src/generate_unconditional_samples.py --top_k 40 --temperature 1 --model_name 345M



!python3 src/interactive_conditional_samples.py  --model_name='345M'  --nsamples=20 --top_k=40 --temperature=.90 --length=1

!python3 src/interactive_conditional_samples.py --model_name='345M' --nsamples=2 --top_k=40 --temperature=.90

!python3 src/interactive_conditional_samples.py --model_name='345M'  --nsamples=5 --temperature=1

#!python3 src/interactive_conditional_samples.py --model_name='345M'  --nsamples=5 --temperature=1

!python3 src/interactive_conditional_samples.py --model_name='345M'  --top_k 40 --temperature 0.9















