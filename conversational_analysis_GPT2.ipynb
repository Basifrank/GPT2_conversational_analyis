{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conversational_analysis_GPT2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Basifrank/GPT2_conversational_analyis/blob/master/conversational_analysis_GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0KgjdshTnN0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "022fd90c-967c-472a-ad15-580809324c40"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL6zr0ecTOde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "import pandas as pd\n",
        "from numpy import int64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhHvAM76TPWO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "12c64750-dc49-4571-e85b-c254fb67d705"
      },
      "source": [
        "#Cloning the gpt-2 model\n",
        "!git clone https://github.com/tenoke/gpt-2"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 244, done.\u001b[K\n",
            "remote: Total 244 (delta 0), reused 0 (delta 0), pack-reused 244\u001b[K\n",
            "Receiving objects: 100% (244/244), 4.39 MiB | 15.02 MiB/s, done.\n",
            "Resolving deltas: 100% (125/125), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1pw3VtnSloy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73190d58-c643-4354-be87-bd7699e29271"
      },
      "source": [
        "#changing the root directory\n",
        "cd gpt-2"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0ANW3MTTQTy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "outputId": "af1c2aa7-1576-4657-9e40-bc95d4bece3c"
      },
      "source": [
        "#Installing the required files in the requirements.txt file\n",
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fire>=0.1.3 (from -r requirements.txt (line 1))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/69/faeaae8687f4de0f5973694d02e9d6c3eb827636a009157352d98de1129e/fire-0.2.1.tar.gz (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 3.8MB/s \n",
            "\u001b[?25hCollecting regex==2017.4.5 (from -r requirements.txt (line 2))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 10.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.21.0)\n",
            "Collecting tqdm==4.31.1 (from -r requirements.txt (line 4))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 19.9MB/s \n",
            "\u001b[?25hCollecting toposort==1.5 (from -r requirements.txt (line 5))\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Building wheels for collected packages: fire, regex\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=103527 sha256=b18e55f055d2dedec584f5e3b1a8da75006b982db453e8241f377ff8c6f92d14\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/9c/c0/07b6dc7faf1844bb4688f46b569efe6cafaa2179c95db821da\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.4.5-cp36-cp36m-linux_x86_64.whl size=533178 sha256=a9db5e0ce3e9f7dc2a7b1010345d8c9f90ac6889b82725886fc057760b181c32\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built fire regex\n",
            "Installing collected packages: fire, regex, tqdm, toposort\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed fire-0.2.1 regex-2017.4.5 toposort-1.5 tqdm-4.31.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84u5A5oaUcJh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "1515008e-106b-40db-ce08-13574b5f703a"
      },
      "source": [
        "#downloading the model\n",
        "!python download_model.py 345M"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rFetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\rFetching checkpoint: 1.00kit [00:00, 542kit/s]                                                      \n",
            "\rFetching encoder.json:   0%|                                           | 0.00/1.04M [00:00<?, ?it/s]\rFetching encoder.json: 1.04Mit [00:00, 35.4Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 570kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:29, 48.1Mit/s]                                 \n",
            "Fetching model.ckpt.index: 11.0kit [00:00, 4.93Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 927kit [00:00, 32.9Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 34.0Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTwYp9JAUdg5",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "f2c920bd-a2e8-4386-d18d-b407b00336d4"
      },
      "source": [
        "#setting up the kaggle api so that we can access our ubuntu corpus dataset\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0d3c1a0f-0508-473c-9d7f-541aba1f490b\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-0d3c1a0f-0508-473c-9d7f-541aba1f490b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"basifrank\",\"key\":\"f5f3c7a9c1f89b369d085833c612e5f3\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmeZysIcUb3h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8b78f71a-6b97-4f6b-f2a5-245c97e3ea8a"
      },
      "source": [
        "!ls -lha kaggle.json"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 65 Oct 12 23:35 kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7GWuw87TQMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Installing the Kaggle API client\n",
        "!pip install -q kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aZzgnjjVd62",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
        "# so move it there.\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# This permissions change avoids a warning on Kaggle tool startup.\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqH9r69jVeoC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "7f3e309f-d04a-4551-d1b1-6ad9669626ce"
      },
      "source": [
        "!kaggle datasets download -d rtatman/ubuntu-dialogue-corpus"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading ubuntu-dialogue-corpus.zip to /content/gpt-2\n",
            " 99% 790M/799M [00:05<00:00, 143MB/s]\n",
            "100% 799M/799M [00:05<00:00, 146MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGqgaKF-VfRZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "3f63add7-22a3-46a0-e868-271a5f5212a7"
      },
      "source": [
        "!unzip ubuntu-dialogue-corpus.zip"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ubuntu-dialogue-corpus.zip\n",
            "  inflating: Ubuntu-dialogue-corpus/dialogueText.csv  \n",
            "  inflating: Ubuntu-dialogue-corpus/dialogueText_196.csv  \n",
            "  inflating: Ubuntu-dialogue-corpus/dialogueText_301.csv  \n",
            "  inflating: toc.csv                 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTUbcuq5VdyK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir ubuntu-data ubuntu-npz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ut0gfVkWGeF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datasets = pd.read_csv('Ubuntu-dialogue-corpus/dialogueText_196.csv', parse_dates=['date'], chunksize=1200000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndh3fESsWGS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = 1\n",
        "\n",
        "for dataset in datasets:\n",
        "  dataset['date'] = dataset['date'].astype(int64) // 10**9\n",
        "  \n",
        "  text_corpus = ''\n",
        "  current = None\n",
        "  for msg in dataset.itertuples():\n",
        "    if msg.dialogueID != current:\n",
        "      current = msg.dialogueID\n",
        "      text_corpus += '\\n\\n'\n",
        "    try:\n",
        "      text_corpus += f\"({msg.date}) {msg._4}: {msg.text}\\n\"\n",
        "    except KeyError:\n",
        "      pass\n",
        "  \n",
        "  with open(f'ubuntu-data/ubuntu-cleaned-{i}.txt', 'w') as f:\n",
        "    f.write(text_corpus)\n",
        "  del(text_corpus)\n",
        "  i +=1\n",
        "    \n",
        "del(datasets)\n",
        "del(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugZJXMTyWGJ4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "06461d8e-4169-4552-8543-c34f73b0460c"
      },
      "source": [
        "!du -h ubuntu-data/*"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "91M\tubuntu-data/ubuntu-cleaned-1.txt\n",
            "92M\tubuntu-data/ubuntu-cleaned-2.txt\n",
            "90M\tubuntu-data/ubuntu-cleaned-3.txt\n",
            "89M\tubuntu-data/ubuntu-cleaned-4.txt\n",
            "89M\tubuntu-data/ubuntu-cleaned-5.txt\n",
            "86M\tubuntu-data/ubuntu-cleaned-6.txt\n",
            "85M\tubuntu-data/ubuntu-cleaned-7.txt\n",
            "60M\tubuntu-data/ubuntu-cleaned-8.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwGJqLqiWGB9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "a1eeeab5-3e37-450c-d8ec-183deda403be"
      },
      "source": [
        "!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-1.txt ubuntu-npz/ubuntu-cleaned-1.txt.npz --model_name 345M\n",
        "!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-2.txt ubuntu-npz/ubuntu-cleaned-2.txt.npz --model_name 345M\n",
        "!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-3.txt ubuntu-npz/ubuntu-cleaned-3.txt.npz --model_name 345M\n",
        "!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-4.txt ubuntu-npz/ubuntu-cleaned-4.txt.npz --model_name 345M\n",
        "!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-5.txt ubuntu-npz/ubuntu-cleaned-5.txt.npz --model_name 345M\n",
        "!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-6.txt ubuntu-npz/ubuntu-cleaned-6.txt.npz --model_name 345M\n",
        "!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-7.txt ubuntu-npz/ubuntu-cleaned-7.txt.npz --model_name 345M\n",
        "!PYTHONPATH=src ./encode.py ubuntu-data/ubuntu-cleaned-8.txt ubuntu-npz/ubuntu-cleaned-8.txt.npz --model_name 345M"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading files\n",
            "100% 1/1 [03:27<00:00, 207.89s/it]\n",
            "Writing ubuntu-npz/ubuntu-cleaned-1.txt.npz\n",
            "Reading files\n",
            "100% 1/1 [03:33<00:00, 213.25s/it]\n",
            "Writing ubuntu-npz/ubuntu-cleaned-2.txt.npz\n",
            "Reading files\n",
            "100% 1/1 [03:25<00:00, 205.22s/it]\n",
            "Writing ubuntu-npz/ubuntu-cleaned-3.txt.npz\n",
            "Reading files\n",
            "100% 1/1 [03:37<00:00, 217.21s/it]\n",
            "Writing ubuntu-npz/ubuntu-cleaned-4.txt.npz\n",
            "Reading files\n",
            "100% 1/1 [03:17<00:00, 197.16s/it]\n",
            "Writing ubuntu-npz/ubuntu-cleaned-5.txt.npz\n",
            "Reading files\n",
            "100% 1/1 [03:26<00:00, 206.83s/it]\n",
            "Writing ubuntu-npz/ubuntu-cleaned-6.txt.npz\n",
            "Reading files\n",
            "100% 1/1 [03:04<00:00, 184.23s/it]\n",
            "Writing ubuntu-npz/ubuntu-cleaned-7.txt.npz\n",
            "Reading files\n",
            "100% 1/1 [02:08<00:00, 128.94s/it]\n",
            "Writing ubuntu-npz/ubuntu-cleaned-8.txt.npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znv17y5lWF3r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "ee3d62e9-a4ee-4319-a472-419128816a23"
      },
      "source": [
        "!du -h ubuntu-npz/*"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "41M\tubuntu-npz/ubuntu-cleaned-1.txt.npz\n",
            "46M\tubuntu-npz/ubuntu-cleaned-2.txt.npz\n",
            "41M\tubuntu-npz/ubuntu-cleaned-3.txt.npz\n",
            "41M\tubuntu-npz/ubuntu-cleaned-4.txt.npz\n",
            "39M\tubuntu-npz/ubuntu-cleaned-5.txt.npz\n",
            "39M\tubuntu-npz/ubuntu-cleaned-6.txt.npz\n",
            "37M\tubuntu-npz/ubuntu-cleaned-7.txt.npz\n",
            "27M\tubuntu-npz/ubuntu-cleaned-8.txt.npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4z9-DSlfVdrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r ubuntu-npz/ /content/drive/My\\ Drive/ubuntu-npz/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blHECfviW-84",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1eb4c3a3-7659-4f2a-ae5e-2bcccb9acd1c"
      },
      "source": [
        "!PYTHONPATH=src ./train.py --dataset ubuntu-npz/ --sample_every=250 --stop_after=1501 --model_name 345M"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:73: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:76: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-10-13 00:43:17.294896: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-10-13 00:43:17.295780: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1e55480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-10-13 00:43:17.295881: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-10-13 00:43:17.330776: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-10-13 00:43:17.500025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 00:43:17.501446: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7601180 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-10-13 00:43:17.501546: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-10-13 00:43:17.502703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 00:43:17.503845: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-10-13 00:43:17.513768: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-13 00:43:17.800587: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-10-13 00:43:17.941801: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-10-13 00:43:17.976181: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-10-13 00:43:18.284750: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-10-13 00:43:18.463542: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-10-13 00:43:19.015723: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-10-13 00:43:19.016048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 00:43:19.017150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 00:43:19.018200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-10-13 00:43:19.024771: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-13 00:43:19.029739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-10-13 00:43:19.029790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-10-13 00:43:19.029952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-10-13 00:43:19.031991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 00:43:19.033761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 00:43:19.034925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From ./train.py:77: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From ./train.py:100: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:113: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
            "Instructions for updating:\n",
            "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:89: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:120: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:122: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:125: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:129: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "Loading checkpoint models/345M/model.ckpt\n",
            "Loading dataset...\n",
            "100% 8/8 [00:09<00:00,  1.22s/it]\n",
            "dataset has 233929304 tokens\n",
            "Training...\n",
            "2019-10-13 00:44:42.404269: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "[1 | 18.77] loss=3.04 avg=3.04\n",
            "[2 | 21.78] loss=2.93 avg=2.99\n",
            "[3 | 24.79] loss=2.56 avg=2.84\n",
            "[4 | 27.79] loss=2.21 avg=2.68\n",
            "[5 | 30.76] loss=2.72 avg=2.69\n",
            "[6 | 33.72] loss=2.54 avg=2.67\n",
            "[7 | 36.69] loss=3.23 avg=2.75\n",
            "[8 | 39.67] loss=2.29 avg=2.69\n",
            "[9 | 42.63] loss=2.37 avg=2.65\n",
            "[10 | 45.61] loss=2.61 avg=2.65\n",
            "[11 | 48.58] loss=2.26 avg=2.61\n",
            "[12 | 51.57] loss=2.40 avg=2.59\n",
            "[13 | 54.54] loss=2.44 avg=2.58\n",
            "[14 | 57.51] loss=2.21 avg=2.55\n",
            "[15 | 60.51] loss=2.70 avg=2.56\n",
            "[16 | 63.48] loss=3.01 avg=2.59\n",
            "[17 | 66.46] loss=2.55 avg=2.59\n",
            "[18 | 69.45] loss=2.77 avg=2.60\n",
            "[19 | 72.42] loss=2.83 avg=2.61\n",
            "[20 | 75.41] loss=2.87 avg=2.63\n",
            "[21 | 78.42] loss=3.18 avg=2.66\n",
            "[22 | 81.40] loss=2.66 avg=2.66\n",
            "[23 | 84.41] loss=2.28 avg=2.64\n",
            "[24 | 87.40] loss=2.59 avg=2.64\n",
            "[25 | 90.39] loss=2.65 avg=2.64\n",
            "[26 | 93.38] loss=2.28 avg=2.62\n",
            "[27 | 96.38] loss=2.42 avg=2.61\n",
            "[28 | 99.39] loss=2.44 avg=2.61\n",
            "[29 | 102.39] loss=2.97 avg=2.62\n",
            "[30 | 105.37] loss=1.88 avg=2.59\n",
            "[31 | 108.37] loss=2.47 avg=2.59\n",
            "[32 | 111.37] loss=2.43 avg=2.58\n",
            "[33 | 114.36] loss=2.25 avg=2.57\n",
            "[34 | 117.37] loss=2.44 avg=2.57\n",
            "[35 | 120.37] loss=2.60 avg=2.57\n",
            "[36 | 123.37] loss=2.41 avg=2.56\n",
            "[37 | 126.36] loss=2.21 avg=2.55\n",
            "[38 | 129.36] loss=2.43 avg=2.55\n",
            "[39 | 132.36] loss=2.26 avg=2.54\n",
            "[40 | 135.37] loss=2.58 avg=2.54\n",
            "[41 | 138.36] loss=2.53 avg=2.54\n",
            "[42 | 141.36] loss=2.73 avg=2.54\n",
            "[43 | 144.35] loss=2.51 avg=2.54\n",
            "[44 | 147.34] loss=2.34 avg=2.54\n",
            "[45 | 150.35] loss=2.07 avg=2.52\n",
            "[46 | 153.36] loss=2.28 avg=2.52\n",
            "[47 | 156.36] loss=3.22 avg=2.54\n",
            "[48 | 159.37] loss=2.54 avg=2.54\n",
            "[49 | 162.36] loss=2.29 avg=2.53\n",
            "[50 | 165.35] loss=2.25 avg=2.52\n",
            "[51 | 168.35] loss=3.11 avg=2.54\n",
            "[52 | 171.35] loss=1.89 avg=2.52\n",
            "[53 | 174.35] loss=2.59 avg=2.52\n",
            "[54 | 177.35] loss=2.81 avg=2.53\n",
            "[55 | 180.33] loss=2.48 avg=2.53\n",
            "[56 | 183.32] loss=2.42 avg=2.53\n",
            "[57 | 186.32] loss=2.49 avg=2.53\n",
            "[58 | 189.36] loss=2.86 avg=2.53\n",
            "[59 | 192.40] loss=2.29 avg=2.53\n",
            "[60 | 195.45] loss=2.66 avg=2.53\n",
            "[61 | 198.51] loss=2.25 avg=2.53\n",
            "[62 | 201.52] loss=2.85 avg=2.53\n",
            "[63 | 204.58] loss=3.06 avg=2.54\n",
            "[64 | 207.61] loss=2.36 avg=2.54\n",
            "[65 | 210.66] loss=2.65 avg=2.54\n",
            "[66 | 213.72] loss=2.33 avg=2.54\n",
            "[67 | 216.74] loss=2.23 avg=2.53\n",
            "[68 | 219.80] loss=2.93 avg=2.54\n",
            "[69 | 222.84] loss=2.24 avg=2.53\n",
            "[70 | 225.89] loss=2.10 avg=2.52\n",
            "[71 | 228.93] loss=2.40 avg=2.52\n",
            "[72 | 231.95] loss=2.18 avg=2.52\n",
            "[73 | 234.98] loss=2.26 avg=2.51\n",
            "[74 | 238.03] loss=2.62 avg=2.51\n",
            "[75 | 241.06] loss=2.30 avg=2.51\n",
            "[76 | 244.09] loss=2.92 avg=2.52\n",
            "[77 | 247.14] loss=2.71 avg=2.52\n",
            "[78 | 250.18] loss=2.24 avg=2.52\n",
            "[79 | 253.24] loss=2.76 avg=2.52\n",
            "[80 | 256.29] loss=2.33 avg=2.52\n",
            "[81 | 259.34] loss=2.65 avg=2.52\n",
            "[82 | 262.39] loss=2.49 avg=2.52\n",
            "[83 | 265.43] loss=2.96 avg=2.53\n",
            "[84 | 268.47] loss=2.38 avg=2.52\n",
            "[85 | 271.55] loss=2.39 avg=2.52\n",
            "[86 | 274.55] loss=2.23 avg=2.52\n",
            "[87 | 277.54] loss=2.49 avg=2.52\n",
            "[88 | 280.54] loss=2.49 avg=2.52\n",
            "[89 | 283.54] loss=2.25 avg=2.51\n",
            "[90 | 286.54] loss=2.44 avg=2.51\n",
            "[91 | 289.53] loss=2.77 avg=2.51\n",
            "[92 | 292.52] loss=2.80 avg=2.52\n",
            "[93 | 295.53] loss=2.72 avg=2.52\n",
            "[94 | 298.52] loss=2.32 avg=2.52\n",
            "[95 | 301.51] loss=2.37 avg=2.52\n",
            "[96 | 304.50] loss=2.50 avg=2.52\n",
            "[97 | 307.51] loss=2.52 avg=2.52\n",
            "[98 | 310.52] loss=2.16 avg=2.51\n",
            "[99 | 313.51] loss=2.53 avg=2.51\n",
            "[100 | 316.51] loss=2.91 avg=2.52\n",
            "[101 | 319.52] loss=2.59 avg=2.52\n",
            "[102 | 322.52] loss=3.09 avg=2.53\n",
            "[103 | 325.53] loss=2.58 avg=2.53\n",
            "[104 | 328.52] loss=3.08 avg=2.54\n",
            "[105 | 331.52] loss=2.76 avg=2.54\n",
            "[106 | 334.52] loss=2.09 avg=2.53\n",
            "[107 | 337.51] loss=2.06 avg=2.53\n",
            "[108 | 340.52] loss=2.73 avg=2.53\n",
            "[109 | 343.52] loss=2.77 avg=2.53\n",
            "[110 | 346.50] loss=2.37 avg=2.53\n",
            "[111 | 349.50] loss=2.54 avg=2.53\n",
            "[112 | 352.49] loss=2.96 avg=2.54\n",
            "[113 | 355.49] loss=2.64 avg=2.54\n",
            "[114 | 358.50] loss=2.31 avg=2.53\n",
            "[115 | 361.49] loss=1.89 avg=2.53\n",
            "[116 | 364.49] loss=2.53 avg=2.53\n",
            "[117 | 367.49] loss=2.36 avg=2.52\n",
            "[118 | 370.49] loss=2.28 avg=2.52\n",
            "[119 | 373.49] loss=1.94 avg=2.51\n",
            "[120 | 376.48] loss=2.32 avg=2.51\n",
            "[121 | 379.47] loss=2.41 avg=2.51\n",
            "[122 | 382.47] loss=2.38 avg=2.51\n",
            "[123 | 385.47] loss=2.32 avg=2.50\n",
            "[124 | 388.48] loss=2.86 avg=2.51\n",
            "[125 | 391.47] loss=2.41 avg=2.51\n",
            "[126 | 394.47] loss=2.43 avg=2.51\n",
            "[127 | 397.46] loss=2.31 avg=2.50\n",
            "[128 | 400.45] loss=3.06 avg=2.51\n",
            "[129 | 403.44] loss=3.22 avg=2.52\n",
            "[130 | 406.44] loss=2.67 avg=2.52\n",
            "[131 | 409.45] loss=2.18 avg=2.52\n",
            "[132 | 412.44] loss=2.71 avg=2.52\n",
            "[133 | 415.44] loss=2.79 avg=2.52\n",
            "[134 | 418.43] loss=2.08 avg=2.52\n",
            "[135 | 421.43] loss=2.45 avg=2.52\n",
            "[136 | 424.42] loss=1.97 avg=2.51\n",
            "[137 | 427.42] loss=2.54 avg=2.51\n",
            "[138 | 430.41] loss=1.99 avg=2.50\n",
            "[139 | 433.40] loss=2.87 avg=2.51\n",
            "[140 | 436.39] loss=3.17 avg=2.52\n",
            "[141 | 439.38] loss=2.03 avg=2.51\n",
            "[142 | 442.39] loss=2.54 avg=2.51\n",
            "[143 | 445.38] loss=2.56 avg=2.51\n",
            "[144 | 448.37] loss=2.04 avg=2.50\n",
            "[145 | 451.36] loss=2.13 avg=2.50\n",
            "[146 | 454.36] loss=2.64 avg=2.50\n",
            "[147 | 457.35] loss=2.55 avg=2.50\n",
            "[148 | 460.35] loss=2.53 avg=2.50\n",
            "[149 | 463.35] loss=2.81 avg=2.51\n",
            "[150 | 466.36] loss=2.61 avg=2.51\n",
            "[151 | 469.35] loss=2.53 avg=2.51\n",
            "[152 | 472.35] loss=2.17 avg=2.50\n",
            "[153 | 475.35] loss=2.41 avg=2.50\n",
            "[154 | 478.34] loss=2.15 avg=2.50\n",
            "[155 | 481.34] loss=2.39 avg=2.50\n",
            "[156 | 484.34] loss=2.69 avg=2.50\n",
            "[157 | 487.33] loss=2.51 avg=2.50\n",
            "[158 | 490.33] loss=2.47 avg=2.50\n",
            "[159 | 493.32] loss=2.33 avg=2.50\n",
            "[160 | 496.31] loss=2.43 avg=2.50\n",
            "[161 | 499.30] loss=2.85 avg=2.50\n",
            "[162 | 502.31] loss=2.66 avg=2.50\n",
            "[163 | 505.32] loss=2.92 avg=2.51\n",
            "[164 | 508.32] loss=2.62 avg=2.51\n",
            "[165 | 511.31] loss=2.19 avg=2.51\n",
            "[166 | 514.30] loss=2.76 avg=2.51\n",
            "[167 | 517.30] loss=2.35 avg=2.51\n",
            "[168 | 520.30] loss=2.13 avg=2.50\n",
            "[169 | 523.30] loss=2.63 avg=2.50\n",
            "[170 | 526.29] loss=2.87 avg=2.51\n",
            "[171 | 529.28] loss=2.18 avg=2.50\n",
            "[172 | 532.27] loss=3.18 avg=2.51\n",
            "[173 | 535.27] loss=2.33 avg=2.51\n",
            "[174 | 538.28] loss=2.29 avg=2.51\n",
            "[175 | 541.27] loss=2.14 avg=2.50\n",
            "[176 | 544.27] loss=2.34 avg=2.50\n",
            "[177 | 547.26] loss=2.53 avg=2.50\n",
            "[178 | 550.26] loss=2.45 avg=2.50\n",
            "[179 | 553.25] loss=2.03 avg=2.50\n",
            "[180 | 556.24] loss=2.36 avg=2.49\n",
            "[181 | 559.24] loss=2.41 avg=2.49\n",
            "[182 | 562.24] loss=2.26 avg=2.49\n",
            "[183 | 565.23] loss=2.36 avg=2.49\n",
            "[184 | 568.24] loss=2.60 avg=2.49\n",
            "[185 | 571.24] loss=2.65 avg=2.49\n",
            "[186 | 574.23] loss=2.65 avg=2.49\n",
            "[187 | 577.23] loss=2.46 avg=2.49\n",
            "[188 | 580.22] loss=2.32 avg=2.49\n",
            "[189 | 583.23] loss=3.08 avg=2.50\n",
            "[190 | 586.23] loss=3.05 avg=2.50\n",
            "[191 | 589.22] loss=2.37 avg=2.50\n",
            "[192 | 592.22] loss=2.65 avg=2.50\n",
            "[193 | 595.22] loss=2.82 avg=2.51\n",
            "[194 | 598.22] loss=2.24 avg=2.50\n",
            "[195 | 601.21] loss=2.74 avg=2.51\n",
            "[196 | 604.20] loss=1.90 avg=2.50\n",
            "[197 | 607.20] loss=2.48 avg=2.50\n",
            "[198 | 610.20] loss=2.58 avg=2.50\n",
            "[199 | 613.20] loss=2.71 avg=2.50\n",
            "[200 | 616.20] loss=2.17 avg=2.50\n",
            "[201 | 619.20] loss=2.05 avg=2.49\n",
            "[202 | 622.20] loss=2.46 avg=2.49\n",
            "[203 | 625.20] loss=2.09 avg=2.49\n",
            "[204 | 628.19] loss=2.35 avg=2.49\n",
            "[205 | 631.19] loss=2.47 avg=2.49\n",
            "[206 | 634.19] loss=2.86 avg=2.49\n",
            "[207 | 637.18] loss=2.13 avg=2.49\n",
            "[208 | 640.18] loss=2.74 avg=2.49\n",
            "[209 | 643.18] loss=2.23 avg=2.49\n",
            "[210 | 646.17] loss=2.58 avg=2.49\n",
            "[211 | 649.16] loss=2.42 avg=2.49\n",
            "[212 | 652.16] loss=2.47 avg=2.49\n",
            "[213 | 655.16] loss=2.40 avg=2.49\n",
            "[214 | 658.16] loss=2.76 avg=2.49\n",
            "[215 | 661.15] loss=3.00 avg=2.50\n",
            "[216 | 664.16] loss=2.46 avg=2.50\n",
            "[217 | 667.17] loss=2.06 avg=2.49\n",
            "[218 | 670.16] loss=2.66 avg=2.49\n",
            "[219 | 673.16] loss=2.52 avg=2.49\n",
            "[220 | 676.15] loss=3.02 avg=2.50\n",
            "[221 | 679.14] loss=2.77 avg=2.50\n",
            "[222 | 682.14] loss=2.48 avg=2.50\n",
            "[223 | 685.15] loss=2.62 avg=2.50\n",
            "[224 | 688.16] loss=2.44 avg=2.50\n",
            "[225 | 691.14] loss=2.24 avg=2.50\n",
            "[226 | 694.13] loss=3.05 avg=2.51\n",
            "[227 | 697.13] loss=2.72 avg=2.51\n",
            "[228 | 700.12] loss=2.47 avg=2.51\n",
            "[229 | 703.12] loss=2.39 avg=2.51\n",
            "[230 | 706.11] loss=2.87 avg=2.51\n",
            "[231 | 709.11] loss=2.41 avg=2.51\n",
            "[232 | 712.11] loss=2.45 avg=2.51\n",
            "[233 | 715.11] loss=2.28 avg=2.51\n",
            "[234 | 718.11] loss=2.58 avg=2.51\n",
            "[235 | 721.12] loss=2.41 avg=2.51\n",
            "[236 | 724.12] loss=2.37 avg=2.50\n",
            "[237 | 727.11] loss=2.98 avg=2.51\n",
            "[238 | 730.10] loss=2.57 avg=2.51\n",
            "[239 | 733.09] loss=2.44 avg=2.51\n",
            "[240 | 736.09] loss=2.23 avg=2.51\n",
            "[241 | 739.09] loss=3.04 avg=2.51\n",
            "[242 | 742.10] loss=2.19 avg=2.51\n",
            "[243 | 745.11] loss=2.21 avg=2.51\n",
            "[244 | 748.10] loss=2.63 avg=2.51\n",
            "[245 | 751.09] loss=2.54 avg=2.51\n",
            "[246 | 754.09] loss=2.10 avg=2.50\n",
            "[247 | 757.09] loss=2.70 avg=2.50\n",
            "[248 | 760.08] loss=2.29 avg=2.50\n",
            "[249 | 763.10] loss=2.47 avg=2.50\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "ue/toy/x-system-console-and-all\n",
            "(11355920) tbakari716: hey, do you know if the old-style terminal is still supported by ubuntu without any special modifications or a GUI? or are I wrong about not getting it?\n",
            "(113565320) tbakari716: i was just wondering if there is a way to edit the root partition in recovery and make it bootable again after this wipe is put in place (that way it can be bootable again and start the console at boot time if it needs to)\n",
            "(1142917300) tbakari716: i think it's ok in the console\n",
            "(1142918160) tbakari716: i don't think there is any need to make a new bootable disk\n",
            "(1142919440) tbakari716: you can either mount it in a terminal and boot in terminal, but it may be too much hassle\n",
            "(1142990140) tbakari716: i'm not going to do anything with it.\n",
            "(1449937000) tbakari716: you can go ahead and do a bootloader (if you can get your system running with an old bootloader) and make it bootable from recovery and put the console in /boot\n",
            "(1459935840) tbakari716: i was thinking of having my system in recovery and setting it up for my desktop setup.\n",
            "(1499391260) tbakari716: i will try that but i don't know if i will have the problem with this\n",
            "\n",
            "\n",
            "(1140488860) jg1: i'm looking for a solution that'll show this error message:\n",
            "\n",
            "\n",
            "(1234171340) w3c0n_: yes\n",
            "(1234172700) w3c0n_: i mean  the problem of finding programs you need, but not the ones you want\n",
            "(1234172820) w3c0n_: this is not how windows works\n",
            "(1234172820) w3c0n_: its a bit crazy,\n",
            "(1234172880) w3c0n_: yes i believe the main thing is not installing anything, instead it just asks for root access to the system\n",
            "(1234173040) w3c0n_: yes, a very little bit crazy\n",
            "(1234171340) w3c0n_: yes, i am the root user\n",
            "(1234171400) w3c0n_: you need to be root\n",
            "(1234172100) w3c0n_: install it\n",
            "(1234171740) srs\n",
            "(1234171740) w3c0n_: yes, i am the root user\n",
            "(1234171860) w3c0n_: thats a problem with booting a system, why it not\n",
            "(1234171900) w3c0n_: i see\n",
            "(1234171900) w3c0n_: yes i saw this in the last post\n",
            "\n",
            "\n",
            "(1227481740) j_: hey guys\n",
            "(1227481820) s3m\n",
            "(1227481950) s3m: just installed ubuntu on my computer and it still won't boot.\n",
            "(1227482600) s3m: i ran the installer and it says i need root privileges to boot it\n",
            "(1227482880) s3m: yes i think my only option is to do it from inside the computer.\n",
            "(1227483400) s3m: i guess i got my system on ubuntu to boot but it still won't boot, its almost like its a computer or it is just the boot\n",
            "(1227484560) s3m: yes that's it\n",
            "(1227484620) s3m: ok thanks\n",
            "(1227484620) d4o3j: You could try opening a terminal with administrator credentials and typing 'sudo'\n",
            "(1227484680) d4o3j: it takes awhile to get accustomed to typing sudo in linux\n",
            "(1227748140) d4o3j: the next option, if you don't need to type it, is 'no'\n",
            "(1227748200) d4o3j: you could not type that in linux\n",
            "(1227748320) vivy: if your root password is really easy to remember, then you are safe.\n",
            "(1227748380) d4o3j: no i said it's not really 'free'\n",
            "(1227748380) vivy: you're not using ubuntu at\n",
            "\n",
            "[250 | 806.71] loss=2.39 avg=2.50\n",
            "[251 | 809.68] loss=2.18 avg=2.50\n",
            "[252 | 812.67] loss=2.27 avg=2.49\n",
            "[253 | 815.65] loss=2.47 avg=2.49\n",
            "[254 | 818.64] loss=2.17 avg=2.49\n",
            "[255 | 821.62] loss=2.50 avg=2.49\n",
            "[256 | 824.60] loss=2.32 avg=2.49\n",
            "[257 | 827.59] loss=2.06 avg=2.48\n",
            "[258 | 830.57] loss=2.92 avg=2.49\n",
            "[259 | 833.57] loss=2.19 avg=2.49\n",
            "[260 | 836.56] loss=2.74 avg=2.49\n",
            "[261 | 839.55] loss=2.78 avg=2.49\n",
            "[262 | 842.53] loss=2.24 avg=2.49\n",
            "[263 | 845.51] loss=2.03 avg=2.48\n",
            "[264 | 848.50] loss=1.94 avg=2.48\n",
            "[265 | 851.48] loss=2.67 avg=2.48\n",
            "[266 | 854.47] loss=2.34 avg=2.48\n",
            "[267 | 857.47] loss=2.10 avg=2.47\n",
            "[268 | 860.46] loss=2.43 avg=2.47\n",
            "[269 | 863.46] loss=2.30 avg=2.47\n",
            "[270 | 866.45] loss=2.40 avg=2.47\n",
            "[271 | 869.43] loss=2.97 avg=2.48\n",
            "[272 | 872.43] loss=2.55 avg=2.48\n",
            "[273 | 875.42] loss=2.16 avg=2.47\n",
            "[274 | 878.41] loss=2.40 avg=2.47\n",
            "[275 | 881.40] loss=2.91 avg=2.48\n",
            "[276 | 884.39] loss=2.77 avg=2.48\n",
            "[277 | 887.39] loss=2.12 avg=2.48\n",
            "[278 | 890.39] loss=2.32 avg=2.48\n",
            "[279 | 893.40] loss=2.60 avg=2.48\n",
            "[280 | 896.40] loss=3.01 avg=2.48\n",
            "[281 | 899.38] loss=2.45 avg=2.48\n",
            "[282 | 902.39] loss=2.24 avg=2.48\n",
            "[283 | 905.38] loss=2.23 avg=2.48\n",
            "[284 | 908.38] loss=2.39 avg=2.48\n",
            "[285 | 911.38] loss=2.27 avg=2.47\n",
            "[286 | 914.36] loss=2.76 avg=2.48\n",
            "[287 | 917.37] loss=2.02 avg=2.47\n",
            "[288 | 920.36] loss=2.44 avg=2.47\n",
            "[289 | 923.36] loss=2.81 avg=2.48\n",
            "[290 | 926.36] loss=2.33 avg=2.47\n",
            "[291 | 929.34] loss=2.53 avg=2.47\n",
            "[292 | 932.34] loss=2.53 avg=2.48\n",
            "[293 | 935.35] loss=2.14 avg=2.47\n",
            "[294 | 938.35] loss=3.02 avg=2.48\n",
            "[295 | 941.34] loss=2.43 avg=2.48\n",
            "[296 | 944.35] loss=2.37 avg=2.48\n",
            "[297 | 947.35] loss=1.93 avg=2.47\n",
            "[298 | 950.33] loss=1.98 avg=2.46\n",
            "[299 | 953.33] loss=3.03 avg=2.47\n",
            "[300 | 956.32] loss=2.05 avg=2.47\n",
            "[301 | 959.32] loss=2.70 avg=2.47\n",
            "[302 | 962.30] loss=2.77 avg=2.47\n",
            "[303 | 965.29] loss=3.10 avg=2.48\n",
            "[304 | 968.30] loss=2.33 avg=2.48\n",
            "[305 | 971.29] loss=2.49 avg=2.48\n",
            "[306 | 974.29] loss=2.18 avg=2.47\n",
            "[307 | 977.29] loss=2.10 avg=2.47\n",
            "[308 | 980.28] loss=2.42 avg=2.47\n",
            "[309 | 983.29] loss=2.12 avg=2.47\n",
            "[310 | 986.28] loss=2.72 avg=2.47\n",
            "[311 | 989.28] loss=2.74 avg=2.47\n",
            "[312 | 992.27] loss=2.38 avg=2.47\n",
            "[313 | 995.26] loss=2.45 avg=2.47\n",
            "[314 | 998.25] loss=2.77 avg=2.47\n",
            "[315 | 1001.25] loss=3.03 avg=2.48\n",
            "[316 | 1004.24] loss=2.63 avg=2.48\n",
            "[317 | 1007.23] loss=2.68 avg=2.48\n",
            "[318 | 1010.21] loss=2.28 avg=2.48\n",
            "[319 | 1013.20] loss=2.24 avg=2.48\n",
            "[320 | 1016.18] loss=2.30 avg=2.48\n",
            "[321 | 1019.16] loss=2.65 avg=2.48\n",
            "[322 | 1022.16] loss=2.14 avg=2.47\n",
            "[323 | 1025.15] loss=2.49 avg=2.47\n",
            "[324 | 1028.15] loss=2.44 avg=2.47\n",
            "[325 | 1031.14] loss=2.52 avg=2.48\n",
            "[326 | 1034.14] loss=2.45 avg=2.47\n",
            "[327 | 1037.13] loss=2.22 avg=2.47\n",
            "[328 | 1040.13] loss=2.21 avg=2.47\n",
            "[329 | 1043.12] loss=2.43 avg=2.47\n",
            "[330 | 1046.11] loss=2.54 avg=2.47\n",
            "[331 | 1049.09] loss=2.65 avg=2.47\n",
            "[332 | 1052.09] loss=2.43 avg=2.47\n",
            "[333 | 1055.08] loss=2.29 avg=2.47\n",
            "[334 | 1058.09] loss=2.30 avg=2.47\n",
            "[335 | 1061.08] loss=2.46 avg=2.47\n",
            "[336 | 1064.06] loss=2.62 avg=2.47\n",
            "[337 | 1067.07] loss=2.50 avg=2.47\n",
            "[338 | 1070.06] loss=2.90 avg=2.47\n",
            "[339 | 1073.05] loss=2.26 avg=2.47\n",
            "[340 | 1076.03] loss=2.64 avg=2.47\n",
            "[341 | 1079.02] loss=2.94 avg=2.48\n",
            "[342 | 1082.02] loss=2.61 avg=2.48\n",
            "[343 | 1085.02] loss=2.31 avg=2.48\n",
            "[344 | 1088.02] loss=2.09 avg=2.47\n",
            "[345 | 1091.02] loss=2.51 avg=2.47\n",
            "[346 | 1094.01] loss=2.84 avg=2.48\n",
            "[347 | 1096.99] loss=2.26 avg=2.48\n",
            "[348 | 1099.99] loss=2.27 avg=2.47\n",
            "[349 | 1102.99] loss=2.92 avg=2.48\n",
            "[350 | 1105.98] loss=2.52 avg=2.48\n",
            "[351 | 1108.97] loss=2.40 avg=2.48\n",
            "[352 | 1111.98] loss=2.76 avg=2.48\n",
            "[353 | 1115.00] loss=2.27 avg=2.48\n",
            "[354 | 1118.01] loss=2.48 avg=2.48\n",
            "[355 | 1121.02] loss=2.31 avg=2.48\n",
            "[356 | 1124.03] loss=2.36 avg=2.48\n",
            "[357 | 1127.06] loss=2.35 avg=2.47\n",
            "[358 | 1130.07] loss=2.21 avg=2.47\n",
            "[359 | 1133.08] loss=2.42 avg=2.47\n",
            "[360 | 1136.10] loss=2.55 avg=2.47\n",
            "[361 | 1139.12] loss=2.27 avg=2.47\n",
            "[362 | 1142.14] loss=2.41 avg=2.47\n",
            "[363 | 1145.15] loss=2.80 avg=2.47\n",
            "[364 | 1148.17] loss=2.19 avg=2.47\n",
            "[365 | 1151.19] loss=2.93 avg=2.47\n",
            "[366 | 1154.20] loss=2.49 avg=2.47\n",
            "[367 | 1157.21] loss=2.79 avg=2.48\n",
            "[368 | 1160.22] loss=1.93 avg=2.47\n",
            "[369 | 1163.24] loss=2.58 avg=2.47\n",
            "[370 | 1166.25] loss=2.50 avg=2.47\n",
            "[371 | 1169.27] loss=2.65 avg=2.48\n",
            "[372 | 1172.28] loss=2.29 avg=2.47\n",
            "[373 | 1175.30] loss=2.71 avg=2.48\n",
            "[374 | 1178.34] loss=2.22 avg=2.47\n",
            "[375 | 1181.36] loss=2.78 avg=2.48\n",
            "[376 | 1184.38] loss=2.17 avg=2.47\n",
            "[377 | 1187.40] loss=2.68 avg=2.48\n",
            "[378 | 1190.42] loss=2.80 avg=2.48\n",
            "[379 | 1193.45] loss=2.43 avg=2.48\n",
            "[380 | 1196.47] loss=2.56 avg=2.48\n",
            "[381 | 1199.50] loss=2.28 avg=2.48\n",
            "[382 | 1202.52] loss=2.68 avg=2.48\n",
            "[383 | 1205.54] loss=2.84 avg=2.48\n",
            "[384 | 1208.57] loss=2.93 avg=2.49\n",
            "[385 | 1211.59] loss=2.24 avg=2.48\n",
            "[386 | 1214.62] loss=2.66 avg=2.49\n",
            "[387 | 1217.63] loss=2.13 avg=2.48\n",
            "[388 | 1220.65] loss=2.49 avg=2.48\n",
            "[389 | 1223.67] loss=2.54 avg=2.48\n",
            "[390 | 1226.70] loss=2.32 avg=2.48\n",
            "[391 | 1229.72] loss=2.32 avg=2.48\n",
            "[392 | 1232.75] loss=2.19 avg=2.48\n",
            "[393 | 1235.77] loss=2.19 avg=2.47\n",
            "[394 | 1238.79] loss=2.46 avg=2.47\n",
            "[395 | 1241.81] loss=2.45 avg=2.47\n",
            "[396 | 1244.83] loss=2.48 avg=2.47\n",
            "[397 | 1247.84] loss=2.61 avg=2.48\n",
            "[398 | 1250.86] loss=2.78 avg=2.48\n",
            "[399 | 1253.88] loss=2.37 avg=2.48\n",
            "[400 | 1256.91] loss=2.27 avg=2.48\n",
            "[401 | 1259.94] loss=2.55 avg=2.48\n",
            "[402 | 1262.96] loss=2.13 avg=2.47\n",
            "[403 | 1265.99] loss=2.97 avg=2.48\n",
            "[404 | 1269.02] loss=2.27 avg=2.48\n",
            "[405 | 1272.05] loss=2.55 avg=2.48\n",
            "[406 | 1275.06] loss=2.42 avg=2.48\n",
            "[407 | 1278.08] loss=1.97 avg=2.47\n",
            "[408 | 1281.10] loss=2.35 avg=2.47\n",
            "[409 | 1284.11] loss=2.47 avg=2.47\n",
            "[410 | 1287.14] loss=2.15 avg=2.47\n",
            "[411 | 1290.17] loss=2.45 avg=2.47\n",
            "[412 | 1293.19] loss=2.24 avg=2.46\n",
            "[413 | 1296.22] loss=2.00 avg=2.46\n",
            "[414 | 1299.23] loss=2.28 avg=2.46\n",
            "[415 | 1302.25] loss=2.20 avg=2.45\n",
            "[416 | 1305.28] loss=2.67 avg=2.46\n",
            "[417 | 1308.30] loss=2.50 avg=2.46\n",
            "[418 | 1311.34] loss=2.11 avg=2.45\n",
            "[419 | 1314.36] loss=2.90 avg=2.46\n",
            "[420 | 1317.37] loss=2.06 avg=2.45\n",
            "[421 | 1320.39] loss=2.80 avg=2.46\n",
            "[422 | 1323.41] loss=2.65 avg=2.46\n",
            "[423 | 1326.43] loss=2.15 avg=2.46\n",
            "[424 | 1329.44] loss=2.15 avg=2.45\n",
            "[425 | 1332.45] loss=2.36 avg=2.45\n",
            "[426 | 1335.46] loss=2.23 avg=2.45\n",
            "[427 | 1338.47] loss=2.20 avg=2.45\n",
            "[428 | 1341.49] loss=1.81 avg=2.44\n",
            "[429 | 1344.51] loss=1.94 avg=2.44\n",
            "[430 | 1347.53] loss=2.90 avg=2.44\n",
            "[431 | 1350.54] loss=2.95 avg=2.45\n",
            "[432 | 1353.55] loss=2.41 avg=2.45\n",
            "[433 | 1356.57] loss=2.32 avg=2.44\n",
            "[434 | 1359.58] loss=2.18 avg=2.44\n",
            "[435 | 1362.60] loss=2.31 avg=2.44\n",
            "[436 | 1365.64] loss=2.07 avg=2.44\n",
            "[437 | 1368.66] loss=2.09 avg=2.43\n",
            "[438 | 1371.68] loss=2.80 avg=2.44\n",
            "[439 | 1374.72] loss=2.88 avg=2.44\n",
            "[440 | 1377.74] loss=2.93 avg=2.45\n",
            "[441 | 1380.76] loss=2.86 avg=2.45\n",
            "[442 | 1383.78] loss=2.25 avg=2.45\n",
            "[443 | 1386.79] loss=2.79 avg=2.45\n",
            "[444 | 1389.80] loss=2.29 avg=2.45\n",
            "[445 | 1392.82] loss=2.33 avg=2.45\n",
            "[446 | 1395.83] loss=2.19 avg=2.45\n",
            "[447 | 1398.85] loss=2.53 avg=2.45\n",
            "[448 | 1401.86] loss=2.35 avg=2.45\n",
            "[449 | 1404.88] loss=2.72 avg=2.45\n",
            "[450 | 1407.90] loss=2.07 avg=2.45\n",
            "[451 | 1410.92] loss=2.65 avg=2.45\n",
            "[452 | 1413.93] loss=2.42 avg=2.45\n",
            "[453 | 1416.94] loss=3.07 avg=2.45\n",
            "[454 | 1419.96] loss=2.50 avg=2.45\n",
            "[455 | 1422.98] loss=2.69 avg=2.46\n",
            "[456 | 1426.01] loss=2.29 avg=2.45\n",
            "[457 | 1429.03] loss=2.01 avg=2.45\n",
            "[458 | 1432.05] loss=2.74 avg=2.45\n",
            "[459 | 1435.08] loss=2.22 avg=2.45\n",
            "[460 | 1438.10] loss=2.21 avg=2.45\n",
            "[461 | 1441.12] loss=2.24 avg=2.45\n",
            "[462 | 1444.15] loss=2.15 avg=2.44\n",
            "[463 | 1447.18] loss=2.17 avg=2.44\n",
            "[464 | 1450.20] loss=2.30 avg=2.44\n",
            "[465 | 1453.21] loss=2.32 avg=2.44\n",
            "[466 | 1456.23] loss=2.39 avg=2.44\n",
            "[467 | 1459.25] loss=2.41 avg=2.44\n",
            "[468 | 1462.29] loss=2.10 avg=2.43\n",
            "[469 | 1465.30] loss=1.94 avg=2.43\n",
            "[470 | 1468.32] loss=2.33 avg=2.43\n",
            "[471 | 1471.34] loss=3.01 avg=2.43\n",
            "[472 | 1474.35] loss=2.53 avg=2.43\n",
            "[473 | 1477.37] loss=2.57 avg=2.44\n",
            "[474 | 1480.40] loss=2.96 avg=2.44\n",
            "[475 | 1483.44] loss=2.00 avg=2.44\n",
            "[476 | 1486.45] loss=2.24 avg=2.43\n",
            "[477 | 1489.46] loss=2.49 avg=2.44\n",
            "[478 | 1492.49] loss=2.26 avg=2.43\n",
            "[479 | 1495.51] loss=2.34 avg=2.43\n",
            "[480 | 1498.52] loss=2.00 avg=2.43\n",
            "[481 | 1501.54] loss=1.84 avg=2.42\n",
            "[482 | 1504.56] loss=2.34 avg=2.42\n",
            "[483 | 1507.58] loss=2.26 avg=2.42\n",
            "[484 | 1510.60] loss=2.49 avg=2.42\n",
            "[485 | 1513.62] loss=2.26 avg=2.42\n",
            "[486 | 1516.64] loss=2.02 avg=2.41\n",
            "[487 | 1519.65] loss=2.41 avg=2.41\n",
            "[488 | 1522.68] loss=2.03 avg=2.41\n",
            "[489 | 1525.69] loss=2.75 avg=2.41\n",
            "[490 | 1528.70] loss=2.18 avg=2.41\n",
            "[491 | 1531.72] loss=2.33 avg=2.41\n",
            "[492 | 1534.75] loss=2.11 avg=2.41\n",
            "[493 | 1537.77] loss=2.29 avg=2.41\n",
            "[494 | 1540.81] loss=2.29 avg=2.41\n",
            "[495 | 1543.82] loss=2.44 avg=2.41\n",
            "[496 | 1546.84] loss=2.17 avg=2.40\n",
            "[497 | 1549.86] loss=2.27 avg=2.40\n",
            "[498 | 1552.87] loss=2.01 avg=2.40\n",
            "[499 | 1555.90] loss=2.22 avg=2.40\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "12) rw: but are you using gnome?\n",
            "(1102713400) rw: if not, the window manager is much better to use for me atm\n",
            "(1102713500) rw: how much should ubuntu be?\n",
            "(1102713460) rw: i think a bit more.\n",
            "(1102713520) jnx: not sure.  I installed the grub command as it already had that configured, but grub's settings probably don't exactly matter\n",
            "\n",
            "\n",
            "(1149041880) ezz: hey guys, it's a new user and that doesn't make sense right?\n",
            "(1149041900) ezz: I have installed ubuntu for the first time on a laptop and it doesn't recognize it, what could be the reason I'm new to the operating system?\n",
            "(1149041920) ezz: Any ideas?\n",
            "(1120352040) ezz:  What's the problem?\n",
            "(1120352100) ezz: Is there anything better than the ubuntu installer/installer to run an xorg program on my PC?\n",
            "(1120352480) ezz: I have a couple of programs I want to run, but they are all in terminal only\n",
            "(1120352480) ezz: Anyone at all know what these programs are, and what is the difference between that and the terminal version?\n",
            "(1120352540) eb1c: there's the gui\n",
            "(1120352600) eb1c: you'll still want to use it from the terminal.\n",
            "(1120352780) eb1c: you will have to do some prepackaging, so I would recommend that you use an external monitor setup if you are not familiar with doing such stuff\n",
            "(1120352840) eb1c: there's a GUI tool that lets you add to the list.  I don't use that, you may need to modify your 'menu.lst' file to include your input device\n",
            "\n",
            "\n",
            "(1217154060) kc_: hi\n",
            "(1217154060) kc_: i have an issue where when its working its really messed up it wont let me use the mouse and stuff, but when it comes back it will load properly, and i have to do everything\n",
            "(1217154120) kc_: theres the same problem\n",
            "(1217154120) kc_: it will always get it working tho, because theres no way to fix it\n",
            "(1217154180) kc_: how you can do something about this\n",
            "(1217154180) kc_: I need something to help me with that\n",
            "(1217154240) kc_: what i need\n",
            "(1217154240) kc_: im using ubuntu from my windows laptop for a moment now, can you give me the exact instructions for how to set it so that i dont need to be in terminal to do things anyway? its not what im used to doing\n",
            "(1217154240) kc_: what should i do?\n",
            "(1217154240) kc_:  i can't make it run anymore :(\n",
            "(1217154240) kc_: but i can't even open my normal windows app, like the windows gui, to get in the system. so should i quit and run Ubuntu on the terminal or get the whole thing running?\n",
            "(1217154240) kc_: what i did was reinstalled ubuntu through the ubuntu-desktop, but that doesnt allow gnome to be opened/terminated\n",
            "(1217154260) kc_: then anyhow, what you have to do is make the screen resolution 1024x768 the full screen resolution, in fact, you should set your screen to a screen resolution of 1920x1200\n",
            "(1217155120) n2nhfs: you have to change some of your settings, you should be able to switch to windows desktop as it says to do.\n",
            "(1217154260) n2nhfs: or what you see at the other end\n",
            "(1217154260) n2nhfs: there's a 'desktop' applet at the top of the Ubuntu interface\n",
            "(1217154320) n2nhfs: this is how to do it. http://pastebin.com/qT9WXcWqf\n",
            "\n",
            "\n",
            "(1247040400) bazb_brahman: hi, I'm using ubuntu 9.04 on a dual-core\n",
            "(1247040400) bazb_brahman: I'm using a dual-core machine, I think I need some help from a friend...\n",
            "(1247040460) bazb_\n",
            "\n",
            "[500 | 1596.00] loss=2.12 avg=2.39\n",
            "[501 | 1599.02] loss=2.53 avg=2.40\n",
            "[502 | 1602.04] loss=2.95 avg=2.40\n",
            "[503 | 1605.05] loss=2.17 avg=2.40\n",
            "[504 | 1608.05] loss=2.07 avg=2.40\n",
            "[505 | 1611.07] loss=2.96 avg=2.40\n",
            "[506 | 1614.09] loss=3.04 avg=2.41\n",
            "[507 | 1617.12] loss=2.26 avg=2.41\n",
            "[508 | 1620.14] loss=2.67 avg=2.41\n",
            "[509 | 1623.16] loss=2.29 avg=2.41\n",
            "[510 | 1626.18] loss=1.50 avg=2.40\n",
            "[511 | 1629.20] loss=2.60 avg=2.40\n",
            "[512 | 1632.22] loss=2.31 avg=2.40\n",
            "[513 | 1635.25] loss=2.58 avg=2.40\n",
            "[514 | 1638.27] loss=2.75 avg=2.40\n",
            "[515 | 1641.29] loss=2.50 avg=2.41\n",
            "[516 | 1644.30] loss=2.92 avg=2.41\n",
            "[517 | 1647.32] loss=2.58 avg=2.41\n",
            "[518 | 1650.34] loss=2.49 avg=2.41\n",
            "[519 | 1653.36] loss=2.18 avg=2.41\n",
            "[520 | 1656.37] loss=2.88 avg=2.42\n",
            "[521 | 1659.39] loss=2.12 avg=2.41\n",
            "[522 | 1662.42] loss=2.37 avg=2.41\n",
            "[523 | 1665.43] loss=2.36 avg=2.41\n",
            "[524 | 1668.46] loss=2.44 avg=2.41\n",
            "[525 | 1671.49] loss=2.25 avg=2.41\n",
            "[526 | 1674.52] loss=2.54 avg=2.41\n",
            "[527 | 1677.54] loss=2.30 avg=2.41\n",
            "[528 | 1680.57] loss=3.08 avg=2.42\n",
            "[529 | 1683.59] loss=2.47 avg=2.42\n",
            "[530 | 1686.60] loss=2.39 avg=2.42\n",
            "[531 | 1689.62] loss=2.37 avg=2.42\n",
            "[532 | 1692.63] loss=2.17 avg=2.41\n",
            "[533 | 1695.65] loss=2.14 avg=2.41\n",
            "[534 | 1698.66] loss=2.45 avg=2.41\n",
            "[535 | 1701.69] loss=2.51 avg=2.41\n",
            "[536 | 1704.72] loss=2.43 avg=2.41\n",
            "[537 | 1707.74] loss=2.60 avg=2.42\n",
            "[538 | 1710.78] loss=2.69 avg=2.42\n",
            "[539 | 1713.81] loss=2.15 avg=2.42\n",
            "[540 | 1716.83] loss=2.13 avg=2.41\n",
            "[541 | 1719.85] loss=2.31 avg=2.41\n",
            "[542 | 1722.85] loss=2.55 avg=2.41\n",
            "[543 | 1725.87] loss=2.09 avg=2.41\n",
            "[544 | 1728.89] loss=2.22 avg=2.41\n",
            "[545 | 1731.90] loss=2.19 avg=2.41\n",
            "[546 | 1734.92] loss=2.44 avg=2.41\n",
            "[547 | 1737.94] loss=2.76 avg=2.41\n",
            "[548 | 1740.96] loss=2.32 avg=2.41\n",
            "[549 | 1743.97] loss=2.29 avg=2.41\n",
            "[550 | 1747.00] loss=2.31 avg=2.41\n",
            "[551 | 1750.03] loss=2.32 avg=2.41\n",
            "[552 | 1753.06] loss=2.41 avg=2.41\n",
            "[553 | 1756.08] loss=2.47 avg=2.41\n",
            "[554 | 1759.12] loss=2.55 avg=2.41\n",
            "[555 | 1762.15] loss=2.79 avg=2.41\n",
            "[556 | 1765.17] loss=2.64 avg=2.41\n",
            "[557 | 1768.19] loss=2.11 avg=2.41\n",
            "[558 | 1771.20] loss=2.26 avg=2.41\n",
            "[559 | 1774.23] loss=1.90 avg=2.40\n",
            "[560 | 1777.24] loss=2.32 avg=2.40\n",
            "[561 | 1780.25] loss=1.86 avg=2.40\n",
            "[562 | 1783.26] loss=2.71 avg=2.40\n",
            "[563 | 1786.27] loss=2.75 avg=2.40\n",
            "[564 | 1789.29] loss=2.25 avg=2.40\n",
            "[565 | 1792.31] loss=2.13 avg=2.40\n",
            "[566 | 1795.33] loss=2.35 avg=2.40\n",
            "[567 | 1798.34] loss=2.03 avg=2.40\n",
            "[568 | 1801.36] loss=2.28 avg=2.39\n",
            "[569 | 1804.39] loss=2.25 avg=2.39\n",
            "[570 | 1807.42] loss=2.44 avg=2.39\n",
            "[571 | 1810.44] loss=2.79 avg=2.40\n",
            "[572 | 1813.45] loss=2.38 avg=2.40\n",
            "[573 | 1816.48] loss=2.76 avg=2.40\n",
            "[574 | 1819.50] loss=2.66 avg=2.40\n",
            "[575 | 1822.53] loss=2.10 avg=2.40\n",
            "[576 | 1825.54] loss=2.34 avg=2.40\n",
            "[577 | 1828.57] loss=2.32 avg=2.40\n",
            "[578 | 1831.58] loss=3.04 avg=2.41\n",
            "[579 | 1834.59] loss=2.17 avg=2.40\n",
            "[580 | 1837.60] loss=2.37 avg=2.40\n",
            "[581 | 1840.62] loss=2.56 avg=2.40\n",
            "[582 | 1843.64] loss=2.40 avg=2.40\n",
            "[583 | 1846.66] loss=2.69 avg=2.41\n",
            "[584 | 1849.68] loss=2.05 avg=2.40\n",
            "[585 | 1852.69] loss=2.21 avg=2.40\n",
            "[586 | 1855.72] loss=2.20 avg=2.40\n",
            "[587 | 1858.74] loss=2.45 avg=2.40\n",
            "[588 | 1861.76] loss=2.59 avg=2.40\n",
            "[589 | 1864.78] loss=2.62 avg=2.40\n",
            "[590 | 1867.80] loss=2.85 avg=2.41\n",
            "[591 | 1870.82] loss=2.53 avg=2.41\n",
            "[592 | 1873.85] loss=2.47 avg=2.41\n",
            "[593 | 1876.88] loss=1.90 avg=2.41\n",
            "[594 | 1879.89] loss=2.40 avg=2.41\n",
            "[595 | 1882.92] loss=1.98 avg=2.40\n",
            "[596 | 1885.93] loss=2.23 avg=2.40\n",
            "[597 | 1888.95] loss=2.48 avg=2.40\n",
            "[598 | 1891.97] loss=2.73 avg=2.40\n",
            "[599 | 1894.98] loss=2.16 avg=2.40\n",
            "[600 | 1898.00] loss=1.88 avg=2.40\n",
            "[601 | 1901.02] loss=2.44 avg=2.40\n",
            "[602 | 1904.03] loss=2.58 avg=2.40\n",
            "[603 | 1907.05] loss=2.55 avg=2.40\n",
            "[604 | 1910.07] loss=2.71 avg=2.40\n",
            "[605 | 1913.10] loss=2.05 avg=2.40\n",
            "[606 | 1916.12] loss=2.46 avg=2.40\n",
            "[607 | 1919.14] loss=2.79 avg=2.40\n",
            "[608 | 1922.16] loss=2.42 avg=2.40\n",
            "[609 | 1925.17] loss=2.39 avg=2.40\n",
            "[610 | 1928.19] loss=2.86 avg=2.41\n",
            "[611 | 1931.23] loss=2.66 avg=2.41\n",
            "[612 | 1934.25] loss=2.38 avg=2.41\n",
            "[613 | 1937.27] loss=2.26 avg=2.41\n",
            "[614 | 1940.28] loss=2.76 avg=2.41\n",
            "[615 | 1943.30] loss=2.54 avg=2.41\n",
            "[616 | 1946.31] loss=2.73 avg=2.42\n",
            "[617 | 1949.33] loss=2.25 avg=2.42\n",
            "[618 | 1952.34] loss=2.44 avg=2.42\n",
            "[619 | 1955.35] loss=2.48 avg=2.42\n",
            "[620 | 1958.37] loss=2.12 avg=2.41\n",
            "[621 | 1961.39] loss=2.41 avg=2.41\n",
            "[622 | 1964.40] loss=2.44 avg=2.41\n",
            "[623 | 1967.42] loss=1.96 avg=2.41\n",
            "[624 | 1970.43] loss=2.25 avg=2.41\n",
            "[625 | 1973.45] loss=2.19 avg=2.41\n",
            "[626 | 1976.45] loss=2.25 avg=2.40\n",
            "[627 | 1979.47] loss=2.32 avg=2.40\n",
            "[628 | 1982.49] loss=2.45 avg=2.40\n",
            "[629 | 1985.50] loss=2.06 avg=2.40\n",
            "[630 | 1988.53] loss=2.45 avg=2.40\n",
            "[631 | 1991.52] loss=2.25 avg=2.40\n",
            "[632 | 1994.53] loss=1.89 avg=2.39\n",
            "[633 | 1997.54] loss=2.10 avg=2.39\n",
            "[634 | 2000.55] loss=2.06 avg=2.39\n",
            "[635 | 2003.57] loss=2.10 avg=2.38\n",
            "[636 | 2006.58] loss=2.66 avg=2.39\n",
            "[637 | 2009.60] loss=2.57 avg=2.39\n",
            "[638 | 2012.62] loss=2.03 avg=2.39\n",
            "[639 | 2015.64] loss=2.48 avg=2.39\n",
            "[640 | 2018.65] loss=2.11 avg=2.38\n",
            "[641 | 2021.67] loss=2.86 avg=2.39\n",
            "[642 | 2024.68] loss=2.33 avg=2.39\n",
            "[643 | 2027.68] loss=2.30 avg=2.39\n",
            "[644 | 2030.69] loss=2.63 avg=2.39\n",
            "[645 | 2033.71] loss=2.99 avg=2.40\n",
            "[646 | 2036.72] loss=2.17 avg=2.39\n",
            "[647 | 2039.73] loss=2.04 avg=2.39\n",
            "[648 | 2042.74] loss=2.21 avg=2.39\n",
            "[649 | 2045.74] loss=2.97 avg=2.39\n",
            "[650 | 2048.75] loss=2.72 avg=2.40\n",
            "[651 | 2051.74] loss=2.14 avg=2.39\n",
            "[652 | 2054.74] loss=2.94 avg=2.40\n",
            "[653 | 2057.74] loss=2.00 avg=2.40\n",
            "[654 | 2060.74] loss=2.71 avg=2.40\n",
            "[655 | 2063.75] loss=1.52 avg=2.39\n",
            "[656 | 2066.75] loss=2.65 avg=2.39\n",
            "[657 | 2069.75] loss=2.32 avg=2.39\n",
            "[658 | 2072.77] loss=2.89 avg=2.40\n",
            "[659 | 2075.78] loss=2.48 avg=2.40\n",
            "[660 | 2078.79] loss=2.84 avg=2.40\n",
            "[661 | 2081.79] loss=2.40 avg=2.40\n",
            "[662 | 2084.80] loss=2.42 avg=2.40\n",
            "[663 | 2087.80] loss=2.30 avg=2.40\n",
            "[664 | 2090.81] loss=2.31 avg=2.40\n",
            "[665 | 2093.82] loss=2.54 avg=2.40\n",
            "[666 | 2096.84] loss=2.36 avg=2.40\n",
            "[667 | 2099.85] loss=2.59 avg=2.40\n",
            "[668 | 2102.86] loss=2.25 avg=2.40\n",
            "[669 | 2105.87] loss=2.27 avg=2.40\n",
            "[670 | 2108.87] loss=1.85 avg=2.40\n",
            "[671 | 2111.89] loss=2.18 avg=2.39\n",
            "[672 | 2114.91] loss=2.89 avg=2.40\n",
            "[673 | 2117.93] loss=2.48 avg=2.40\n",
            "[674 | 2120.93] loss=2.35 avg=2.40\n",
            "[675 | 2123.95] loss=2.45 avg=2.40\n",
            "[676 | 2126.98] loss=2.86 avg=2.40\n",
            "[677 | 2130.00] loss=2.32 avg=2.40\n",
            "[678 | 2133.02] loss=2.32 avg=2.40\n",
            "[679 | 2136.04] loss=2.51 avg=2.40\n",
            "[680 | 2139.05] loss=2.37 avg=2.40\n",
            "[681 | 2142.06] loss=2.21 avg=2.40\n",
            "[682 | 2145.09] loss=2.40 avg=2.40\n",
            "[683 | 2148.11] loss=2.37 avg=2.40\n",
            "[684 | 2151.11] loss=2.27 avg=2.40\n",
            "[685 | 2154.13] loss=2.76 avg=2.40\n",
            "[686 | 2157.15] loss=2.21 avg=2.40\n",
            "[687 | 2160.16] loss=1.86 avg=2.40\n",
            "[688 | 2163.17] loss=2.13 avg=2.39\n",
            "[689 | 2166.19] loss=2.19 avg=2.39\n",
            "[690 | 2169.21] loss=2.17 avg=2.39\n",
            "[691 | 2172.22] loss=2.56 avg=2.39\n",
            "[692 | 2175.23] loss=2.90 avg=2.40\n",
            "[693 | 2178.24] loss=2.84 avg=2.40\n",
            "[694 | 2181.26] loss=2.49 avg=2.40\n",
            "[695 | 2184.27] loss=2.27 avg=2.40\n",
            "[696 | 2187.28] loss=2.80 avg=2.40\n",
            "[697 | 2190.28] loss=2.41 avg=2.40\n",
            "[698 | 2193.28] loss=2.41 avg=2.40\n",
            "[699 | 2196.30] loss=2.42 avg=2.40\n",
            "[700 | 2199.31] loss=3.03 avg=2.41\n",
            "[701 | 2202.32] loss=2.84 avg=2.41\n",
            "[702 | 2205.33] loss=2.61 avg=2.42\n",
            "[703 | 2208.34] loss=2.62 avg=2.42\n",
            "[704 | 2211.36] loss=2.36 avg=2.42\n",
            "[705 | 2214.37] loss=2.73 avg=2.42\n",
            "[706 | 2217.37] loss=2.12 avg=2.42\n",
            "[707 | 2220.39] loss=2.82 avg=2.42\n",
            "[708 | 2223.41] loss=2.27 avg=2.42\n",
            "[709 | 2226.43] loss=2.39 avg=2.42\n",
            "[710 | 2229.44] loss=2.67 avg=2.42\n",
            "[711 | 2232.46] loss=2.18 avg=2.42\n",
            "[712 | 2235.47] loss=2.14 avg=2.42\n",
            "[713 | 2238.48] loss=2.64 avg=2.42\n",
            "[714 | 2241.50] loss=2.68 avg=2.42\n",
            "[715 | 2244.50] loss=2.21 avg=2.42\n",
            "[716 | 2247.52] loss=2.32 avg=2.42\n",
            "[717 | 2250.54] loss=2.69 avg=2.42\n",
            "[718 | 2253.54] loss=2.86 avg=2.43\n",
            "[719 | 2256.56] loss=2.45 avg=2.43\n",
            "[720 | 2259.57] loss=2.06 avg=2.42\n",
            "[721 | 2262.58] loss=2.49 avg=2.42\n",
            "[722 | 2265.60] loss=2.36 avg=2.42\n",
            "[723 | 2268.62] loss=2.12 avg=2.42\n",
            "[724 | 2271.63] loss=2.43 avg=2.42\n",
            "[725 | 2274.65] loss=2.05 avg=2.42\n",
            "[726 | 2277.65] loss=2.21 avg=2.41\n",
            "[727 | 2280.67] loss=2.35 avg=2.41\n",
            "[728 | 2283.69] loss=2.07 avg=2.41\n",
            "[729 | 2286.71] loss=1.89 avg=2.40\n",
            "[730 | 2289.72] loss=2.53 avg=2.41\n",
            "[731 | 2292.73] loss=2.39 avg=2.41\n",
            "[732 | 2295.73] loss=2.84 avg=2.41\n",
            "[733 | 2298.75] loss=2.08 avg=2.41\n",
            "[734 | 2301.76] loss=2.38 avg=2.41\n",
            "[735 | 2304.78] loss=2.25 avg=2.41\n",
            "[736 | 2307.80] loss=2.23 avg=2.40\n",
            "[737 | 2310.81] loss=2.15 avg=2.40\n",
            "[738 | 2313.83] loss=2.20 avg=2.40\n",
            "[739 | 2316.85] loss=2.48 avg=2.40\n",
            "[740 | 2319.86] loss=2.43 avg=2.40\n",
            "[741 | 2322.88] loss=2.54 avg=2.40\n",
            "[742 | 2325.90] loss=2.21 avg=2.40\n",
            "[743 | 2328.91] loss=1.92 avg=2.39\n",
            "[744 | 2331.92] loss=2.44 avg=2.40\n",
            "[745 | 2334.93] loss=2.23 avg=2.39\n",
            "[746 | 2337.94] loss=2.15 avg=2.39\n",
            "[747 | 2340.96] loss=2.42 avg=2.39\n",
            "[748 | 2343.96] loss=2.25 avg=2.39\n",
            "[749 | 2346.98] loss=2.36 avg=2.39\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "200,1000\n",
            "(1221467100) jfh: I see\n",
            "(1221467100) jfh: how can i fix that?\n",
            "(1221467100) jfh: i dont really understand, im kind of confused with the install process...\n",
            "(1221467100) jfh: i didnt install ubuntu until now but i installed 'ubuntu' the first step.. now i have to manually type that command everytime i want to go over a command and want to do something other than 'sudo -i'\n",
            "(1221467160) jfh: when i make a shortcut to a directory using the command i had in my system there are times that the directory doesnt open.\n",
            "(1221467160) jfh: but now at the top of the directory is the place to make a shortcut for.\n",
            "(1221467320) jfh: so is there a way to make that location the most important as opposed to the default?\n",
            "(1221467320) jfh: i'm not sure what exactly is correct?\n",
            "(1221467380) jfh: what is 'normal'?\n",
            "(1221467440) jfh: my system boots automatically everytime an application starts and the windows does not do that. when i make a shortcut to a file its automatically executed when the file is opened. now when i make it the file that opens does NOT execute at all. when i open the file the file does run. when i open it the file does execute\n",
            "(1221467440) jfh: if you want something to run when a file is opened the file opens with the first command line.. that command line should execute when an application is launched\n",
            "(1221467500) jfh: the shortcut is like so:\n",
            "(1221467560) jfh: it opens with the first command line when the file is opened... how is a folder not runing?\n",
            "(1221467660) jfh: i'm not sure what that command line is exactly\n",
            "(1221467660) jfh: it opens the folder and then executes the command\n",
            "(1221467660) jfh: to a command line but thats not what we need\n",
            "(1221467660) jfh: i'm not sure how to tell how to do that?\n",
            "\n",
            "\n",
            "(1197587240) jed: hello\n",
            "(119758800) jed: i need help, i cant access my network card\n",
            "(119758800) jed: any ideas what could be wrong?\n",
            "(119758860) jed: i'm currently on a laptop\n",
            "(119758860) jed: i see a wifi link in my network\n",
            "(119758860) Jed: I need help\n",
            "(119758860) jed: i use to know\n",
            "(119758860) jed: but the wifi link in the network card is no.\n",
            "(119758860) jed: my laptop\n",
            "(119758860) jed: doesnt work as if the card was wired\n",
            "(119758860) jed: network card?\n",
            "(119758860) jed: so I need to manually configure the wireless, do i need to install wireless driver too?\n",
            "(119758860) jed: can anybody help me?\n",
            "(1197588940) jed: i'm using this same laptop\n",
            "(1197588940) jed: but the wifi card is still working\n",
            "(1197589000) jed: not sure.\n",
            "(1197589000) jed: network card not working, network card not supported\n",
            "(1197589110) jed: so the network card is working out of the gate, but the wifi link is not working?\n",
            "(1197589220) jed: what's wrong\n",
            "(1197589000) jed: what's for ?\n",
            "(1197589420) jed: I'm using my laptop\n",
            "(1197589420) jed: usb wireless adapter\n",
            "(1197589420) jed: no connection to wireless card\n",
            "(1197589420) jed: there's no wireless\n",
            "(1197589420) jed: so I can't get a wifi radio connected\n",
            "(1197589420) Jed: oh\n",
            "(1197589420) Jed: I'm doing\n",
            "(1197589420) jed: and\n",
            "(1197589420) jed: it doesnt respond\n",
            "(1197589420) jed: but I do not know what could be wrong\n",
            "(1197589420) jed: ok\n",
            "(1197589420) jed: thanks\n",
            "(1197589420) jed: thanks\n",
            "(1197589420) jed: can you try\n",
            "\n",
            "[750 | 2387.80] loss=2.42 avg=2.39\n",
            "[751 | 2390.78] loss=2.59 avg=2.39\n",
            "[752 | 2393.75] loss=2.38 avg=2.39\n",
            "[753 | 2396.74] loss=2.38 avg=2.39\n",
            "[754 | 2399.71] loss=2.33 avg=2.39\n",
            "[755 | 2402.69] loss=2.24 avg=2.39\n",
            "[756 | 2405.84] loss=2.57 avg=2.39\n",
            "[757 | 2408.82] loss=2.27 avg=2.39\n",
            "[758 | 2411.79] loss=2.29 avg=2.39\n",
            "[759 | 2414.78] loss=2.15 avg=2.39\n",
            "[760 | 2417.76] loss=2.37 avg=2.39\n",
            "[761 | 2420.74] loss=2.45 avg=2.39\n",
            "[762 | 2423.79] loss=2.28 avg=2.39\n",
            "[763 | 2426.78] loss=2.39 avg=2.39\n",
            "[764 | 2429.75] loss=2.14 avg=2.38\n",
            "[765 | 2432.73] loss=2.57 avg=2.39\n",
            "[766 | 2435.79] loss=2.02 avg=2.38\n",
            "[767 | 2438.77] loss=1.99 avg=2.38\n",
            "[768 | 2441.75] loss=2.05 avg=2.37\n",
            "[769 | 2444.73] loss=1.96 avg=2.37\n",
            "[770 | 2447.79] loss=2.55 avg=2.37\n",
            "[771 | 2450.76] loss=2.16 avg=2.37\n",
            "[772 | 2453.74] loss=2.23 avg=2.37\n",
            "[773 | 2456.72] loss=2.59 avg=2.37\n",
            "[774 | 2459.68] loss=2.88 avg=2.38\n",
            "[775 | 2462.65] loss=2.27 avg=2.38\n",
            "[776 | 2465.62] loss=2.19 avg=2.37\n",
            "[777 | 2468.61] loss=2.42 avg=2.37\n",
            "[778 | 2471.60] loss=2.37 avg=2.37\n",
            "[779 | 2474.57] loss=2.37 avg=2.37\n",
            "[780 | 2477.55] loss=1.72 avg=2.37\n",
            "[781 | 2480.53] loss=2.19 avg=2.37\n",
            "[782 | 2483.51] loss=2.36 avg=2.37\n",
            "[783 | 2486.49] loss=2.41 avg=2.37\n",
            "[784 | 2489.48] loss=2.87 avg=2.37\n",
            "[785 | 2492.45] loss=2.83 avg=2.38\n",
            "[786 | 2495.52] loss=2.57 avg=2.38\n",
            "[787 | 2498.50] loss=2.94 avg=2.38\n",
            "[788 | 2501.56] loss=2.45 avg=2.38\n",
            "[789 | 2504.63] loss=2.32 avg=2.38\n",
            "[790 | 2507.61] loss=2.51 avg=2.38\n",
            "[791 | 2510.59] loss=2.47 avg=2.39\n",
            "[792 | 2513.56] loss=2.54 avg=2.39\n",
            "[793 | 2516.62] loss=2.59 avg=2.39\n",
            "[794 | 2519.66] loss=2.04 avg=2.39\n",
            "[795 | 2522.65] loss=2.19 avg=2.38\n",
            "[796 | 2525.64] loss=2.13 avg=2.38\n",
            "[797 | 2528.62] loss=2.12 avg=2.38\n",
            "[798 | 2531.60] loss=1.91 avg=2.37\n",
            "[799 | 2534.56] loss=2.36 avg=2.37\n",
            "[800 | 2537.52] loss=2.82 avg=2.38\n",
            "[801 | 2540.49] loss=1.99 avg=2.37\n",
            "[802 | 2543.47] loss=2.46 avg=2.37\n",
            "[803 | 2546.46] loss=2.15 avg=2.37\n",
            "[804 | 2549.43] loss=2.08 avg=2.37\n",
            "[805 | 2552.49] loss=2.22 avg=2.37\n",
            "[806 | 2555.47] loss=2.33 avg=2.37\n",
            "[807 | 2558.45] loss=2.16 avg=2.37\n",
            "[808 | 2561.44] loss=2.53 avg=2.37\n",
            "[809 | 2564.40] loss=2.56 avg=2.37\n",
            "[810 | 2567.38] loss=2.43 avg=2.37\n",
            "[811 | 2570.38] loss=2.65 avg=2.37\n",
            "[812 | 2573.34] loss=2.17 avg=2.37\n",
            "[813 | 2576.32] loss=2.31 avg=2.37\n",
            "[814 | 2579.31] loss=1.84 avg=2.36\n",
            "[815 | 2582.29] loss=2.38 avg=2.36\n",
            "[816 | 2585.27] loss=2.35 avg=2.36\n",
            "[817 | 2588.27] loss=2.24 avg=2.36\n",
            "[818 | 2591.33] loss=2.45 avg=2.36\n",
            "[819 | 2594.39] loss=2.17 avg=2.36\n",
            "[820 | 2597.45] loss=2.21 avg=2.36\n",
            "[821 | 2600.54] loss=2.38 avg=2.36\n",
            "[822 | 2603.52] loss=2.26 avg=2.36\n",
            "[823 | 2606.51] loss=1.97 avg=2.36\n",
            "[824 | 2609.47] loss=2.63 avg=2.36\n",
            "[825 | 2612.44] loss=2.92 avg=2.36\n",
            "[826 | 2615.40] loss=2.03 avg=2.36\n",
            "[827 | 2618.36] loss=2.21 avg=2.36\n",
            "[828 | 2621.33] loss=2.27 avg=2.36\n",
            "[829 | 2624.29] loss=2.69 avg=2.36\n",
            "[830 | 2627.26] loss=2.32 avg=2.36\n",
            "[831 | 2630.24] loss=2.43 avg=2.36\n",
            "[832 | 2633.22] loss=2.33 avg=2.36\n",
            "[833 | 2636.20] loss=2.28 avg=2.36\n",
            "[834 | 2639.18] loss=2.91 avg=2.37\n",
            "[835 | 2642.16] loss=2.03 avg=2.36\n",
            "[836 | 2645.18] loss=2.17 avg=2.36\n",
            "[837 | 2648.17] loss=2.29 avg=2.36\n",
            "[838 | 2651.17] loss=2.54 avg=2.36\n",
            "[839 | 2654.16] loss=2.00 avg=2.36\n",
            "[840 | 2657.13] loss=2.55 avg=2.36\n",
            "[841 | 2660.13] loss=1.92 avg=2.36\n",
            "[842 | 2663.10] loss=2.94 avg=2.36\n",
            "[843 | 2666.07] loss=2.31 avg=2.36\n",
            "[844 | 2669.06] loss=2.25 avg=2.36\n",
            "[845 | 2672.06] loss=2.67 avg=2.36\n",
            "[846 | 2675.04] loss=2.18 avg=2.36\n",
            "[847 | 2678.02] loss=2.40 avg=2.36\n",
            "[848 | 2681.02] loss=2.33 avg=2.36\n",
            "[849 | 2684.00] loss=2.14 avg=2.36\n",
            "[850 | 2686.98] loss=2.42 avg=2.36\n",
            "[851 | 2689.95] loss=2.42 avg=2.36\n",
            "[852 | 2692.95] loss=2.95 avg=2.37\n",
            "[853 | 2695.93] loss=2.32 avg=2.37\n",
            "[854 | 2698.91] loss=2.29 avg=2.37\n",
            "[855 | 2701.90] loss=2.97 avg=2.37\n",
            "[856 | 2704.89] loss=2.96 avg=2.38\n",
            "[857 | 2707.88] loss=1.79 avg=2.37\n",
            "[858 | 2710.86] loss=3.04 avg=2.38\n",
            "[859 | 2713.85] loss=2.20 avg=2.38\n",
            "[860 | 2716.83] loss=2.92 avg=2.38\n",
            "[861 | 2719.81] loss=2.84 avg=2.39\n",
            "[862 | 2722.80] loss=2.39 avg=2.39\n",
            "[863 | 2725.79] loss=2.46 avg=2.39\n",
            "[864 | 2728.77] loss=1.79 avg=2.38\n",
            "[865 | 2731.75] loss=2.24 avg=2.38\n",
            "[866 | 2734.74] loss=2.11 avg=2.38\n",
            "[867 | 2737.72] loss=2.72 avg=2.38\n",
            "[868 | 2740.71] loss=2.51 avg=2.38\n",
            "[869 | 2743.70] loss=2.13 avg=2.38\n",
            "[870 | 2746.69] loss=1.97 avg=2.38\n",
            "[871 | 2749.67] loss=2.16 avg=2.37\n",
            "[872 | 2752.66] loss=2.17 avg=2.37\n",
            "[873 | 2755.65] loss=2.55 avg=2.37\n",
            "[874 | 2758.63] loss=2.44 avg=2.37\n",
            "[875 | 2761.62] loss=2.37 avg=2.37\n",
            "[876 | 2764.60] loss=2.18 avg=2.37\n",
            "[877 | 2767.60] loss=2.78 avg=2.38\n",
            "[878 | 2770.68] loss=2.13 avg=2.37\n",
            "[879 | 2773.68] loss=2.37 avg=2.37\n",
            "[880 | 2776.68] loss=2.48 avg=2.37\n",
            "[881 | 2779.68] loss=2.38 avg=2.37\n",
            "[882 | 2782.67] loss=2.21 avg=2.37\n",
            "[883 | 2785.74] loss=2.20 avg=2.37\n",
            "[884 | 2788.72] loss=2.20 avg=2.37\n",
            "[885 | 2791.72] loss=2.55 avg=2.37\n",
            "[886 | 2794.70] loss=2.19 avg=2.37\n",
            "[887 | 2797.68] loss=2.59 avg=2.37\n",
            "[888 | 2800.76] loss=2.88 avg=2.38\n",
            "[889 | 2803.74] loss=2.63 avg=2.38\n",
            "[890 | 2806.71] loss=2.64 avg=2.38\n",
            "[891 | 2809.79] loss=2.32 avg=2.38\n",
            "[892 | 2812.76] loss=2.95 avg=2.39\n",
            "[893 | 2815.74] loss=2.94 avg=2.39\n",
            "[894 | 2818.72] loss=2.83 avg=2.40\n",
            "[895 | 2821.69] loss=2.29 avg=2.40\n",
            "[896 | 2824.68] loss=2.51 avg=2.40\n",
            "[897 | 2827.65] loss=2.58 avg=2.40\n",
            "[898 | 2830.62] loss=2.88 avg=2.40\n",
            "[899 | 2833.61] loss=2.11 avg=2.40\n",
            "[900 | 2836.59] loss=2.28 avg=2.40\n",
            "[901 | 2839.58] loss=1.98 avg=2.40\n",
            "[902 | 2842.66] loss=2.36 avg=2.40\n",
            "[903 | 2845.63] loss=2.88 avg=2.40\n",
            "[904 | 2848.70] loss=2.07 avg=2.40\n",
            "[905 | 2851.68] loss=2.12 avg=2.39\n",
            "[906 | 2854.65] loss=2.34 avg=2.39\n",
            "[907 | 2857.63] loss=2.09 avg=2.39\n",
            "[908 | 2860.62] loss=2.23 avg=2.39\n",
            "[909 | 2863.60] loss=3.03 avg=2.40\n",
            "[910 | 2866.59] loss=2.69 avg=2.40\n",
            "[911 | 2869.57] loss=2.43 avg=2.40\n",
            "[912 | 2872.56] loss=2.39 avg=2.40\n",
            "[913 | 2875.53] loss=2.95 avg=2.40\n",
            "[914 | 2878.52] loss=2.92 avg=2.41\n",
            "[915 | 2881.50] loss=2.35 avg=2.41\n",
            "[916 | 2884.47] loss=2.52 avg=2.41\n",
            "[917 | 2887.46] loss=1.91 avg=2.40\n",
            "[918 | 2890.44] loss=2.86 avg=2.41\n",
            "[919 | 2893.42] loss=2.21 avg=2.41\n",
            "[920 | 2896.40] loss=2.43 avg=2.41\n",
            "[921 | 2899.38] loss=2.36 avg=2.41\n",
            "[922 | 2902.36] loss=2.26 avg=2.41\n",
            "[923 | 2905.35] loss=1.73 avg=2.40\n",
            "[924 | 2908.40] loss=2.09 avg=2.40\n",
            "[925 | 2911.40] loss=2.30 avg=2.39\n",
            "[926 | 2914.38] loss=2.51 avg=2.40\n",
            "[927 | 2917.34] loss=2.44 avg=2.40\n",
            "[928 | 2920.32] loss=2.49 avg=2.40\n",
            "[929 | 2923.29] loss=2.23 avg=2.40\n",
            "[930 | 2926.29] loss=2.17 avg=2.39\n",
            "[931 | 2929.29] loss=2.60 avg=2.40\n",
            "[932 | 2932.27] loss=2.28 avg=2.39\n",
            "[933 | 2935.25] loss=2.30 avg=2.39\n",
            "[934 | 2938.32] loss=2.29 avg=2.39\n",
            "[935 | 2941.29] loss=2.47 avg=2.39\n",
            "[936 | 2944.28] loss=2.73 avg=2.40\n",
            "[937 | 2947.27] loss=2.70 avg=2.40\n",
            "[938 | 2950.26] loss=1.87 avg=2.39\n",
            "[939 | 2953.34] loss=2.18 avg=2.39\n",
            "[940 | 2956.41] loss=2.33 avg=2.39\n",
            "[941 | 2959.40] loss=2.11 avg=2.39\n",
            "[942 | 2962.39] loss=2.63 avg=2.39\n",
            "[943 | 2965.36] loss=2.33 avg=2.39\n",
            "[944 | 2968.35] loss=2.20 avg=2.39\n",
            "[945 | 2971.41] loss=1.98 avg=2.38\n",
            "[946 | 2974.39] loss=2.38 avg=2.38\n",
            "[947 | 2977.54] loss=2.20 avg=2.38\n",
            "[948 | 2980.61] loss=2.34 avg=2.38\n",
            "[949 | 2983.58] loss=2.04 avg=2.38\n",
            "[950 | 2986.56] loss=2.97 avg=2.38\n",
            "[951 | 2989.62] loss=2.58 avg=2.39\n",
            "[952 | 2992.60] loss=2.32 avg=2.39\n",
            "[953 | 2995.58] loss=2.27 avg=2.38\n",
            "[954 | 2998.56] loss=2.26 avg=2.38\n",
            "[955 | 3001.55] loss=2.33 avg=2.38\n",
            "[956 | 3004.52] loss=2.04 avg=2.38\n",
            "[957 | 3007.50] loss=2.10 avg=2.38\n",
            "[958 | 3010.46] loss=2.62 avg=2.38\n",
            "[959 | 3013.43] loss=2.52 avg=2.38\n",
            "[960 | 3016.42] loss=1.79 avg=2.37\n",
            "[961 | 3019.40] loss=2.80 avg=2.38\n",
            "[962 | 3022.38] loss=2.28 avg=2.38\n",
            "[963 | 3025.35] loss=2.31 avg=2.38\n",
            "[964 | 3028.32] loss=2.18 avg=2.38\n",
            "[965 | 3031.30] loss=2.19 avg=2.37\n",
            "[966 | 3034.29] loss=2.29 avg=2.37\n",
            "[967 | 3037.26] loss=2.80 avg=2.38\n",
            "[968 | 3040.23] loss=2.17 avg=2.37\n",
            "[969 | 3043.22] loss=2.37 avg=2.37\n",
            "[970 | 3046.22] loss=2.85 avg=2.38\n",
            "[971 | 3049.20] loss=2.38 avg=2.38\n",
            "[972 | 3052.17] loss=2.59 avg=2.38\n",
            "[973 | 3055.17] loss=2.89 avg=2.39\n",
            "[974 | 3058.15] loss=2.69 avg=2.39\n",
            "[975 | 3061.13] loss=2.35 avg=2.39\n",
            "[976 | 3064.12] loss=2.93 avg=2.39\n",
            "[977 | 3067.11] loss=2.43 avg=2.39\n",
            "[978 | 3070.10] loss=2.20 avg=2.39\n",
            "[979 | 3073.09] loss=2.23 avg=2.39\n",
            "[980 | 3076.09] loss=2.57 avg=2.39\n",
            "[981 | 3079.08] loss=2.22 avg=2.39\n",
            "[982 | 3082.07] loss=2.43 avg=2.39\n",
            "[983 | 3085.06] loss=1.99 avg=2.39\n",
            "[984 | 3088.05] loss=2.74 avg=2.39\n",
            "[985 | 3091.04] loss=2.12 avg=2.39\n",
            "[986 | 3094.04] loss=1.73 avg=2.38\n",
            "[987 | 3097.02] loss=2.48 avg=2.38\n",
            "[988 | 3100.02] loss=2.17 avg=2.38\n",
            "[989 | 3103.02] loss=2.38 avg=2.38\n",
            "[990 | 3106.02] loss=2.33 avg=2.38\n",
            "[991 | 3109.01] loss=2.19 avg=2.38\n",
            "[992 | 3111.99] loss=2.17 avg=2.38\n",
            "[993 | 3114.99] loss=2.21 avg=2.37\n",
            "[994 | 3117.98] loss=2.12 avg=2.37\n",
            "[995 | 3120.97] loss=3.81 avg=2.39\n",
            "[996 | 3123.95] loss=2.24 avg=2.38\n",
            "[997 | 3126.94] loss=2.27 avg=2.38\n",
            "[998 | 3129.93] loss=2.30 avg=2.38\n",
            "[999 | 3132.92] loss=2.16 avg=2.38\n",
            "Saving checkpoint/run1/model-1000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " or it doesnt work :S\n",
            "(1136139960) S_A_L_Z: do you know how\n",
            "(1136139960) S_A_L_Z: ok, why cant you just paste in this\n",
            "\n",
            "\n",
            "(1161858120) jtpp: Hi i havent upgraded my ubuntu in a very long time... i'm in an LTS version\n",
            "(1161858180) jtpp: How long would that take?\n",
            "(1161858240) jtpp: i was using ubuntu beta\n",
            "(1161858180) jtpp: any idea?\n",
            "(1161858320) zarq:  don't you have to install a 64 bit kernel for a 32 bit system ?\n",
            "(1161858380) jtpp:   i dont know what you mean.... whats '64 bit' ?\n",
            "(1161858440) jtpp: i need a 64 bit kernel for a 64 bit system\n",
            "(1161858500) jtpp: i can't access it with the desktop\n",
            "(1161858600) zarq: do you have 64 bit nvidia or whatnot installed ?\n",
            "(1161858600) jtpp: no\n",
            "(1161858700) jtpp: i have some nvidia drivers installed\n",
            "(1161858700) nvidia: what is 64 bit kernel\n",
            "(1161858900) jtpp: 64 bit kernel\n",
            "(1161858960) zarq: what command do you give for x86_64 ?\n",
            "(1161858960) jtpp: nvidia is running...\n",
            "(1161859280) jtpp: what kind of nvidia version do you have ?\n",
            "(1161859600) jtpp: 64 bit kernel\n",
            "(1161859600) zarq: sorry, i'm a little confused\n",
            "(1161859600) jtpp: it says X server 2.6.22-12-generic #1 SRC\n",
            "(1161859600) jtpp: 64 bit kernel\n",
            "(1161859720) jtpp:  i need to install a 64 bit kernel for a 64 bit system\n",
            "(1161859720) jtpp: i know there is a kernel install\n",
            "(1161859780) jtpp: in terminal i enter\n",
            "(1161859780) jtpp: i want to install a 64 bit kernel\n",
            "(1161859780) zarq: what do you run as X login for x86_64 ?\n",
            "(1161859780) jtpp: http://paste.ubuntu-nl.org/864089/\n",
            "(1161859780) jtpp: x86_64 isn't there any good 64 bit linux linux software?\n",
            "(1161859780) S_A_L_Z: well, I had to read some articles... i'm not so sure\n",
            "(1161859780) Jtpp: i'm trying to install 32 bit linux software...\n",
            "(1161859780) jtpp: well, i need a 64 bit kernel so i'm installing 64 bit nvidia\n",
            "(1161859780) S_A_L_Z: the 64 bit kernel does ?\n",
            "(1161859880) jtpp: no\n",
            "(1161860040) S_A_L_Z: it says : linux-image-x86.deb\n",
            "\n",
            "\n",
            "(1151956240) m3rd_2nd: what is your question\n",
            "(1151956300) m3rd_2nd: what do i have to do?\n",
            "(1151956300) m3rd_2nd: it isn't going to be easy\n",
            "(1151956360) m3rd_2nd: is this a good forum ?\n",
            "(1151956360) m3rd_2nd: i have not installed any\n",
            "(1151956420) m3rd_2nd: what if i had to switch over to that server?\n",
            "(1151956420) m3rd_2nd: if i do not have x11 i am not going to run ubuntu :s\n",
            "(1151956420) m3rd_2nd: do i need to install a 32 bit kernel?\n",
            "(1151956420) m3rd_2nd: i will start xd\n",
            "(1151956420) m3rd_2nd: if  iceroot /?\n",
            "(1151956420) m3rd_2nd: it is the 'i686 kernel'\n",
            "(1151956420) m3rd_2nd: i have not installed any 32 bit kernel\n",
            "(1151956420) m3rd_2nd: i have no x11 server\n",
            "(1151957280) m3rd\n",
            "\n",
            "[1000 | 3182.75] loss=2.32 avg=2.38\n",
            "[1001 | 3185.72] loss=2.12 avg=2.38\n",
            "[1002 | 3188.70] loss=2.71 avg=2.38\n",
            "[1003 | 3191.68] loss=1.91 avg=2.38\n",
            "[1004 | 3194.65] loss=2.23 avg=2.37\n",
            "[1005 | 3197.72] loss=2.38 avg=2.37\n",
            "[1006 | 3200.94] loss=2.37 avg=2.37\n",
            "[1007 | 3203.93] loss=2.10 avg=2.37\n",
            "[1008 | 3206.89] loss=2.01 avg=2.37\n",
            "[1009 | 3209.86] loss=2.43 avg=2.37\n",
            "[1010 | 3212.83] loss=2.84 avg=2.37\n",
            "[1011 | 3215.81] loss=2.55 avg=2.38\n",
            "[1012 | 3218.80] loss=2.58 avg=2.38\n",
            "[1013 | 3221.77] loss=2.31 avg=2.38\n",
            "[1014 | 3224.75] loss=2.13 avg=2.37\n",
            "[1015 | 3227.75] loss=2.49 avg=2.38\n",
            "[1016 | 3230.82] loss=2.31 avg=2.37\n",
            "[1017 | 3233.80] loss=2.23 avg=2.37\n",
            "[1018 | 3236.78] loss=1.68 avg=2.37\n",
            "[1019 | 3239.75] loss=1.94 avg=2.36\n",
            "[1020 | 3242.73] loss=2.34 avg=2.36\n",
            "[1021 | 3245.70] loss=2.44 avg=2.36\n",
            "[1022 | 3248.77] loss=2.51 avg=2.36\n",
            "[1023 | 3251.82] loss=2.74 avg=2.37\n",
            "[1024 | 3254.80] loss=1.92 avg=2.36\n",
            "[1025 | 3257.78] loss=2.21 avg=2.36\n",
            "[1026 | 3260.76] loss=2.10 avg=2.36\n",
            "[1027 | 3263.82] loss=2.39 avg=2.36\n",
            "[1028 | 3266.79] loss=2.94 avg=2.37\n",
            "[1029 | 3269.77] loss=2.32 avg=2.36\n",
            "[1030 | 3272.85] loss=2.50 avg=2.37\n",
            "[1031 | 3275.83] loss=2.16 avg=2.36\n",
            "[1032 | 3278.82] loss=1.78 avg=2.36\n",
            "[1033 | 3281.80] loss=2.40 avg=2.36\n",
            "[1034 | 3284.76] loss=2.00 avg=2.36\n",
            "[1035 | 3287.75] loss=2.27 avg=2.35\n",
            "[1036 | 3290.73] loss=2.54 avg=2.36\n",
            "[1037 | 3293.72] loss=2.85 avg=2.36\n",
            "[1038 | 3296.70] loss=2.80 avg=2.37\n",
            "[1039 | 3299.68] loss=2.55 avg=2.37\n",
            "[1040 | 3302.67] loss=2.46 avg=2.37\n",
            "[1041 | 3305.76] loss=2.16 avg=2.37\n",
            "[1042 | 3308.73] loss=2.37 avg=2.37\n",
            "[1043 | 3311.71] loss=2.44 avg=2.37\n",
            "[1044 | 3314.70] loss=2.56 avg=2.37\n",
            "[1045 | 3317.75] loss=2.20 avg=2.37\n",
            "[1046 | 3320.73] loss=2.34 avg=2.37\n",
            "[1047 | 3323.71] loss=2.29 avg=2.37\n",
            "[1048 | 3326.69] loss=2.30 avg=2.37\n",
            "[1049 | 3329.67] loss=2.29 avg=2.36\n",
            "[1050 | 3332.64] loss=2.28 avg=2.36\n",
            "[1051 | 3335.62] loss=2.79 avg=2.37\n",
            "[1052 | 3338.61] loss=2.64 avg=2.37\n",
            "[1053 | 3341.60] loss=2.66 avg=2.37\n",
            "[1054 | 3344.59] loss=2.08 avg=2.37\n",
            "[1055 | 3347.57] loss=1.95 avg=2.37\n",
            "[1056 | 3350.56] loss=2.41 avg=2.37\n",
            "[1057 | 3353.55] loss=3.12 avg=2.37\n",
            "[1058 | 3356.52] loss=2.90 avg=2.38\n",
            "[1059 | 3359.50] loss=2.51 avg=2.38\n",
            "[1060 | 3362.48] loss=2.34 avg=2.38\n",
            "[1061 | 3365.45] loss=2.66 avg=2.38\n",
            "[1062 | 3368.43] loss=2.28 avg=2.38\n",
            "[1063 | 3371.41] loss=2.36 avg=2.38\n",
            "[1064 | 3374.48] loss=2.64 avg=2.39\n",
            "[1065 | 3377.46] loss=2.23 avg=2.38\n",
            "[1066 | 3380.43] loss=2.30 avg=2.38\n",
            "[1067 | 3383.51] loss=2.19 avg=2.38\n",
            "[1068 | 3386.49] loss=2.30 avg=2.38\n",
            "[1069 | 3389.46] loss=2.58 avg=2.38\n",
            "[1070 | 3392.44] loss=2.64 avg=2.38\n",
            "[1071 | 3395.58] loss=1.96 avg=2.38\n",
            "[1072 | 3398.56] loss=2.42 avg=2.38\n",
            "[1073 | 3401.53] loss=2.59 avg=2.38\n",
            "[1074 | 3404.50] loss=2.27 avg=2.38\n",
            "[1075 | 3407.47] loss=2.69 avg=2.38\n",
            "[1076 | 3410.44] loss=2.09 avg=2.38\n",
            "[1077 | 3413.41] loss=2.77 avg=2.39\n",
            "[1078 | 3416.37] loss=2.41 avg=2.39\n",
            "[1079 | 3419.34] loss=2.25 avg=2.38\n",
            "[1080 | 3422.30] loss=2.77 avg=2.39\n",
            "[1081 | 3425.26] loss=2.57 avg=2.39\n",
            "[1082 | 3428.23] loss=1.80 avg=2.38\n",
            "[1083 | 3431.20] loss=2.90 avg=2.39\n",
            "[1084 | 3434.15] loss=2.34 avg=2.39\n",
            "[1085 | 3437.13] loss=2.50 avg=2.39\n",
            "[1086 | 3440.11] loss=2.42 avg=2.39\n",
            "[1087 | 3443.09] loss=2.68 avg=2.39\n",
            "[1088 | 3446.08] loss=2.11 avg=2.39\n",
            "[1089 | 3449.06] loss=2.29 avg=2.39\n",
            "[1090 | 3452.05] loss=2.39 avg=2.39\n",
            "[1091 | 3455.03] loss=2.56 avg=2.39\n",
            "[1092 | 3458.01] loss=2.84 avg=2.40\n",
            "[1093 | 3461.00] loss=2.41 avg=2.40\n",
            "[1094 | 3463.98] loss=2.63 avg=2.40\n",
            "[1095 | 3466.96] loss=2.67 avg=2.40\n",
            "[1096 | 3469.95] loss=2.45 avg=2.40\n",
            "[1097 | 3472.95] loss=2.32 avg=2.40\n",
            "[1098 | 3475.94] loss=2.26 avg=2.40\n",
            "[1099 | 3478.92] loss=2.15 avg=2.40\n",
            "[1100 | 3481.90] loss=2.10 avg=2.39\n",
            "[1101 | 3484.88] loss=2.14 avg=2.39\n",
            "[1102 | 3487.85] loss=2.23 avg=2.39\n",
            "[1103 | 3490.83] loss=2.27 avg=2.39\n",
            "[1104 | 3493.81] loss=2.17 avg=2.39\n",
            "[1105 | 3496.80] loss=2.51 avg=2.39\n",
            "[1106 | 3499.78] loss=2.36 avg=2.39\n",
            "[1107 | 3502.76] loss=2.78 avg=2.39\n",
            "[1108 | 3505.73] loss=2.39 avg=2.39\n",
            "[1109 | 3508.72] loss=2.27 avg=2.39\n",
            "[1110 | 3511.72] loss=2.32 avg=2.39\n",
            "[1111 | 3514.71] loss=2.33 avg=2.39\n",
            "[1112 | 3517.69] loss=1.97 avg=2.38\n",
            "[1113 | 3520.67] loss=2.24 avg=2.38\n",
            "[1114 | 3523.65] loss=2.23 avg=2.38\n",
            "[1115 | 3526.62] loss=2.50 avg=2.38\n",
            "[1116 | 3529.60] loss=2.23 avg=2.38\n",
            "[1117 | 3532.59] loss=2.85 avg=2.39\n",
            "[1118 | 3535.57] loss=2.28 avg=2.38\n",
            "[1119 | 3538.57] loss=2.62 avg=2.39\n",
            "[1120 | 3541.56] loss=2.52 avg=2.39\n",
            "[1121 | 3544.55] loss=2.14 avg=2.39\n",
            "[1122 | 3547.54] loss=2.29 avg=2.38\n",
            "[1123 | 3550.52] loss=2.12 avg=2.38\n",
            "[1124 | 3553.50] loss=2.70 avg=2.39\n",
            "[1125 | 3556.49] loss=2.09 avg=2.38\n",
            "[1126 | 3559.48] loss=2.34 avg=2.38\n",
            "[1127 | 3562.46] loss=2.31 avg=2.38\n",
            "[1128 | 3565.45] loss=2.40 avg=2.38\n",
            "[1129 | 3568.43] loss=2.93 avg=2.39\n",
            "[1130 | 3571.42] loss=2.45 avg=2.39\n",
            "[1131 | 3574.42] loss=2.29 avg=2.39\n",
            "[1132 | 3577.43] loss=2.18 avg=2.38\n",
            "[1133 | 3580.43] loss=2.06 avg=2.38\n",
            "[1134 | 3583.41] loss=1.83 avg=2.38\n",
            "[1135 | 3586.40] loss=2.85 avg=2.38\n",
            "[1136 | 3589.39] loss=2.53 avg=2.38\n",
            "[1137 | 3592.38] loss=2.63 avg=2.38\n",
            "[1138 | 3595.34] loss=2.28 avg=2.38\n",
            "[1139 | 3598.32] loss=2.24 avg=2.38\n",
            "[1140 | 3601.30] loss=2.27 avg=2.38\n",
            "[1141 | 3604.27] loss=2.88 avg=2.39\n",
            "[1142 | 3607.34] loss=2.52 avg=2.39\n",
            "[1143 | 3610.31] loss=2.40 avg=2.39\n",
            "[1144 | 3613.38] loss=2.67 avg=2.39\n",
            "[1145 | 3616.36] loss=2.60 avg=2.39\n",
            "[1146 | 3619.33] loss=2.51 avg=2.39\n",
            "[1147 | 3622.31] loss=2.33 avg=2.39\n",
            "[1148 | 3625.28] loss=2.34 avg=2.39\n",
            "[1149 | 3628.26] loss=2.02 avg=2.39\n",
            "[1150 | 3631.24] loss=2.23 avg=2.39\n",
            "[1151 | 3634.30] loss=2.01 avg=2.38\n",
            "[1152 | 3637.28] loss=2.42 avg=2.38\n",
            "[1153 | 3640.25] loss=2.42 avg=2.38\n",
            "[1154 | 3643.23] loss=2.11 avg=2.38\n",
            "[1155 | 3646.21] loss=2.38 avg=2.38\n",
            "[1156 | 3649.26] loss=2.00 avg=2.38\n",
            "[1157 | 3652.25] loss=2.60 avg=2.38\n",
            "[1158 | 3655.21] loss=2.64 avg=2.38\n",
            "[1159 | 3658.19] loss=2.94 avg=2.39\n",
            "[1160 | 3661.16] loss=2.52 avg=2.39\n",
            "[1161 | 3664.14] loss=2.37 avg=2.39\n",
            "[1162 | 3667.12] loss=2.04 avg=2.39\n",
            "[1163 | 3670.10] loss=2.30 avg=2.38\n",
            "[1164 | 3673.16] loss=2.67 avg=2.39\n",
            "[1165 | 3676.12] loss=2.45 avg=2.39\n",
            "[1166 | 3679.19] loss=2.94 avg=2.39\n",
            "[1167 | 3682.16] loss=2.02 avg=2.39\n",
            "[1168 | 3685.16] loss=2.15 avg=2.39\n",
            "[1169 | 3688.14] loss=2.33 avg=2.39\n",
            "[1170 | 3691.11] loss=2.48 avg=2.39\n",
            "[1171 | 3694.08] loss=2.14 avg=2.39\n",
            "[1172 | 3697.05] loss=2.24 avg=2.38\n",
            "[1173 | 3700.11] loss=2.17 avg=2.38\n",
            "[1174 | 3703.08] loss=2.39 avg=2.38\n",
            "[1175 | 3706.07] loss=2.56 avg=2.38\n",
            "[1176 | 3709.14] loss=2.59 avg=2.39\n",
            "[1177 | 3712.12] loss=2.07 avg=2.38\n",
            "[1178 | 3715.09] loss=2.24 avg=2.38\n",
            "[1179 | 3718.07] loss=2.24 avg=2.38\n",
            "[1180 | 3721.13] loss=2.60 avg=2.38\n",
            "[1181 | 3724.10] loss=2.65 avg=2.38\n",
            "[1182 | 3727.07] loss=1.99 avg=2.38\n",
            "[1183 | 3730.04] loss=2.31 avg=2.38\n",
            "[1184 | 3733.01] loss=2.44 avg=2.38\n",
            "[1185 | 3735.98] loss=2.12 avg=2.38\n",
            "[1186 | 3738.95] loss=1.97 avg=2.37\n",
            "[1187 | 3741.92] loss=2.37 avg=2.37\n",
            "[1188 | 3744.90] loss=2.19 avg=2.37\n",
            "[1189 | 3747.89] loss=2.27 avg=2.37\n",
            "[1190 | 3750.87] loss=2.85 avg=2.38\n",
            "[1191 | 3753.86] loss=2.04 avg=2.37\n",
            "[1192 | 3756.84] loss=2.92 avg=2.38\n",
            "[1193 | 3759.81] loss=2.78 avg=2.38\n",
            "[1194 | 3762.80] loss=2.89 avg=2.39\n",
            "[1195 | 3765.79] loss=2.01 avg=2.38\n",
            "[1196 | 3768.76] loss=2.76 avg=2.39\n",
            "[1197 | 3771.74] loss=2.47 avg=2.39\n",
            "[1198 | 3774.71] loss=2.85 avg=2.39\n",
            "[1199 | 3777.68] loss=2.58 avg=2.39\n",
            "[1200 | 3780.67] loss=2.09 avg=2.39\n",
            "[1201 | 3783.64] loss=2.86 avg=2.40\n",
            "[1202 | 3786.63] loss=2.27 avg=2.39\n",
            "[1203 | 3789.60] loss=2.34 avg=2.39\n",
            "[1204 | 3792.58] loss=2.06 avg=2.39\n",
            "[1205 | 3795.55] loss=2.22 avg=2.39\n",
            "[1206 | 3798.52] loss=2.35 avg=2.39\n",
            "[1207 | 3801.49] loss=2.02 avg=2.38\n",
            "[1208 | 3804.48] loss=2.08 avg=2.38\n",
            "[1209 | 3807.44] loss=2.86 avg=2.39\n",
            "[1210 | 3810.41] loss=2.25 avg=2.39\n",
            "[1211 | 3813.36] loss=2.25 avg=2.38\n",
            "[1212 | 3816.34] loss=2.43 avg=2.38\n",
            "[1213 | 3819.31] loss=2.06 avg=2.38\n",
            "[1214 | 3822.28] loss=2.18 avg=2.38\n",
            "[1215 | 3825.26] loss=2.12 avg=2.38\n",
            "[1216 | 3828.23] loss=2.01 avg=2.37\n",
            "[1217 | 3831.19] loss=2.86 avg=2.38\n",
            "[1218 | 3834.16] loss=2.70 avg=2.38\n",
            "[1219 | 3837.15] loss=2.40 avg=2.38\n",
            "[1220 | 3840.14] loss=2.43 avg=2.38\n",
            "[1221 | 3843.11] loss=2.64 avg=2.38\n",
            "[1222 | 3846.10] loss=2.83 avg=2.39\n",
            "[1223 | 3849.08] loss=2.05 avg=2.39\n",
            "[1224 | 3852.06] loss=2.16 avg=2.38\n",
            "[1225 | 3855.05] loss=2.40 avg=2.38\n",
            "[1226 | 3858.03] loss=2.15 avg=2.38\n",
            "[1227 | 3861.01] loss=2.74 avg=2.38\n",
            "[1228 | 3864.00] loss=2.07 avg=2.38\n",
            "[1229 | 3866.99] loss=2.41 avg=2.38\n",
            "[1230 | 3869.98] loss=2.22 avg=2.38\n",
            "[1231 | 3872.95] loss=2.58 avg=2.38\n",
            "[1232 | 3875.94] loss=2.29 avg=2.38\n",
            "[1233 | 3878.93] loss=2.63 avg=2.38\n",
            "[1234 | 3881.90] loss=2.23 avg=2.38\n",
            "[1235 | 3884.89] loss=2.33 avg=2.38\n",
            "[1236 | 3887.89] loss=2.01 avg=2.38\n",
            "[1237 | 3890.88] loss=2.53 avg=2.38\n",
            "[1238 | 3893.87] loss=2.47 avg=2.38\n",
            "[1239 | 3896.86] loss=2.49 avg=2.38\n",
            "[1240 | 3899.83] loss=2.32 avg=2.38\n",
            "[1241 | 3902.80] loss=2.52 avg=2.38\n",
            "[1242 | 3905.79] loss=2.36 avg=2.38\n",
            "[1243 | 3908.78] loss=2.60 avg=2.38\n",
            "[1244 | 3911.76] loss=2.53 avg=2.39\n",
            "[1245 | 3914.73] loss=2.16 avg=2.38\n",
            "[1246 | 3917.73] loss=2.44 avg=2.38\n",
            "[1247 | 3920.72] loss=2.64 avg=2.39\n",
            "[1248 | 3923.69] loss=2.26 avg=2.39\n",
            "[1249 | 3926.69] loss=2.53 avg=2.39\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "(193729100) The_Mighty: I have the exact same problem, and also this, and no other suggestions..\n",
            "(193729100) The_Mighty:   1st part of my problem:        and that's it.           now I have 2nd part,             and the cd is still corrupted.  and you have to do sudo mount /dev/cdrom /mountdir\n",
            "(193729100) The_Mighty: if you follow the instructions,  just use the commands: mount /dev/cdrom /mountdir\n",
            "(193729100) dr_willis: I've done the wrong thing and now the install is corrupted.   i think we can use a backup of the filesystem\n",
            "(193729100) dr_willis: hm\n",
            "(193729110) The_Mighty: hm,  i want a second, thanks\n",
            "(193729110) dr_willis: i wouldnt be that way, because after i install the cd the installer will show you some errors and it's probably the cd and all that are broken and corrupt. but that's not a good idea, the cd may be corrupted with some broken parts inside.  we're gonna use a backup of the disk so i don't have to waste the rest of our disk space!\n",
            "(193729110) dr_willis: I know, and you shouldnt be using a cd on any device that has been messed with..\n",
            "(193729110) The_Mighty: ok i get lost\n",
            "(193729110) Dr_Willis: hai, thanks :)\n",
            "(193729220) dr_willis:   mount -l /dev/cdrom /mountdir /mnt/local or  mount -a /dev/cdrom -o loop /mountdir /mnt/local \n",
            "(193729220) The_Mighty: mount\n",
            "(193729280) dr_willis: thanks it is a good thing if you have to run, if you dont, there's a better way\n",
            "\n",
            "\n",
            "(1277536680) DIMG-: What do you mean by reboot\n",
            "(1277562740) DIMG-: Yes\n",
            "(1277562740) DIMG-: Yes, then\n",
            "(1277536700) DIMG-: You want to restart Xubuntu to update, but are not sure, this is hard to do, is there an install CD handy\n",
            "(1277562740) DIMG-: I can't say anything yet..\n",
            "(1277562740) dst: you want to restart Xubuntu to update, but are not sure, this is hard to do, is there an install CD handy\n",
            "(1277537020) dst: I don't think there's any linux distro without at least six install discs, is that the case? it's all Ubuntu, and it's pretty darned easy\n",
            "(1277537020) dst: so I have no idea what you mean, and why you would try this\n",
            "(1277547040) dst: you wouldnt need to download anything\n",
            "(1277567020) dst: there is a ubuntu-archive-linux-source package you use to get it installed, and yes, that will work fine\n",
            "(1277548080) dst: the easiest way is to use this one\n",
            "(1277548160) dst: if you are getting this error when trying to make an initrd to reinstall then use the package you downloaded\n",
            "(1277549160) dst: yes, you can download, but I dont know if it is worth it\n",
            "(1277549160) dst: http://unetbootin.sourceforge.net/download\n",
            "(1277549540) dst: try this\n",
            "(1277549640) DIMG-: it worked, it did\n",
            "(1277549700) dst: yes, if you are running the installer from a live cd it has a menu/menu-entry like initrd.img\n",
            "(1277571020) dst: I assume it is not worth the effort. just look\n",
            "(1277517140) dst: you're not the first one\n",
            "(1280108280) DIMG-: I have seen other users, that say they do NOT know how to\n",
            "(1280108340) dst: I dont really know what you did, but you have been using Xubuntu for a VERY long time. Ubuntu has gotten a lot better, and if it makes that much difference. or how to make an initrd to restart from the live cd and re-install Xubuntu, what\n",
            "\n",
            "[1250 | 3967.26] loss=2.58 avg=2.39\n",
            "[1251 | 3970.33] loss=2.22 avg=2.39\n",
            "[1252 | 3973.31] loss=2.55 avg=2.39\n",
            "[1253 | 3976.30] loss=2.53 avg=2.39\n",
            "[1254 | 3979.30] loss=2.31 avg=2.39\n",
            "[1255 | 3982.27] loss=2.60 avg=2.39\n",
            "[1256 | 3985.24] loss=2.09 avg=2.39\n",
            "[1257 | 3988.22] loss=2.53 avg=2.39\n",
            "[1258 | 3991.28] loss=2.00 avg=2.39\n",
            "[1259 | 3994.26] loss=2.08 avg=2.38\n",
            "[1260 | 3997.26] loss=2.70 avg=2.39\n",
            "[1261 | 4000.24] loss=2.33 avg=2.39\n",
            "[1262 | 4003.22] loss=1.97 avg=2.38\n",
            "[1263 | 4006.30] loss=2.56 avg=2.38\n",
            "[1264 | 4009.27] loss=2.46 avg=2.38\n",
            "[1265 | 4012.26] loss=2.03 avg=2.38\n",
            "[1266 | 4015.32] loss=2.39 avg=2.38\n",
            "[1267 | 4018.39] loss=1.95 avg=2.38\n",
            "[1268 | 4021.37] loss=2.01 avg=2.37\n",
            "[1269 | 4024.34] loss=2.34 avg=2.37\n",
            "[1270 | 4027.32] loss=2.35 avg=2.37\n",
            "[1271 | 4030.29] loss=2.00 avg=2.37\n",
            "[1272 | 4033.28] loss=2.56 avg=2.37\n",
            "[1273 | 4036.27] loss=1.52 avg=2.36\n",
            "[1274 | 4039.26] loss=2.40 avg=2.36\n",
            "[1275 | 4042.23] loss=2.46 avg=2.36\n",
            "[1276 | 4045.21] loss=2.26 avg=2.36\n",
            "[1277 | 4048.20] loss=2.63 avg=2.36\n",
            "[1278 | 4051.18] loss=2.23 avg=2.36\n",
            "[1279 | 4054.16] loss=2.14 avg=2.36\n",
            "[1280 | 4057.14] loss=2.31 avg=2.36\n",
            "[1281 | 4060.12] loss=2.07 avg=2.36\n",
            "[1282 | 4063.18] loss=2.46 avg=2.36\n",
            "[1283 | 4066.16] loss=2.03 avg=2.36\n",
            "[1284 | 4069.14] loss=2.72 avg=2.36\n",
            "[1285 | 4072.12] loss=2.34 avg=2.36\n",
            "[1286 | 4075.10] loss=1.92 avg=2.35\n",
            "[1287 | 4078.25] loss=2.43 avg=2.36\n",
            "[1288 | 4081.23] loss=2.60 avg=2.36\n",
            "[1289 | 4084.20] loss=2.42 avg=2.36\n",
            "[1290 | 4087.26] loss=2.63 avg=2.36\n",
            "[1291 | 4090.25] loss=2.27 avg=2.36\n",
            "[1292 | 4093.31] loss=2.46 avg=2.36\n",
            "[1293 | 4096.29] loss=2.42 avg=2.36\n",
            "[1294 | 4099.27] loss=2.28 avg=2.36\n",
            "[1295 | 4102.25] loss=2.13 avg=2.36\n",
            "[1296 | 4105.22] loss=2.45 avg=2.36\n",
            "[1297 | 4108.20] loss=2.40 avg=2.36\n",
            "[1298 | 4111.16] loss=3.01 avg=2.37\n",
            "[1299 | 4114.15] loss=2.34 avg=2.37\n",
            "[1300 | 4117.13] loss=2.42 avg=2.37\n",
            "[1301 | 4120.12] loss=2.29 avg=2.37\n",
            "[1302 | 4123.09] loss=1.95 avg=2.36\n",
            "[1303 | 4126.07] loss=2.74 avg=2.37\n",
            "[1304 | 4129.05] loss=2.36 avg=2.37\n",
            "[1305 | 4132.03] loss=2.17 avg=2.36\n",
            "[1306 | 4135.10] loss=2.25 avg=2.36\n",
            "[1307 | 4138.07] loss=1.91 avg=2.36\n",
            "[1308 | 4141.04] loss=2.24 avg=2.36\n",
            "[1309 | 4144.02] loss=2.28 avg=2.36\n",
            "[1310 | 4147.01] loss=2.52 avg=2.36\n",
            "[1311 | 4149.99] loss=2.37 avg=2.36\n",
            "[1312 | 4152.97] loss=2.36 avg=2.36\n",
            "[1313 | 4155.94] loss=2.50 avg=2.36\n",
            "[1314 | 4158.93] loss=2.51 avg=2.36\n",
            "[1315 | 4161.89] loss=2.38 avg=2.36\n",
            "[1316 | 4164.96] loss=2.08 avg=2.36\n",
            "[1317 | 4167.95] loss=2.10 avg=2.36\n",
            "[1318 | 4170.93] loss=2.38 avg=2.36\n",
            "[1319 | 4173.91] loss=2.29 avg=2.35\n",
            "[1320 | 4176.88] loss=2.20 avg=2.35\n",
            "[1321 | 4179.85] loss=2.03 avg=2.35\n",
            "[1322 | 4182.82] loss=2.20 avg=2.35\n",
            "[1323 | 4185.79] loss=2.45 avg=2.35\n",
            "[1324 | 4188.76] loss=2.21 avg=2.35\n",
            "[1325 | 4191.74] loss=2.64 avg=2.35\n",
            "[1326 | 4194.73] loss=2.30 avg=2.35\n",
            "[1327 | 4197.70] loss=2.23 avg=2.35\n",
            "[1328 | 4200.67] loss=2.28 avg=2.35\n",
            "[1329 | 4203.65] loss=1.78 avg=2.34\n",
            "[1330 | 4206.60] loss=2.61 avg=2.35\n",
            "[1331 | 4209.57] loss=2.00 avg=2.34\n",
            "[1332 | 4212.52] loss=2.74 avg=2.35\n",
            "[1333 | 4215.49] loss=2.38 avg=2.35\n",
            "[1334 | 4218.46] loss=1.83 avg=2.34\n",
            "[1335 | 4221.43] loss=2.41 avg=2.34\n",
            "[1336 | 4224.40] loss=2.29 avg=2.34\n",
            "[1337 | 4227.37] loss=2.24 avg=2.34\n",
            "[1338 | 4230.34] loss=2.23 avg=2.34\n",
            "[1339 | 4233.32] loss=2.28 avg=2.34\n",
            "[1340 | 4236.30] loss=2.57 avg=2.34\n",
            "[1341 | 4239.28] loss=3.00 avg=2.35\n",
            "[1342 | 4242.26] loss=2.27 avg=2.35\n",
            "[1343 | 4245.24] loss=2.16 avg=2.35\n",
            "[1344 | 4248.21] loss=2.27 avg=2.34\n",
            "[1345 | 4251.19] loss=2.41 avg=2.35\n",
            "[1346 | 4254.20] loss=2.50 avg=2.35\n",
            "[1347 | 4257.18] loss=2.31 avg=2.35\n",
            "[1348 | 4260.15] loss=2.38 avg=2.35\n",
            "[1349 | 4263.14] loss=2.29 avg=2.35\n",
            "[1350 | 4266.11] loss=2.43 avg=2.35\n",
            "[1351 | 4269.11] loss=2.01 avg=2.34\n",
            "[1352 | 4272.10] loss=2.13 avg=2.34\n",
            "[1353 | 4275.08] loss=2.12 avg=2.34\n",
            "[1354 | 4278.05] loss=2.45 avg=2.34\n",
            "[1355 | 4281.03] loss=2.24 avg=2.34\n",
            "[1356 | 4284.01] loss=2.36 avg=2.34\n",
            "[1357 | 4286.99] loss=1.94 avg=2.34\n",
            "[1358 | 4289.98] loss=2.56 avg=2.34\n",
            "[1359 | 4292.97] loss=2.25 avg=2.34\n",
            "[1360 | 4295.95] loss=2.90 avg=2.34\n",
            "[1361 | 4298.94] loss=2.12 avg=2.34\n",
            "[1362 | 4301.94] loss=2.16 avg=2.34\n",
            "[1363 | 4304.93] loss=1.81 avg=2.33\n",
            "[1364 | 4307.90] loss=2.12 avg=2.33\n",
            "[1365 | 4310.89] loss=2.13 avg=2.33\n",
            "[1366 | 4313.86] loss=2.33 avg=2.33\n",
            "[1367 | 4316.86] loss=1.99 avg=2.33\n",
            "[1368 | 4319.83] loss=2.27 avg=2.32\n",
            "[1369 | 4322.83] loss=2.32 avg=2.32\n",
            "[1370 | 4325.81] loss=2.15 avg=2.32\n",
            "[1371 | 4328.78] loss=2.10 avg=2.32\n",
            "[1372 | 4331.77] loss=2.44 avg=2.32\n",
            "[1373 | 4334.75] loss=2.46 avg=2.32\n",
            "[1374 | 4337.73] loss=2.73 avg=2.33\n",
            "[1375 | 4340.71] loss=2.62 avg=2.33\n",
            "[1376 | 4343.69] loss=3.04 avg=2.34\n",
            "[1377 | 4346.68] loss=2.37 avg=2.34\n",
            "[1378 | 4349.67] loss=2.20 avg=2.34\n",
            "[1379 | 4352.65] loss=2.58 avg=2.34\n",
            "[1380 | 4355.64] loss=2.06 avg=2.34\n",
            "[1381 | 4358.62] loss=2.80 avg=2.34\n",
            "[1382 | 4361.60] loss=2.19 avg=2.34\n",
            "[1383 | 4364.59] loss=2.21 avg=2.34\n",
            "[1384 | 4367.57] loss=2.07 avg=2.34\n",
            "[1385 | 4370.57] loss=1.87 avg=2.33\n",
            "[1386 | 4373.56] loss=2.01 avg=2.33\n",
            "[1387 | 4376.56] loss=2.60 avg=2.33\n",
            "[1388 | 4379.63] loss=2.27 avg=2.33\n",
            "[1389 | 4382.62] loss=2.09 avg=2.33\n",
            "[1390 | 4385.62] loss=1.81 avg=2.32\n",
            "[1391 | 4388.60] loss=2.71 avg=2.33\n",
            "[1392 | 4391.58] loss=2.42 avg=2.33\n",
            "[1393 | 4394.56] loss=2.32 avg=2.33\n",
            "[1394 | 4397.53] loss=2.24 avg=2.33\n",
            "[1395 | 4400.51] loss=2.08 avg=2.32\n",
            "[1396 | 4403.50] loss=2.02 avg=2.32\n",
            "[1397 | 4406.48] loss=1.79 avg=2.32\n",
            "[1398 | 4409.45] loss=2.42 avg=2.32\n",
            "[1399 | 4412.44] loss=2.88 avg=2.32\n",
            "[1400 | 4415.50] loss=2.81 avg=2.33\n",
            "[1401 | 4418.57] loss=2.29 avg=2.33\n",
            "[1402 | 4421.56] loss=2.29 avg=2.33\n",
            "[1403 | 4424.53] loss=2.40 avg=2.33\n",
            "[1404 | 4427.51] loss=2.28 avg=2.33\n",
            "[1405 | 4430.47] loss=2.49 avg=2.33\n",
            "[1406 | 4433.53] loss=2.70 avg=2.33\n",
            "[1407 | 4436.50] loss=2.24 avg=2.33\n",
            "[1408 | 4439.48] loss=2.27 avg=2.33\n",
            "[1409 | 4442.44] loss=1.95 avg=2.33\n",
            "[1410 | 4445.42] loss=2.45 avg=2.33\n",
            "[1411 | 4448.39] loss=2.72 avg=2.33\n",
            "[1412 | 4451.37] loss=2.34 avg=2.33\n",
            "[1413 | 4454.35] loss=2.22 avg=2.33\n",
            "[1414 | 4457.33] loss=2.81 avg=2.34\n",
            "[1415 | 4460.29] loss=2.38 avg=2.34\n",
            "[1416 | 4463.28] loss=1.53 avg=2.33\n",
            "[1417 | 4466.27] loss=2.52 avg=2.33\n",
            "[1418 | 4469.24] loss=2.15 avg=2.33\n",
            "[1419 | 4472.37] loss=2.45 avg=2.33\n",
            "[1420 | 4475.35] loss=2.80 avg=2.33\n",
            "[1421 | 4478.33] loss=2.32 avg=2.33\n",
            "[1422 | 4481.31] loss=2.29 avg=2.33\n",
            "[1423 | 4484.29] loss=1.68 avg=2.33\n",
            "[1424 | 4487.27] loss=2.16 avg=2.33\n",
            "[1425 | 4490.25] loss=2.05 avg=2.32\n",
            "[1426 | 4493.23] loss=2.41 avg=2.32\n",
            "[1427 | 4496.22] loss=2.65 avg=2.33\n",
            "[1428 | 4499.19] loss=2.08 avg=2.32\n",
            "[1429 | 4502.26] loss=2.33 avg=2.32\n",
            "[1430 | 4505.24] loss=2.38 avg=2.32\n",
            "[1431 | 4508.29] loss=2.65 avg=2.33\n",
            "[1432 | 4511.38] loss=2.36 avg=2.33\n",
            "[1433 | 4514.35] loss=1.98 avg=2.32\n",
            "[1434 | 4517.33] loss=2.15 avg=2.32\n",
            "[1435 | 4520.30] loss=2.40 avg=2.32\n",
            "[1436 | 4523.27] loss=2.15 avg=2.32\n",
            "[1437 | 4526.34] loss=2.57 avg=2.32\n",
            "[1438 | 4529.31] loss=1.98 avg=2.32\n",
            "[1439 | 4532.29] loss=2.89 avg=2.33\n",
            "[1440 | 4535.36] loss=2.36 avg=2.33\n",
            "[1441 | 4538.34] loss=1.97 avg=2.32\n",
            "[1442 | 4541.48] loss=2.68 avg=2.33\n",
            "[1443 | 4544.55] loss=2.36 avg=2.33\n",
            "[1444 | 4547.54] loss=2.19 avg=2.33\n",
            "[1445 | 4550.52] loss=2.32 avg=2.33\n",
            "[1446 | 4553.51] loss=1.78 avg=2.32\n",
            "[1447 | 4556.66] loss=2.64 avg=2.32\n",
            "[1448 | 4559.65] loss=2.03 avg=2.32\n",
            "[1449 | 4562.72] loss=2.20 avg=2.32\n",
            "[1450 | 4565.69] loss=2.53 avg=2.32\n",
            "[1451 | 4568.68] loss=2.30 avg=2.32\n",
            "[1452 | 4571.66] loss=2.62 avg=2.32\n",
            "[1453 | 4574.63] loss=1.87 avg=2.32\n",
            "[1454 | 4577.61] loss=2.73 avg=2.32\n",
            "[1455 | 4580.60] loss=2.47 avg=2.33\n",
            "[1456 | 4583.59] loss=2.32 avg=2.33\n",
            "[1457 | 4586.56] loss=2.58 avg=2.33\n",
            "[1458 | 4589.54] loss=1.93 avg=2.32\n",
            "[1459 | 4592.51] loss=2.75 avg=2.33\n",
            "[1460 | 4595.48] loss=2.29 avg=2.33\n",
            "[1461 | 4598.46] loss=2.98 avg=2.33\n",
            "[1462 | 4601.43] loss=2.33 avg=2.33\n",
            "[1463 | 4604.39] loss=2.54 avg=2.34\n",
            "[1464 | 4607.36] loss=1.97 avg=2.33\n",
            "[1465 | 4610.32] loss=2.33 avg=2.33\n",
            "[1466 | 4613.29] loss=2.43 avg=2.33\n",
            "[1467 | 4616.27] loss=2.22 avg=2.33\n",
            "[1468 | 4619.23] loss=2.81 avg=2.34\n",
            "[1469 | 4622.20] loss=2.54 avg=2.34\n",
            "[1470 | 4625.16] loss=2.64 avg=2.34\n",
            "[1471 | 4628.13] loss=2.44 avg=2.34\n",
            "[1472 | 4631.10] loss=2.00 avg=2.34\n",
            "[1473 | 4634.08] loss=2.38 avg=2.34\n",
            "[1474 | 4637.06] loss=2.82 avg=2.35\n",
            "[1475 | 4640.02] loss=2.21 avg=2.34\n",
            "[1476 | 4643.01] loss=2.28 avg=2.34\n",
            "[1477 | 4645.98] loss=2.15 avg=2.34\n",
            "[1478 | 4648.96] loss=1.88 avg=2.34\n",
            "[1479 | 4651.93] loss=2.50 avg=2.34\n",
            "[1480 | 4654.92] loss=2.50 avg=2.34\n",
            "[1481 | 4657.90] loss=2.08 avg=2.34\n",
            "[1482 | 4660.89] loss=2.36 avg=2.34\n",
            "[1483 | 4663.87] loss=2.09 avg=2.33\n",
            "[1484 | 4666.86] loss=1.99 avg=2.33\n",
            "[1485 | 4669.85] loss=2.21 avg=2.33\n",
            "[1486 | 4672.83] loss=2.44 avg=2.33\n",
            "[1487 | 4675.81] loss=2.18 avg=2.33\n",
            "[1488 | 4678.79] loss=2.33 avg=2.33\n",
            "[1489 | 4681.77] loss=2.63 avg=2.33\n",
            "[1490 | 4684.76] loss=2.70 avg=2.34\n",
            "[1491 | 4687.74] loss=2.81 avg=2.34\n",
            "[1492 | 4690.73] loss=2.71 avg=2.35\n",
            "[1493 | 4693.72] loss=2.58 avg=2.35\n",
            "[1494 | 4696.71] loss=2.08 avg=2.34\n",
            "[1495 | 4699.70] loss=2.17 avg=2.34\n",
            "[1496 | 4702.69] loss=2.30 avg=2.34\n",
            "[1497 | 4705.67] loss=2.18 avg=2.34\n",
            "[1498 | 4708.65] loss=2.14 avg=2.34\n",
            "[1499 | 4711.63] loss=2.40 avg=2.34\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "PP, if not then not. If you are on the system to which the usb stick is attached, make sure to use 'sudo fdisk -l' and copy the list of devices to a second screen\n",
            "\n",
            "\n",
            "(1215274880) fazbq: is there any method to disable it's own boot device ?\n",
            "(1215274940) fazbq: is there any method to disable it's own boot device ... like the usb hard drive drive or pc card ..\n",
            "(1215277500) fazbq: i just want to make sure there is a way it wont give an error when i try to install ubuntu on my pc with the wubi, or the dmesg and the bios or whatever driver it thinks i should use\n",
            "(1215277620) fazbq: hm\n",
            "(1215277620) fazbq: that means that its on bootup now\n",
            "(1215277620) bimberi: you want ubuntu installed on the same wubi drive as your dmesg?\n",
            "(1215277620) bimberi: yes, but it works only on windows\n",
            "(1215277680) fazbq: hm bimbberi: i have wubi installed.\n",
            "(1215277680) bimberi: did it take much in the way of effort?\n",
            "(1215277740) bimberi: so that you can use it as it will be used by default in windows?\n",
            "(1215277800) bimberi: it will most likely, and is a win7 boot\n",
            "(1215277920) bimberi: try to do it in wubi first\n",
            "(1215277980) bimberi: then install it from there\n",
            "(1215970000) bimberi: it seems that there are a lot of USB drives and flash drive's out there, I can't imagine who is the biggest manufacturer\n",
            "(1215800000) bimberi: you may have seen that a few months ago... I noticed that the one I got from my linux store had a different usb slot than the original wubi box, so I asked around a while about the differences. I got the wubi 2nd slot instead of the 1st, and after talking with the guys there, they confirmed that the 1st slot is indeed a dvd drive, and as such, all windows usb drives do, with the same driver, but it seems that the 2nd is the biggest manufacturers, as i was not able to find anything on my search\n",
            "(1215800000) bimberi: you will need to be specific with your model, however since they were probably not on my search, it is most likely that they also exist in general.\n",
            "(1215800000) bimberi: also, you may just be able to install wubi with the original drivers, but it is hard to track down the driver from a generic store or from a usb.com product\n",
            "(1215800000) bimberi: well I don't think that there is one for linux, but you have to check your search results to find out which one is there, or, worse yet, find out what drivers you get from the repos and whether it has a newer version.\n",
            "\n",
            "\n",
            "(1283629360) alex_: !xterm > mouz.\n",
            "(1283629720) mouz: is there an option for my display?\n",
            "(1283632740) mouz: ok, and is there a way that i can mount that?  I havent set that up.\n",
            "(1283632580) i_was_absoaboo: you're not on ubuntu are you ?\n",
            "(1283632580) mouz: yes,\n",
            "(1283633700) MOUZ: ok, i'll check with you\n",
            "(1283634300) mouz: ok,\n",
            "(1283634600) MOUZ: oh i see right, ill check it out...\n",
            "\n",
            "\n",
            "(1284569140) dannymc: what are the pros and cons of Ubuntu for Linux ?\n",
            "(1284569140) dannymc: for non-linux ?\n",
            "(1284569140) dannymc: yes\n",
            "(1284569140) dannymc: yes\n",
            "(1284569140) dannymc: my laptop works but i cant type\n",
            "(1284569200) dannymc: yes\n",
            "(1284569200) dannymc: i have 4 gb of ram and 880 mmio\n",
            "(1284569200) dannymc: i have 8 xmb\n",
            "(1284569200) dannymc:\n",
            "\n",
            "[1500 | 4751.87] loss=2.56 avg=2.34\n",
            "Saving checkpoint/run1/model-1501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V80Lwn8QW_Si",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e8e5b1c7-142c-4054-e345-f5ee203a90e8"
      },
      "source": [
        "!PYTHONPATH=src ./train.py --dataset ubuntu-npz/ --sample_every=250 --learning_rate 0.0001 --stop_after 3501 --model_name 345M"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:73: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:76: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-10-13 02:18:45.552149: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-10-13 02:18:45.552470: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1787480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-10-13 02:18:45.552509: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-10-13 02:18:45.554721: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-10-13 02:18:45.621867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 02:18:45.622791: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x6f33180 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-10-13 02:18:45.622829: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-10-13 02:18:45.623066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 02:18:45.623786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-10-13 02:18:45.624227: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-13 02:18:45.625826: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-10-13 02:18:45.627422: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-10-13 02:18:45.627793: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-10-13 02:18:45.629847: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-10-13 02:18:45.631332: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-10-13 02:18:45.635629: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-10-13 02:18:45.635784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 02:18:45.636672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 02:18:45.637350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-10-13 02:18:45.637426: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-13 02:18:45.639046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-10-13 02:18:45.639082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-10-13 02:18:45.639096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-10-13 02:18:45.639274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 02:18:45.640062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 02:18:45.640790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From ./train.py:77: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From ./train.py:100: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:113: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
            "Instructions for updating:\n",
            "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:89: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:120: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:122: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:125: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:129: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "Loading checkpoint checkpoint/run1/model-1746\n",
            "Loading dataset...\n",
            "100% 8/8 [00:08<00:00,  1.05s/it]\n",
            "dataset has 233929304 tokens\n",
            "Training...\n",
            "2019-10-13 02:19:55.583207: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "[1747 | 15.91] loss=2.22 avg=2.22\n",
            "[1748 | 18.94] loss=2.23 avg=2.23\n",
            "[1749 | 21.96] loss=2.11 avg=2.19\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " C.\n",
            "(1323684880) hkreps: and if you want to compile your own kernel, you can't...just compile it...it won't run.\n",
            "(1323684940) hkreps: it does?...what do you mean you are calling it a 'vmware virtual machine'? do you mean a virtual machine?\n",
            "(1323685000) hkreps: did you do something to your kernel to make it run? I am not sure, and could be wrong.\n",
            "(1323685000) hkreps: did you install it from a DVD? I would be quite surprised if you did.\n",
            "(1323685500) hkreps: why, for the love of God, don't you use a DVD?\n",
            "(1323685560) hkreps: a DVD is only a shell\n",
            "(1323685620) hkreps: you use a VM.\n",
            "(1323738900) hkreps: if you put it in a DVD?\n",
            "(1323739120) hkreps: it's really easy...for me, it's a bit more work :/\n",
            "(1323739120) hkreps: just give the DVD to your friend :D\n",
            "(1323739120) hkreps: and he can just boot it off the DVD? :D\n",
            "(1323739120) chris@chris___: I tried that, and it didn't work either, but if you don't want to make an ISO, why use a DVD?\n",
            "(1323739180) hkreps: to set up a virtual machine, you're in software.\n",
            "(1323747020) hkreps: he should have booted off a DVD.\n",
            "\n",
            "\n",
            "(1164783760) j_phr0m: hi everyone, is there any way to check my /etc/sudo_conf file for changes during shutdown?\n",
            "(1164783760) j_phr0m: I can't access my /etc/sudo_conf file from the terminal, why?\n",
            "(1164783820) j_phr0m: anyone on this computer?\n",
            "(1164783940) j_phr0m: I have /usr/share/ubuntu-ui/ubuntu-ui-5.6\n",
            "(1164784000) j_phr0m: I can't access it!\n",
            "(1164784000) j_phr0m: I just added it with another computer?\n",
            "(1164784240) j_phr0m: yes, i'll give that a go\n",
            "(1164784240) j_phr0m: I'd rather not have to restart, it should be running now. It's the most important thing I want to be doing right now.  I can't find the GUI, and it won't boot, so i couldn't do anything.\n",
            "(1164784000) j_phr0m: oh yeah, it's working now, I'm very pleased!\n",
            "(1164784240) j_phr0m: should i get a new computer? Because the last time i've worked on something there was a big mess.\n",
            "(1164784240) j_phr0m: thanks, will wait, hopefully if it does it will get fixed. :)\n",
            "(1164784240) j_phr0m: it's working now!\n",
            "(1164784240) j_phr0m: I just checked /etc/sudo_conf, and it's a bit messed up, just want to correct this if there's even a hint of it changing!\n",
            "(1164784300) j_phr0m: how could someone correct my /etc/sudo_conf?\n",
            "(1164784360) j_phr0m: well... there must be some file in /etc/sudo_conf that is set to NO\n",
            "(1164784420) j_phr0m: ah...that makes a lot of sense.  I've been messing around with how the CLI works.  That might be too complex for some people and not be enough for me.\n",
            "(1164784540) j_phr0m: any idea why it wasn't fixed?\n",
            "(1164784600) j_phr0m: it should be a line like ' NO\n",
            "(1164784600) j_phr0m: hmm...I was just reading the page, but there is a lot more to help!\n",
            "(1164784660) j_phr0m: I just tried adding sudo to start, then it wouldn't run...\n",
            "(1164784660) j\n",
            "\n",
            "[1750 | 68.74] loss=2.30 avg=2.21\n",
            "[1751 | 71.78] loss=2.27 avg=2.23\n",
            "[1752 | 74.88] loss=2.20 avg=2.22\n",
            "[1753 | 77.90] loss=1.79 avg=2.16\n",
            "[1754 | 80.91] loss=2.19 avg=2.16\n",
            "[1755 | 83.95] loss=2.36 avg=2.18\n",
            "[1756 | 87.10] loss=2.38 avg=2.20\n",
            "[1757 | 90.21] loss=1.57 avg=2.14\n",
            "[1758 | 93.28] loss=2.13 avg=2.14\n",
            "[1759 | 96.36] loss=1.63 avg=2.10\n",
            "[1760 | 99.42] loss=2.62 avg=2.14\n",
            "[1761 | 102.51] loss=2.33 avg=2.15\n",
            "[1762 | 105.67] loss=2.31 avg=2.16\n",
            "[1763 | 108.76] loss=2.79 avg=2.20\n",
            "[1764 | 111.94] loss=3.08 avg=2.26\n",
            "[1765 | 115.00] loss=2.17 avg=2.25\n",
            "[1766 | 118.08] loss=2.15 avg=2.25\n",
            "[1767 | 121.20] loss=2.73 avg=2.27\n",
            "[1768 | 124.36] loss=2.24 avg=2.27\n",
            "[1769 | 127.53] loss=2.33 avg=2.27\n",
            "[1770 | 130.64] loss=1.95 avg=2.26\n",
            "[1771 | 133.81] loss=2.65 avg=2.28\n",
            "[1772 | 136.88] loss=1.75 avg=2.25\n",
            "[1773 | 139.98] loss=1.77 avg=2.23\n",
            "[1774 | 143.08] loss=2.52 avg=2.24\n",
            "[1775 | 146.37] loss=2.09 avg=2.24\n",
            "[1776 | 149.47] loss=2.29 avg=2.24\n",
            "[1777 | 152.55] loss=2.81 avg=2.26\n",
            "[1778 | 155.71] loss=2.18 avg=2.26\n",
            "[1779 | 158.90] loss=2.22 avg=2.26\n",
            "[1780 | 162.07] loss=2.04 avg=2.25\n",
            "[1781 | 165.14] loss=2.68 avg=2.26\n",
            "[1782 | 168.38] loss=2.29 avg=2.26\n",
            "[1783 | 171.64] loss=2.22 avg=2.26\n",
            "[1784 | 174.80] loss=2.30 avg=2.26\n",
            "[1785 | 177.91] loss=2.68 avg=2.28\n",
            "[1786 | 181.00] loss=2.54 avg=2.29\n",
            "[1787 | 184.17] loss=2.37 avg=2.29\n",
            "[1788 | 187.26] loss=2.20 avg=2.29\n",
            "[1789 | 190.32] loss=2.23 avg=2.28\n",
            "[1790 | 193.57] loss=2.24 avg=2.28\n",
            "[1791 | 196.75] loss=1.85 avg=2.27\n",
            "[1792 | 199.92] loss=2.81 avg=2.29\n",
            "[1793 | 203.01] loss=2.42 avg=2.29\n",
            "[1794 | 206.17] loss=2.53 avg=2.29\n",
            "[1795 | 209.26] loss=2.26 avg=2.29\n",
            "[1796 | 212.42] loss=2.46 avg=2.30\n",
            "[1797 | 215.57] loss=2.12 avg=2.29\n",
            "[1798 | 218.73] loss=2.41 avg=2.30\n",
            "[1799 | 221.90] loss=2.22 avg=2.29\n",
            "[1800 | 225.06] loss=2.37 avg=2.30\n",
            "[1801 | 228.14] loss=2.69 avg=2.31\n",
            "[1802 | 231.23] loss=2.28 avg=2.31\n",
            "[1803 | 234.33] loss=2.06 avg=2.30\n",
            "[1804 | 237.49] loss=2.26 avg=2.30\n",
            "[1805 | 240.77] loss=2.37 avg=2.30\n",
            "[1806 | 243.86] loss=2.13 avg=2.30\n",
            "[1807 | 246.96] loss=2.55 avg=2.30\n",
            "[1808 | 250.22] loss=2.12 avg=2.30\n",
            "[1809 | 253.32] loss=2.27 avg=2.30\n",
            "[1810 | 256.43] loss=2.52 avg=2.30\n",
            "[1811 | 259.63] loss=2.17 avg=2.30\n",
            "[1812 | 262.79] loss=2.70 avg=2.31\n",
            "[1813 | 265.97] loss=2.47 avg=2.31\n",
            "[1814 | 269.11] loss=2.34 avg=2.31\n",
            "[1815 | 272.25] loss=2.64 avg=2.32\n",
            "[1816 | 275.43] loss=2.27 avg=2.32\n",
            "[1817 | 278.51] loss=2.12 avg=2.31\n",
            "[1818 | 281.53] loss=2.81 avg=2.32\n",
            "[1819 | 284.58] loss=3.17 avg=2.34\n",
            "[1820 | 287.62] loss=2.54 avg=2.34\n",
            "[1821 | 290.66] loss=2.04 avg=2.34\n",
            "[1822 | 293.70] loss=2.87 avg=2.35\n",
            "[1823 | 296.76] loss=2.43 avg=2.35\n",
            "[1824 | 299.82] loss=1.86 avg=2.34\n",
            "[1825 | 302.87] loss=2.25 avg=2.34\n",
            "[1826 | 305.93] loss=2.19 avg=2.34\n",
            "[1827 | 308.99] loss=2.70 avg=2.34\n",
            "[1828 | 312.06] loss=2.91 avg=2.35\n",
            "[1829 | 315.13] loss=2.41 avg=2.35\n",
            "[1830 | 318.19] loss=2.96 avg=2.36\n",
            "[1831 | 321.26] loss=2.49 avg=2.37\n",
            "[1832 | 324.32] loss=2.05 avg=2.36\n",
            "[1833 | 327.37] loss=2.51 avg=2.36\n",
            "[1834 | 330.42] loss=2.08 avg=2.36\n",
            "[1835 | 333.49] loss=2.16 avg=2.36\n",
            "[1836 | 336.53] loss=2.64 avg=2.36\n",
            "[1837 | 339.59] loss=2.11 avg=2.36\n",
            "[1838 | 342.65] loss=2.97 avg=2.37\n",
            "[1839 | 345.72] loss=2.19 avg=2.36\n",
            "[1840 | 348.78] loss=2.73 avg=2.37\n",
            "[1841 | 351.84] loss=2.54 avg=2.37\n",
            "[1842 | 354.90] loss=2.31 avg=2.37\n",
            "[1843 | 357.98] loss=2.17 avg=2.37\n",
            "[1844 | 361.05] loss=2.10 avg=2.36\n",
            "[1845 | 364.10] loss=2.28 avg=2.36\n",
            "[1846 | 367.18] loss=2.67 avg=2.37\n",
            "[1847 | 370.22] loss=2.19 avg=2.36\n",
            "[1848 | 373.29] loss=2.28 avg=2.36\n",
            "[1849 | 376.36] loss=2.18 avg=2.36\n",
            "[1850 | 379.40] loss=2.32 avg=2.36\n",
            "[1851 | 382.49] loss=2.63 avg=2.36\n",
            "[1852 | 385.56] loss=2.19 avg=2.36\n",
            "[1853 | 388.64] loss=2.23 avg=2.36\n",
            "[1854 | 391.71] loss=2.18 avg=2.36\n",
            "[1855 | 394.78] loss=2.47 avg=2.36\n",
            "[1856 | 397.85] loss=2.39 avg=2.36\n",
            "[1857 | 400.92] loss=2.32 avg=2.36\n",
            "[1858 | 403.95] loss=2.36 avg=2.36\n",
            "[1859 | 407.02] loss=2.04 avg=2.35\n",
            "[1860 | 410.07] loss=2.01 avg=2.35\n",
            "[1861 | 413.12] loss=2.23 avg=2.35\n",
            "[1862 | 416.20] loss=3.02 avg=2.36\n",
            "[1863 | 419.26] loss=2.32 avg=2.36\n",
            "[1864 | 422.33] loss=2.20 avg=2.35\n",
            "[1865 | 425.38] loss=2.35 avg=2.35\n",
            "[1866 | 428.47] loss=2.48 avg=2.35\n",
            "[1867 | 431.58] loss=2.40 avg=2.36\n",
            "[1868 | 434.75] loss=2.17 avg=2.35\n",
            "[1869 | 438.03] loss=2.82 avg=2.36\n",
            "[1870 | 441.15] loss=2.34 avg=2.36\n",
            "[1871 | 444.25] loss=2.18 avg=2.36\n",
            "[1872 | 447.54] loss=2.78 avg=2.36\n",
            "[1873 | 450.65] loss=1.82 avg=2.35\n",
            "[1874 | 453.75] loss=2.33 avg=2.35\n",
            "[1875 | 456.89] loss=2.48 avg=2.36\n",
            "[1876 | 459.98] loss=1.96 avg=2.35\n",
            "[1877 | 463.19] loss=2.44 avg=2.35\n",
            "[1878 | 466.45] loss=2.94 avg=2.36\n",
            "[1879 | 469.62] loss=2.44 avg=2.36\n",
            "[1880 | 472.86] loss=2.35 avg=2.36\n",
            "[1881 | 476.05] loss=1.99 avg=2.36\n",
            "[1882 | 479.28] loss=2.54 avg=2.36\n",
            "[1883 | 482.56] loss=2.09 avg=2.35\n",
            "[1884 | 485.66] loss=2.62 avg=2.36\n",
            "[1885 | 488.85] loss=2.41 avg=2.36\n",
            "[1886 | 491.92] loss=2.52 avg=2.36\n",
            "[1887 | 495.05] loss=2.28 avg=2.36\n",
            "[1888 | 498.24] loss=2.60 avg=2.36\n",
            "[1889 | 501.35] loss=2.48 avg=2.36\n",
            "[1890 | 504.53] loss=1.78 avg=2.36\n",
            "[1891 | 507.78] loss=2.57 avg=2.36\n",
            "[1892 | 510.93] loss=2.34 avg=2.36\n",
            "[1893 | 514.25] loss=2.35 avg=2.36\n",
            "[1894 | 517.36] loss=2.30 avg=2.36\n",
            "[1895 | 520.52] loss=2.20 avg=2.36\n",
            "[1896 | 523.64] loss=1.88 avg=2.35\n",
            "[1897 | 526.83] loss=2.64 avg=2.35\n",
            "[1898 | 529.99] loss=2.57 avg=2.36\n",
            "[1899 | 533.09] loss=2.17 avg=2.35\n",
            "[1900 | 536.25] loss=2.27 avg=2.35\n",
            "[1901 | 539.61] loss=2.28 avg=2.35\n",
            "[1902 | 543.01] loss=2.35 avg=2.35\n",
            "[1903 | 546.20] loss=1.95 avg=2.35\n",
            "[1904 | 549.45] loss=2.88 avg=2.35\n",
            "[1905 | 552.53] loss=2.29 avg=2.35\n",
            "[1906 | 555.80] loss=1.96 avg=2.35\n",
            "[1907 | 558.87] loss=2.66 avg=2.35\n",
            "[1908 | 562.03] loss=2.48 avg=2.35\n",
            "[1909 | 565.11] loss=2.69 avg=2.36\n",
            "[1910 | 568.27] loss=2.30 avg=2.36\n",
            "[1911 | 571.44] loss=2.86 avg=2.36\n",
            "[1912 | 574.53] loss=2.19 avg=2.36\n",
            "[1913 | 577.63] loss=2.23 avg=2.36\n",
            "[1914 | 580.81] loss=2.03 avg=2.36\n",
            "[1915 | 583.88] loss=2.43 avg=2.36\n",
            "[1916 | 587.04] loss=1.98 avg=2.35\n",
            "[1917 | 590.19] loss=2.63 avg=2.36\n",
            "[1918 | 593.28] loss=1.73 avg=2.35\n",
            "[1919 | 596.45] loss=2.28 avg=2.35\n",
            "[1920 | 599.63] loss=2.31 avg=2.35\n",
            "[1921 | 602.72] loss=2.30 avg=2.35\n",
            "[1922 | 605.81] loss=2.37 avg=2.35\n",
            "[1923 | 609.00] loss=2.04 avg=2.34\n",
            "[1924 | 612.17] loss=2.01 avg=2.34\n",
            "[1925 | 615.24] loss=2.01 avg=2.33\n",
            "[1926 | 618.48] loss=2.29 avg=2.33\n",
            "[1927 | 621.64] loss=2.70 avg=2.34\n",
            "[1928 | 624.68] loss=2.18 avg=2.34\n",
            "[1929 | 627.72] loss=2.50 avg=2.34\n",
            "[1930 | 630.75] loss=2.22 avg=2.34\n",
            "[1931 | 633.79] loss=2.22 avg=2.34\n",
            "[1932 | 636.84] loss=2.19 avg=2.33\n",
            "[1933 | 639.87] loss=2.33 avg=2.33\n",
            "[1934 | 642.90] loss=2.10 avg=2.33\n",
            "[1935 | 645.95] loss=2.45 avg=2.33\n",
            "[1936 | 648.96] loss=2.29 avg=2.33\n",
            "[1937 | 652.01] loss=2.14 avg=2.33\n",
            "[1938 | 655.04] loss=2.22 avg=2.33\n",
            "[1939 | 658.09] loss=2.01 avg=2.32\n",
            "[1940 | 661.21] loss=1.98 avg=2.32\n",
            "[1941 | 664.24] loss=1.84 avg=2.32\n",
            "[1942 | 667.27] loss=2.07 avg=2.31\n",
            "[1943 | 670.31] loss=2.21 avg=2.31\n",
            "[1944 | 673.42] loss=2.90 avg=2.32\n",
            "[1945 | 676.45] loss=2.12 avg=2.32\n",
            "[1946 | 679.47] loss=2.22 avg=2.31\n",
            "[1947 | 682.51] loss=2.09 avg=2.31\n",
            "[1948 | 685.55] loss=2.13 avg=2.31\n",
            "[1949 | 688.56] loss=2.35 avg=2.31\n",
            "[1950 | 691.60] loss=1.98 avg=2.31\n",
            "[1951 | 694.64] loss=2.61 avg=2.31\n",
            "[1952 | 697.69] loss=2.84 avg=2.32\n",
            "[1953 | 700.73] loss=2.34 avg=2.32\n",
            "[1954 | 703.77] loss=2.05 avg=2.31\n",
            "[1955 | 706.82] loss=2.46 avg=2.32\n",
            "[1956 | 709.87] loss=2.12 avg=2.31\n",
            "[1957 | 712.93] loss=2.01 avg=2.31\n",
            "[1958 | 715.99] loss=2.23 avg=2.31\n",
            "[1959 | 719.02] loss=2.60 avg=2.31\n",
            "[1960 | 722.06] loss=2.50 avg=2.31\n",
            "[1961 | 725.10] loss=2.66 avg=2.32\n",
            "[1962 | 728.15] loss=2.78 avg=2.32\n",
            "[1963 | 731.21] loss=2.30 avg=2.32\n",
            "[1964 | 734.26] loss=2.28 avg=2.32\n",
            "[1965 | 737.31] loss=3.00 avg=2.33\n",
            "[1966 | 740.37] loss=1.98 avg=2.33\n",
            "[1967 | 743.41] loss=2.32 avg=2.33\n",
            "[1968 | 746.43] loss=2.36 avg=2.33\n",
            "[1969 | 749.49] loss=3.08 avg=2.33\n",
            "[1970 | 752.52] loss=2.23 avg=2.33\n",
            "[1971 | 755.55] loss=2.03 avg=2.33\n",
            "[1972 | 758.60] loss=2.14 avg=2.33\n",
            "[1973 | 761.67] loss=2.34 avg=2.33\n",
            "[1974 | 764.72] loss=2.39 avg=2.33\n",
            "[1975 | 767.84] loss=2.03 avg=2.33\n",
            "[1976 | 770.94] loss=2.39 avg=2.33\n",
            "[1977 | 774.01] loss=2.32 avg=2.33\n",
            "[1978 | 777.10] loss=2.93 avg=2.33\n",
            "[1979 | 780.16] loss=2.73 avg=2.34\n",
            "[1980 | 783.25] loss=2.17 avg=2.34\n",
            "[1981 | 786.33] loss=2.41 avg=2.34\n",
            "[1982 | 789.43] loss=2.92 avg=2.34\n",
            "[1983 | 792.53] loss=2.32 avg=2.34\n",
            "[1984 | 795.62] loss=2.44 avg=2.34\n",
            "[1985 | 798.71] loss=2.41 avg=2.34\n",
            "[1986 | 801.81] loss=2.04 avg=2.34\n",
            "[1987 | 804.87] loss=2.91 avg=2.35\n",
            "[1988 | 807.96] loss=2.67 avg=2.35\n",
            "[1989 | 811.04] loss=2.13 avg=2.35\n",
            "[1990 | 814.13] loss=2.15 avg=2.35\n",
            "[1991 | 817.23] loss=2.38 avg=2.35\n",
            "[1992 | 820.32] loss=2.21 avg=2.34\n",
            "[1993 | 823.40] loss=2.64 avg=2.35\n",
            "[1994 | 826.47] loss=2.38 avg=2.35\n",
            "[1995 | 829.58] loss=2.67 avg=2.35\n",
            "[1996 | 832.68] loss=2.45 avg=2.35\n",
            "[1997 | 835.74] loss=2.11 avg=2.35\n",
            "[1998 | 838.85] loss=2.26 avg=2.35\n",
            "[1999 | 842.03] loss=2.74 avg=2.35\n",
            "Saving checkpoint/run1/model-2000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " random 'I cant find it!'\n",
            "\n",
            "\n",
            "(1287595200) jrib: if this is what my system is looking for, can I do that too?\n",
            "(1287595200) jrib: the ubuntuguide isn't showing me the options on the main desktop icon\n",
            "(1287595200) jrib: i did install ubuntu and it just stayed the same\n",
            "(1287595200) jrib: i guess i still have a chance\n",
            "(1287595200) jrib: thanks\n",
            "(1287595260) jrib: i guess i need an option\n",
            "(1287595260) jrib: maybe that is a problem with gnome\n",
            "(1287595260) jrib: i installed gnome and now the screen is showing but it wont let me change the size\n",
            "(1287595260) jrib: so what i can do to fix this problem?\n",
            "(1287595260) jrib: is it an ubuntu problem (hard to install)\n",
            "(1287595260) jrib: oops\n",
            "(1287595260) jrib: my ubuntu won't allow me to change the size\n",
            "(1287595320) jrib: can i install any app (im running 7.10) without changing the size ? i installed the xfce app but when i was installing it it showed but now it dont\n",
            "(1287595320) jrib: no\n",
            "(1287595320) jrib: the app is running fine\n",
            "(1287595380) jrib: can it be related to something else ?\n",
            "(1287595380) jrib: its not an ubuntu issue\n",
            "(1287595380) jrib: i didnt install it\n",
            "(1287595380) jrib: i just used xfce\n",
            "(1287595380) jrib: so i do need a solution\n",
            "(1287595380) jrib: so i will go install and get back to here\n",
            "(1287595380) jrib: thanks again\n",
            "(1287585300) jrib: is there a way to install it without a problem ?\n",
            "(1287585360) jrib: can the computer get a newer version after installing\n",
            "(1287585360) jrib: well its a new one\n",
            "(1287585360) jrib: now i need an extra one\n",
            "(1287585420) jrib: so how can the computer get a newer version of ubuntu?\n",
            "\n",
            "\n",
            "(1293538340) jk_: is that a command?\n",
            "(1293538340) jk_: i have a computer\n",
            "(1293538340) jk_: and i cannot get sound on my soundcard\n",
            "(1293538340) jk_: i guess it is, but i am new to linux\n",
            "(1293538460) jk_: what soundcard do you have installed?\n",
            "(1293538580) jk_: i will try again\n",
            "(1293538640) jk_: you are using a 32-bit machine with a 64-bit card. does the card have sound?\n",
            "(1293538640) jk_: you just did not know what to do with the soundcard\n",
            "(1293538700) jk_: so which card will this one have ? will they work together\n",
            "(1293539020) jk_: is your soundcard registered?\n",
            "(1293539120) jk_: have you tried to look up info about it?\n",
            "(1293539240) jeffy: yeah\n",
            "(1293539300) jeffy: are you speaking from experience?\n",
            "(1293539360) jeffy: you should see the sound card you have plugged in and go to sound and press the button on it to see what you should be able to make out.\n",
            "(1293539960) jeffy: oh, right, my mistake, then tried again\n",
            "(1293539960) jeffy: and now it sounds like the sound card is not being properly registered\n",
            "(1293539960) jeffy: but the card is recognized\n",
            "(1293539960) jk_: thanks a lot, i just think the sound card is registered\n",
            "(1293539960) jeffy: it's not working\n",
            "(1293539960) jeffy: the card is working just fine\n",
            "\n",
            "\n",
            "(1233508360) bazhang: what do you mean by a 'partition'\n",
            "(1233508420) bazhang: partition is the way that you partition a file into partitions\n",
            "(1233508480) bazhang: partitioning is a partition of space\n",
            "(1233508480) bazhang: partitioning is the way that you partition a file into partitions\n",
            "\n",
            "\n",
            "[2000 | 892.44] loss=2.47 avg=2.35\n",
            "[2001 | 895.64] loss=2.32 avg=2.35\n",
            "[2002 | 898.73] loss=2.22 avg=2.35\n",
            "[2003 | 901.97] loss=2.22 avg=2.35\n",
            "[2004 | 905.26] loss=2.68 avg=2.36\n",
            "[2005 | 908.42] loss=2.76 avg=2.36\n",
            "[2006 | 911.51] loss=2.48 avg=2.36\n",
            "[2007 | 914.69] loss=2.93 avg=2.37\n",
            "[2008 | 917.86] loss=2.74 avg=2.37\n",
            "[2009 | 921.05] loss=2.76 avg=2.38\n",
            "[2010 | 924.23] loss=2.61 avg=2.38\n",
            "[2011 | 927.28] loss=2.89 avg=2.38\n",
            "[2012 | 930.46] loss=2.51 avg=2.38\n",
            "[2013 | 933.88] loss=2.32 avg=2.38\n",
            "[2014 | 936.94] loss=2.24 avg=2.38\n",
            "[2015 | 940.11] loss=2.54 avg=2.38\n",
            "[2016 | 943.23] loss=2.00 avg=2.38\n",
            "[2017 | 946.40] loss=2.24 avg=2.38\n",
            "[2018 | 949.49] loss=3.02 avg=2.39\n",
            "[2019 | 952.57] loss=1.73 avg=2.38\n",
            "[2020 | 955.67] loss=2.53 avg=2.38\n",
            "[2021 | 958.77] loss=2.74 avg=2.38\n",
            "[2022 | 961.93] loss=2.54 avg=2.39\n",
            "[2023 | 965.02] loss=2.26 avg=2.38\n",
            "[2024 | 968.11] loss=2.32 avg=2.38\n",
            "[2025 | 971.16] loss=2.96 avg=2.39\n",
            "[2026 | 974.24] loss=1.86 avg=2.38\n",
            "[2027 | 977.30] loss=2.52 avg=2.39\n",
            "[2028 | 980.33] loss=2.30 avg=2.38\n",
            "[2029 | 983.38] loss=1.81 avg=2.38\n",
            "[2030 | 986.45] loss=1.99 avg=2.37\n",
            "[2031 | 989.50] loss=2.53 avg=2.38\n",
            "[2032 | 992.54] loss=2.04 avg=2.37\n",
            "[2033 | 995.63] loss=2.46 avg=2.37\n",
            "[2034 | 998.69] loss=2.47 avg=2.37\n",
            "[2035 | 1001.75] loss=2.08 avg=2.37\n",
            "[2036 | 1004.82] loss=2.62 avg=2.37\n",
            "[2037 | 1007.90] loss=2.49 avg=2.37\n",
            "[2038 | 1011.03] loss=2.29 avg=2.37\n",
            "[2039 | 1014.08] loss=2.78 avg=2.38\n",
            "[2040 | 1017.23] loss=2.07 avg=2.37\n",
            "[2041 | 1020.27] loss=2.88 avg=2.38\n",
            "[2042 | 1023.31] loss=2.53 avg=2.38\n",
            "[2043 | 1026.38] loss=1.85 avg=2.38\n",
            "[2044 | 1029.52] loss=2.70 avg=2.38\n",
            "[2045 | 1032.56] loss=2.22 avg=2.38\n",
            "[2046 | 1035.63] loss=2.50 avg=2.38\n",
            "[2047 | 1038.67] loss=2.28 avg=2.38\n",
            "[2048 | 1041.81] loss=2.26 avg=2.38\n",
            "[2049 | 1044.86] loss=1.70 avg=2.37\n",
            "[2050 | 1048.00] loss=2.08 avg=2.37\n",
            "[2051 | 1051.05] loss=2.64 avg=2.37\n",
            "[2052 | 1054.20] loss=1.84 avg=2.36\n",
            "[2053 | 1057.26] loss=3.06 avg=2.37\n",
            "[2054 | 1060.31] loss=2.02 avg=2.37\n",
            "[2055 | 1063.37] loss=2.59 avg=2.37\n",
            "[2056 | 1066.51] loss=2.11 avg=2.37\n",
            "[2057 | 1069.57] loss=2.69 avg=2.37\n",
            "[2058 | 1072.61] loss=1.93 avg=2.37\n",
            "[2059 | 1075.65] loss=2.58 avg=2.37\n",
            "[2060 | 1078.70] loss=2.53 avg=2.37\n",
            "[2061 | 1081.75] loss=2.41 avg=2.37\n",
            "[2062 | 1084.81] loss=2.59 avg=2.37\n",
            "[2063 | 1087.84] loss=2.22 avg=2.37\n",
            "[2064 | 1090.89] loss=1.99 avg=2.37\n",
            "[2065 | 1093.94] loss=3.07 avg=2.37\n",
            "[2066 | 1096.98] loss=2.05 avg=2.37\n",
            "[2067 | 1100.06] loss=2.51 avg=2.37\n",
            "[2068 | 1103.09] loss=2.36 avg=2.37\n",
            "[2069 | 1106.14] loss=2.91 avg=2.38\n",
            "[2070 | 1109.18] loss=2.38 avg=2.38\n",
            "[2071 | 1112.24] loss=2.00 avg=2.37\n",
            "[2072 | 1115.33] loss=2.28 avg=2.37\n",
            "[2073 | 1118.41] loss=2.12 avg=2.37\n",
            "[2074 | 1121.49] loss=2.84 avg=2.38\n",
            "[2075 | 1124.60] loss=2.52 avg=2.38\n",
            "[2076 | 1127.70] loss=2.36 avg=2.38\n",
            "[2077 | 1130.80] loss=1.88 avg=2.37\n",
            "[2078 | 1133.91] loss=1.87 avg=2.37\n",
            "[2079 | 1136.97] loss=2.44 avg=2.37\n",
            "[2080 | 1140.06] loss=2.25 avg=2.37\n",
            "[2081 | 1143.17] loss=2.24 avg=2.36\n",
            "[2082 | 1146.24] loss=2.43 avg=2.37\n",
            "[2083 | 1149.33] loss=2.18 avg=2.36\n",
            "[2084 | 1152.41] loss=2.28 avg=2.36\n",
            "[2085 | 1155.50] loss=2.68 avg=2.37\n",
            "[2086 | 1158.56] loss=2.90 avg=2.37\n",
            "[2087 | 1161.67] loss=1.75 avg=2.36\n",
            "[2088 | 1164.79] loss=2.33 avg=2.36\n",
            "[2089 | 1167.90] loss=2.05 avg=2.36\n",
            "[2090 | 1171.00] loss=2.32 avg=2.36\n",
            "[2091 | 1174.09] loss=1.93 avg=2.36\n",
            "[2092 | 1177.21] loss=2.45 avg=2.36\n",
            "[2093 | 1180.29] loss=2.62 avg=2.36\n",
            "[2094 | 1183.39] loss=2.51 avg=2.36\n",
            "[2095 | 1186.50] loss=1.91 avg=2.36\n",
            "[2096 | 1189.59] loss=2.03 avg=2.35\n",
            "[2097 | 1192.70] loss=2.02 avg=2.35\n",
            "[2098 | 1195.79] loss=2.21 avg=2.35\n",
            "[2099 | 1198.92] loss=2.52 avg=2.35\n",
            "[2100 | 1202.01] loss=2.14 avg=2.35\n",
            "[2101 | 1205.08] loss=2.29 avg=2.35\n",
            "[2102 | 1208.16] loss=2.74 avg=2.35\n",
            "[2103 | 1211.27] loss=2.73 avg=2.36\n",
            "[2104 | 1214.37] loss=2.40 avg=2.36\n",
            "[2105 | 1217.45] loss=2.13 avg=2.35\n",
            "[2106 | 1220.52] loss=2.76 avg=2.36\n",
            "[2107 | 1223.59] loss=2.36 avg=2.36\n",
            "[2108 | 1226.75] loss=2.45 avg=2.36\n",
            "[2109 | 1229.87] loss=2.04 avg=2.36\n",
            "[2110 | 1233.00] loss=2.18 avg=2.35\n",
            "[2111 | 1236.09] loss=1.94 avg=2.35\n",
            "[2112 | 1239.22] loss=2.37 avg=2.35\n",
            "[2113 | 1242.56] loss=2.51 avg=2.35\n",
            "[2114 | 1245.65] loss=2.51 avg=2.35\n",
            "[2115 | 1248.91] loss=2.26 avg=2.35\n",
            "[2116 | 1252.01] loss=2.66 avg=2.36\n",
            "[2117 | 1255.26] loss=2.59 avg=2.36\n",
            "[2118 | 1258.45] loss=2.42 avg=2.36\n",
            "[2119 | 1261.63] loss=2.58 avg=2.36\n",
            "[2120 | 1264.70] loss=2.13 avg=2.36\n",
            "[2121 | 1267.79] loss=2.46 avg=2.36\n",
            "[2122 | 1270.88] loss=2.16 avg=2.36\n",
            "[2123 | 1273.95] loss=2.59 avg=2.36\n",
            "[2124 | 1277.11] loss=2.00 avg=2.36\n",
            "[2125 | 1280.27] loss=2.34 avg=2.36\n",
            "[2126 | 1283.43] loss=2.14 avg=2.35\n",
            "[2127 | 1286.60] loss=2.20 avg=2.35\n",
            "[2128 | 1290.11] loss=2.28 avg=2.35\n",
            "[2129 | 1293.42] loss=2.83 avg=2.36\n",
            "[2130 | 1296.68] loss=2.12 avg=2.35\n",
            "[2131 | 1299.74] loss=2.38 avg=2.35\n",
            "[2132 | 1302.88] loss=2.00 avg=2.35\n",
            "[2133 | 1306.13] loss=2.77 avg=2.35\n",
            "[2134 | 1309.31] loss=1.91 avg=2.35\n",
            "[2135 | 1312.56] loss=2.33 avg=2.35\n",
            "[2136 | 1315.64] loss=2.43 avg=2.35\n",
            "[2137 | 1318.82] loss=3.04 avg=2.36\n",
            "[2138 | 1321.90] loss=2.38 avg=2.36\n",
            "[2139 | 1324.97] loss=2.23 avg=2.36\n",
            "[2140 | 1328.14] loss=2.04 avg=2.35\n",
            "[2141 | 1331.20] loss=2.31 avg=2.35\n",
            "[2142 | 1334.23] loss=2.59 avg=2.36\n",
            "[2143 | 1337.25] loss=2.83 avg=2.36\n",
            "[2144 | 1340.37] loss=2.47 avg=2.36\n",
            "[2145 | 1343.57] loss=2.28 avg=2.36\n",
            "[2146 | 1346.68] loss=2.39 avg=2.36\n",
            "[2147 | 1349.69] loss=2.71 avg=2.36\n",
            "[2148 | 1352.72] loss=2.56 avg=2.37\n",
            "[2149 | 1355.75] loss=2.16 avg=2.36\n",
            "[2150 | 1358.79] loss=2.04 avg=2.36\n",
            "[2151 | 1361.90] loss=2.02 avg=2.36\n",
            "[2152 | 1364.95] loss=2.40 avg=2.36\n",
            "[2153 | 1368.08] loss=1.99 avg=2.35\n",
            "[2154 | 1371.12] loss=2.79 avg=2.36\n",
            "[2155 | 1374.16] loss=2.32 avg=2.36\n",
            "[2156 | 1377.31] loss=2.35 avg=2.36\n",
            "[2157 | 1380.44] loss=2.53 avg=2.36\n",
            "[2158 | 1383.49] loss=2.49 avg=2.36\n",
            "[2159 | 1386.52] loss=2.38 avg=2.36\n",
            "[2160 | 1389.63] loss=2.24 avg=2.36\n",
            "[2161 | 1392.67] loss=2.38 avg=2.36\n",
            "[2162 | 1395.80] loss=1.96 avg=2.36\n",
            "[2163 | 1399.00] loss=2.42 avg=2.36\n",
            "[2164 | 1402.13] loss=2.18 avg=2.36\n",
            "[2165 | 1405.27] loss=2.02 avg=2.35\n",
            "[2166 | 1408.30] loss=2.23 avg=2.35\n",
            "[2167 | 1411.33] loss=2.55 avg=2.35\n",
            "[2168 | 1414.38] loss=2.43 avg=2.35\n",
            "[2169 | 1417.61] loss=2.42 avg=2.35\n",
            "[2170 | 1420.64] loss=2.40 avg=2.35\n",
            "[2171 | 1423.68] loss=2.51 avg=2.36\n",
            "[2172 | 1426.71] loss=2.31 avg=2.36\n",
            "[2173 | 1429.75] loss=2.39 avg=2.36\n",
            "[2174 | 1432.78] loss=1.98 avg=2.35\n",
            "[2175 | 1435.80] loss=2.77 avg=2.36\n",
            "[2176 | 1438.84] loss=2.42 avg=2.36\n",
            "[2177 | 1441.86] loss=2.22 avg=2.36\n",
            "[2178 | 1444.90] loss=2.34 avg=2.36\n",
            "[2179 | 1447.94] loss=2.53 avg=2.36\n",
            "[2180 | 1450.98] loss=2.38 avg=2.36\n",
            "[2181 | 1454.01] loss=2.38 avg=2.36\n",
            "[2182 | 1457.04] loss=2.71 avg=2.36\n",
            "[2183 | 1460.07] loss=2.29 avg=2.36\n",
            "[2184 | 1463.11] loss=2.50 avg=2.36\n",
            "[2185 | 1466.14] loss=2.34 avg=2.36\n",
            "[2186 | 1469.22] loss=2.67 avg=2.36\n",
            "[2187 | 1472.28] loss=2.40 avg=2.37\n",
            "[2188 | 1475.37] loss=2.49 avg=2.37\n",
            "[2189 | 1478.48] loss=2.41 avg=2.37\n",
            "[2190 | 1481.58] loss=2.95 avg=2.37\n",
            "[2191 | 1484.69] loss=2.56 avg=2.37\n",
            "[2192 | 1487.80] loss=2.13 avg=2.37\n",
            "[2193 | 1490.90] loss=2.24 avg=2.37\n",
            "[2194 | 1494.03] loss=2.31 avg=2.37\n",
            "[2195 | 1497.12] loss=2.76 avg=2.37\n",
            "[2196 | 1500.22] loss=2.42 avg=2.37\n",
            "[2197 | 1503.33] loss=2.02 avg=2.37\n",
            "[2198 | 1506.47] loss=2.17 avg=2.37\n",
            "[2199 | 1509.57] loss=1.91 avg=2.36\n",
            "[2200 | 1512.69] loss=2.69 avg=2.37\n",
            "[2201 | 1515.82] loss=2.28 avg=2.37\n",
            "[2202 | 1518.94] loss=2.28 avg=2.37\n",
            "[2203 | 1522.05] loss=2.42 avg=2.37\n",
            "[2204 | 1525.15] loss=2.59 avg=2.37\n",
            "[2205 | 1528.25] loss=2.55 avg=2.37\n",
            "[2206 | 1531.36] loss=2.00 avg=2.37\n",
            "[2207 | 1534.47] loss=1.92 avg=2.36\n",
            "[2208 | 1537.59] loss=2.54 avg=2.36\n",
            "[2209 | 1540.72] loss=2.01 avg=2.36\n",
            "[2210 | 1543.83] loss=2.25 avg=2.36\n",
            "[2211 | 1546.94] loss=2.76 avg=2.36\n",
            "[2212 | 1550.05] loss=2.92 avg=2.37\n",
            "[2213 | 1553.17] loss=2.05 avg=2.37\n",
            "[2214 | 1556.29] loss=2.48 avg=2.37\n",
            "[2215 | 1559.39] loss=2.24 avg=2.37\n",
            "[2216 | 1562.51] loss=2.13 avg=2.36\n",
            "[2217 | 1565.62] loss=1.98 avg=2.36\n",
            "[2218 | 1568.71] loss=2.57 avg=2.36\n",
            "[2219 | 1571.84] loss=2.60 avg=2.36\n",
            "[2220 | 1574.95] loss=2.31 avg=2.36\n",
            "[2221 | 1578.07] loss=2.86 avg=2.37\n",
            "[2222 | 1581.18] loss=2.24 avg=2.37\n",
            "[2223 | 1584.30] loss=2.65 avg=2.37\n",
            "[2224 | 1587.43] loss=2.23 avg=2.37\n",
            "[2225 | 1590.56] loss=2.17 avg=2.37\n",
            "[2226 | 1593.69] loss=2.14 avg=2.36\n",
            "[2227 | 1596.81] loss=2.23 avg=2.36\n",
            "[2228 | 1599.93] loss=2.11 avg=2.36\n",
            "[2229 | 1603.07] loss=2.33 avg=2.36\n",
            "[2230 | 1606.21] loss=1.93 avg=2.36\n",
            "[2231 | 1609.32] loss=2.05 avg=2.35\n",
            "[2232 | 1612.44] loss=2.37 avg=2.35\n",
            "[2233 | 1615.55] loss=2.24 avg=2.35\n",
            "[2234 | 1618.67] loss=2.12 avg=2.35\n",
            "[2235 | 1621.81] loss=2.51 avg=2.35\n",
            "[2236 | 1624.95] loss=2.67 avg=2.35\n",
            "[2237 | 1628.06] loss=2.12 avg=2.35\n",
            "[2238 | 1631.18] loss=2.06 avg=2.35\n",
            "[2239 | 1634.30] loss=2.05 avg=2.35\n",
            "[2240 | 1637.40] loss=2.74 avg=2.35\n",
            "[2241 | 1640.54] loss=2.23 avg=2.35\n",
            "[2242 | 1643.67] loss=2.20 avg=2.35\n",
            "[2243 | 1646.79] loss=2.47 avg=2.35\n",
            "[2244 | 1649.95] loss=2.60 avg=2.35\n",
            "[2245 | 1653.08] loss=2.54 avg=2.35\n",
            "[2246 | 1656.20] loss=2.28 avg=2.35\n",
            "[2247 | 1659.31] loss=2.72 avg=2.36\n",
            "[2248 | 1662.44] loss=2.37 avg=2.36\n",
            "[2249 | 1665.55] loss=2.68 avg=2.36\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " is a program to create a backup in a folder and that is then stored on local computer. I would like a program that runs a script of how that folder /name.\n",
            "(1195965420) cjhaind: i would like to take your example and make the file in a folder somewhere else and then later make the file at home. but where would I put the backup file when I do? I am not sure what my point could be\n",
            "(1195965420) cjhaind: or how to open the file to where the backup file is.\n",
            "(1195965420) cjhaind: i am so used to ubuntu. how would i do this?\n",
            "\n",
            "\n",
            "(1184536220) wesley_: hi guys.\n",
            "(1184536280) wesley_: how can I use a command like 'gksudo apt-get update?'\n",
            "(1184536340) ughhgghh: gksuitemask='-1'\n",
            "(1184536340) wesley_: how do i do that without using the terminal?\n",
            "(1184536360) ughhgah: gksuitemask='-1'\n",
            "(1184536520) wesley_: ok, I have gksuitemask='-1'\n",
            "(1184536520) wesley_: i tried 'gksuitemask=:0'\n",
            "(1184536640) ughh: try it with gksu (gksu)\n",
            "(1184536700) wesley_: and, 'gksuitemask='-1'\n",
            "(1184536940) ughh: i doubt gnome-terminal\n",
            "(11845317460) wesley_: ok.\n",
            "\n",
            "\n",
            "(1199372420) jrib: and then how did you manage those install?\n",
            "(1199372420) jrib: yes\n",
            "(1199372540) jrib: do you have a network on or offline?\n",
            "(1199372880) masonjr: not sure the link would be the same but it's likely theres a networking option.\n",
            "(1199372880) masonjr: I was going to try the link http://pauls.org/doc/manuals/gw-manuals/epp.htm\n",
            "(1199372880) masonjr: But i just thought there were some docs.\n",
            "(1199372940) masonjr: there are a few other ways i could get the same effect i would from downloading the whole thing. i just know what i did by a slight tweak.\n",
            "(1199373000) masonjr: yup. that's the gist of it, but i wasn't really interested in what you're doing.\n",
            "(1199377200) masonjr: there're no GUI to do it from the net anyways. I mean just drag your mouse.\n",
            "(1199377200) masonjr: I don't have the ability to use a keyboard right now.\n",
            "(1199377200) masonjr: I'm thinking something is wrong.   It seems.\n",
            "(1199378100) Masonjr: that's my main concern.\n",
            "(1199378100) Masonjr: Is there an option to get rid of the 'no network connection' warning (not from /etc/network/\n",
            "(1199378160) Masonjr: I've found that some people had a 'no network connection' dialog, and I just got bored.\n",
            "(1199378160) Masonjr: And that's one of them.  I don't have a network.\n",
            "(1199378160) Masonjr: I just get tired.\n",
            "\n",
            "\n",
            "(1185481700) ikonia: hello, my machine got unkown and I dont work with linux\n",
            "(1185481820) ikonia: oh\n",
            "(1185481820) ikonia: please! :P\n",
            "(1185481880) ikonia: I'll try\n",
            "(1185481880) ikonia: how to disable some hardware?\n",
            "(1185481880) ikonia: and do I need a drivers for all ?\n",
            "(1185481940) ikonia: no\n",
            "(1185481940) ikonia: no. No I do not\n",
            "(1185481940) ikonia: no\n",
            "(1185481940) ikonia: ok, here is why:\n",
            "(1185481940) ikonia: my machine is running at 80% (performance on a low graphics) and\n",
            "\n",
            "[2250 | 1706.63] loss=2.52 avg=2.36\n",
            "[2251 | 1709.73] loss=2.30 avg=2.36\n",
            "[2252 | 1712.81] loss=2.81 avg=2.36\n",
            "[2253 | 1715.91] loss=1.99 avg=2.36\n",
            "[2254 | 1718.99] loss=2.91 avg=2.37\n",
            "[2255 | 1722.06] loss=2.38 avg=2.37\n",
            "[2256 | 1725.15] loss=2.53 avg=2.37\n",
            "[2257 | 1728.23] loss=2.08 avg=2.37\n",
            "[2258 | 1731.32] loss=2.14 avg=2.36\n",
            "[2259 | 1734.42] loss=1.94 avg=2.36\n",
            "[2260 | 1737.51] loss=2.74 avg=2.36\n",
            "[2261 | 1740.59] loss=2.35 avg=2.36\n",
            "[2262 | 1743.67] loss=2.52 avg=2.36\n",
            "[2263 | 1746.76] loss=2.16 avg=2.36\n",
            "[2264 | 1749.84] loss=2.68 avg=2.37\n",
            "[2265 | 1752.92] loss=2.14 avg=2.36\n",
            "[2266 | 1756.01] loss=2.56 avg=2.36\n",
            "[2267 | 1759.10] loss=2.06 avg=2.36\n",
            "[2268 | 1762.18] loss=2.20 avg=2.36\n",
            "[2269 | 1765.28] loss=2.34 avg=2.36\n",
            "[2270 | 1768.37] loss=2.66 avg=2.36\n",
            "[2271 | 1771.47] loss=2.35 avg=2.36\n",
            "[2272 | 1774.55] loss=2.23 avg=2.36\n",
            "[2273 | 1777.62] loss=2.06 avg=2.36\n",
            "[2274 | 1780.71] loss=1.55 avg=2.35\n",
            "[2275 | 1783.78] loss=2.21 avg=2.35\n",
            "[2276 | 1786.84] loss=2.09 avg=2.35\n",
            "[2277 | 1789.91] loss=2.02 avg=2.34\n",
            "[2278 | 1792.97] loss=2.49 avg=2.34\n",
            "[2279 | 1796.04] loss=2.43 avg=2.35\n",
            "[2280 | 1799.11] loss=2.86 avg=2.35\n",
            "[2281 | 1802.20] loss=2.14 avg=2.35\n",
            "[2282 | 1805.27] loss=2.42 avg=2.35\n",
            "[2283 | 1808.36] loss=2.21 avg=2.35\n",
            "[2284 | 1811.46] loss=2.78 avg=2.35\n",
            "[2285 | 1814.54] loss=2.45 avg=2.35\n",
            "[2286 | 1817.62] loss=2.56 avg=2.36\n",
            "[2287 | 1820.70] loss=2.14 avg=2.35\n",
            "[2288 | 1823.78] loss=2.96 avg=2.36\n",
            "[2289 | 1826.83] loss=2.57 avg=2.36\n",
            "[2290 | 1829.91] loss=2.50 avg=2.36\n",
            "[2291 | 1832.99] loss=2.60 avg=2.36\n",
            "[2292 | 1836.07] loss=2.11 avg=2.36\n",
            "[2293 | 1839.16] loss=2.55 avg=2.36\n",
            "[2294 | 1842.24] loss=2.11 avg=2.36\n",
            "[2295 | 1845.32] loss=2.18 avg=2.36\n",
            "[2296 | 1848.38] loss=2.31 avg=2.36\n",
            "[2297 | 1851.48] loss=2.28 avg=2.36\n",
            "[2298 | 1854.56] loss=2.67 avg=2.36\n",
            "[2299 | 1857.66] loss=2.71 avg=2.37\n",
            "[2300 | 1860.75] loss=2.38 avg=2.37\n",
            "[2301 | 1863.84] loss=1.95 avg=2.36\n",
            "[2302 | 1866.92] loss=2.55 avg=2.36\n",
            "[2303 | 1870.03] loss=1.77 avg=2.36\n",
            "[2304 | 1873.11] loss=2.37 avg=2.36\n",
            "[2305 | 1876.21] loss=2.16 avg=2.36\n",
            "[2306 | 1879.32] loss=2.79 avg=2.36\n",
            "[2307 | 1882.40] loss=2.62 avg=2.36\n",
            "[2308 | 1885.48] loss=2.29 avg=2.36\n",
            "[2309 | 1888.58] loss=1.94 avg=2.36\n",
            "[2310 | 1891.69] loss=2.17 avg=2.36\n",
            "[2311 | 1894.81] loss=2.18 avg=2.35\n",
            "[2312 | 1897.95] loss=2.53 avg=2.36\n",
            "[2313 | 1901.07] loss=2.61 avg=2.36\n",
            "[2314 | 1904.19] loss=2.22 avg=2.36\n",
            "[2315 | 1907.35] loss=2.11 avg=2.35\n",
            "[2316 | 1910.49] loss=2.12 avg=2.35\n",
            "[2317 | 1913.63] loss=2.21 avg=2.35\n",
            "[2318 | 1916.72] loss=1.97 avg=2.35\n",
            "[2319 | 1919.82] loss=2.68 avg=2.35\n",
            "[2320 | 1922.93] loss=2.35 avg=2.35\n",
            "[2321 | 1926.07] loss=2.55 avg=2.35\n",
            "[2322 | 1929.18] loss=2.15 avg=2.35\n",
            "[2323 | 1932.29] loss=2.55 avg=2.35\n",
            "[2324 | 1935.43] loss=2.32 avg=2.35\n",
            "[2325 | 1938.55] loss=2.05 avg=2.35\n",
            "[2326 | 1941.65] loss=2.25 avg=2.35\n",
            "[2327 | 1944.78] loss=2.15 avg=2.35\n",
            "[2328 | 1947.90] loss=2.24 avg=2.34\n",
            "[2329 | 1951.03] loss=2.52 avg=2.35\n",
            "[2330 | 1954.14] loss=2.62 avg=2.35\n",
            "[2331 | 1957.26] loss=2.01 avg=2.35\n",
            "[2332 | 1960.41] loss=2.67 avg=2.35\n",
            "[2333 | 1963.51] loss=2.35 avg=2.35\n",
            "[2334 | 1966.63] loss=2.70 avg=2.35\n",
            "[2335 | 1969.75] loss=2.16 avg=2.35\n",
            "[2336 | 1972.85] loss=2.77 avg=2.35\n",
            "[2337 | 1975.97] loss=2.32 avg=2.35\n",
            "[2338 | 1979.09] loss=2.52 avg=2.36\n",
            "[2339 | 1982.20] loss=2.47 avg=2.36\n",
            "[2340 | 1985.30] loss=2.34 avg=2.36\n",
            "[2341 | 1988.44] loss=1.95 avg=2.35\n",
            "[2342 | 1991.57] loss=1.90 avg=2.35\n",
            "[2343 | 1994.71] loss=2.03 avg=2.35\n",
            "[2344 | 1997.83] loss=2.37 avg=2.35\n",
            "[2345 | 2000.94] loss=2.17 avg=2.34\n",
            "[2346 | 2004.06] loss=2.15 avg=2.34\n",
            "[2347 | 2007.22] loss=2.94 avg=2.35\n",
            "[2348 | 2010.36] loss=2.31 avg=2.35\n",
            "[2349 | 2013.47] loss=2.95 avg=2.35\n",
            "[2350 | 2016.60] loss=2.04 avg=2.35\n",
            "[2351 | 2019.73] loss=2.77 avg=2.35\n",
            "[2352 | 2022.88] loss=2.82 avg=2.36\n",
            "[2353 | 2025.99] loss=2.15 avg=2.36\n",
            "[2354 | 2029.10] loss=2.30 avg=2.36\n",
            "[2355 | 2032.22] loss=2.44 avg=2.36\n",
            "[2356 | 2035.34] loss=2.13 avg=2.36\n",
            "[2357 | 2038.46] loss=2.46 avg=2.36\n",
            "[2358 | 2041.59] loss=2.75 avg=2.36\n",
            "[2359 | 2044.69] loss=2.45 avg=2.36\n",
            "[2360 | 2047.79] loss=2.24 avg=2.36\n",
            "[2361 | 2050.94] loss=2.12 avg=2.36\n",
            "[2362 | 2054.03] loss=2.52 avg=2.36\n",
            "[2363 | 2057.16] loss=2.81 avg=2.36\n",
            "[2364 | 2060.32] loss=2.23 avg=2.36\n",
            "[2365 | 2063.44] loss=2.60 avg=2.36\n",
            "[2366 | 2066.56] loss=2.14 avg=2.36\n",
            "[2367 | 2069.69] loss=2.19 avg=2.36\n",
            "[2368 | 2072.79] loss=2.13 avg=2.36\n",
            "[2369 | 2075.89] loss=2.16 avg=2.36\n",
            "[2370 | 2079.00] loss=2.60 avg=2.36\n",
            "[2371 | 2082.11] loss=2.31 avg=2.36\n",
            "[2372 | 2085.23] loss=2.36 avg=2.36\n",
            "[2373 | 2088.33] loss=2.95 avg=2.36\n",
            "[2374 | 2091.44] loss=2.33 avg=2.36\n",
            "[2375 | 2094.54] loss=2.41 avg=2.36\n",
            "[2376 | 2097.67] loss=2.77 avg=2.37\n",
            "[2377 | 2100.78] loss=2.16 avg=2.37\n",
            "[2378 | 2103.90] loss=2.26 avg=2.37\n",
            "[2379 | 2107.00] loss=2.73 avg=2.37\n",
            "[2380 | 2110.13] loss=2.06 avg=2.37\n",
            "[2381 | 2113.24] loss=1.97 avg=2.36\n",
            "[2382 | 2116.33] loss=2.13 avg=2.36\n",
            "[2383 | 2119.41] loss=2.07 avg=2.36\n",
            "[2384 | 2122.49] loss=2.41 avg=2.36\n",
            "[2385 | 2125.56] loss=1.87 avg=2.35\n",
            "[2386 | 2128.65] loss=2.30 avg=2.35\n",
            "[2387 | 2131.72] loss=2.27 avg=2.35\n",
            "[2388 | 2134.77] loss=2.24 avg=2.35\n",
            "[2389 | 2137.85] loss=2.80 avg=2.35\n",
            "[2390 | 2140.92] loss=2.25 avg=2.35\n",
            "[2391 | 2143.99] loss=2.74 avg=2.36\n",
            "[2392 | 2147.05] loss=2.39 avg=2.36\n",
            "[2393 | 2150.14] loss=2.66 avg=2.36\n",
            "[2394 | 2153.20] loss=2.46 avg=2.36\n",
            "[2395 | 2156.27] loss=2.43 avg=2.36\n",
            "[2396 | 2159.33] loss=2.90 avg=2.37\n",
            "[2397 | 2162.39] loss=2.09 avg=2.36\n",
            "[2398 | 2165.46] loss=2.35 avg=2.36\n",
            "[2399 | 2168.53] loss=2.48 avg=2.37\n",
            "[2400 | 2171.63] loss=2.50 avg=2.37\n",
            "[2401 | 2174.69] loss=2.81 avg=2.37\n",
            "[2402 | 2177.77] loss=2.46 avg=2.37\n",
            "[2403 | 2180.86] loss=2.29 avg=2.37\n",
            "[2404 | 2183.96] loss=2.48 avg=2.37\n",
            "[2405 | 2187.04] loss=2.20 avg=2.37\n",
            "[2406 | 2190.12] loss=2.19 avg=2.37\n",
            "[2407 | 2193.19] loss=2.55 avg=2.37\n",
            "[2408 | 2196.26] loss=2.64 avg=2.37\n",
            "[2409 | 2199.35] loss=2.89 avg=2.38\n",
            "[2410 | 2202.43] loss=1.93 avg=2.37\n",
            "[2411 | 2205.50] loss=2.68 avg=2.38\n",
            "[2412 | 2208.57] loss=2.32 avg=2.38\n",
            "[2413 | 2211.64] loss=2.14 avg=2.37\n",
            "[2414 | 2214.72] loss=2.27 avg=2.37\n",
            "[2415 | 2217.80] loss=2.08 avg=2.37\n",
            "[2416 | 2220.87] loss=2.16 avg=2.37\n",
            "[2417 | 2223.95] loss=2.18 avg=2.37\n",
            "[2418 | 2227.04] loss=1.77 avg=2.36\n",
            "[2419 | 2230.11] loss=2.35 avg=2.36\n",
            "[2420 | 2233.19] loss=2.08 avg=2.36\n",
            "[2421 | 2236.26] loss=2.49 avg=2.36\n",
            "[2422 | 2239.32] loss=2.27 avg=2.36\n",
            "[2423 | 2242.39] loss=2.75 avg=2.36\n",
            "[2424 | 2245.48] loss=2.07 avg=2.36\n",
            "[2425 | 2248.55] loss=2.19 avg=2.36\n",
            "[2426 | 2251.62] loss=2.17 avg=2.36\n",
            "[2427 | 2254.68] loss=2.35 avg=2.36\n",
            "[2428 | 2257.75] loss=2.38 avg=2.36\n",
            "[2429 | 2260.83] loss=2.65 avg=2.36\n",
            "[2430 | 2263.88] loss=2.93 avg=2.36\n",
            "[2431 | 2266.98] loss=2.24 avg=2.36\n",
            "[2432 | 2270.11] loss=2.50 avg=2.36\n",
            "[2433 | 2273.26] loss=2.59 avg=2.37\n",
            "[2434 | 2276.37] loss=2.35 avg=2.37\n",
            "[2435 | 2279.50] loss=2.50 avg=2.37\n",
            "[2436 | 2282.61] loss=2.03 avg=2.36\n",
            "[2437 | 2285.73] loss=2.29 avg=2.36\n",
            "[2438 | 2288.83] loss=2.41 avg=2.36\n",
            "[2439 | 2291.96] loss=2.35 avg=2.36\n",
            "[2440 | 2295.07] loss=1.96 avg=2.36\n",
            "[2441 | 2298.17] loss=2.38 avg=2.36\n",
            "[2442 | 2301.30] loss=2.20 avg=2.36\n",
            "[2443 | 2304.41] loss=2.84 avg=2.36\n",
            "[2444 | 2307.56] loss=2.73 avg=2.37\n",
            "[2445 | 2310.69] loss=2.21 avg=2.37\n",
            "[2446 | 2313.82] loss=2.64 avg=2.37\n",
            "[2447 | 2316.95] loss=2.19 avg=2.37\n",
            "[2448 | 2320.11] loss=2.16 avg=2.36\n",
            "[2449 | 2323.22] loss=2.43 avg=2.37\n",
            "[2450 | 2326.32] loss=2.03 avg=2.36\n",
            "[2451 | 2329.45] loss=2.36 avg=2.36\n",
            "[2452 | 2332.57] loss=2.32 avg=2.36\n",
            "[2453 | 2335.70] loss=2.19 avg=2.36\n",
            "[2454 | 2338.84] loss=1.85 avg=2.35\n",
            "[2455 | 2341.98] loss=2.11 avg=2.35\n",
            "[2456 | 2345.12] loss=1.90 avg=2.35\n",
            "[2457 | 2348.26] loss=2.18 avg=2.35\n",
            "[2458 | 2351.35] loss=2.21 avg=2.34\n",
            "[2459 | 2354.48] loss=2.16 avg=2.34\n",
            "[2460 | 2357.60] loss=2.15 avg=2.34\n",
            "[2461 | 2360.73] loss=2.43 avg=2.34\n",
            "[2462 | 2363.87] loss=2.00 avg=2.34\n",
            "[2463 | 2366.99] loss=2.78 avg=2.34\n",
            "[2464 | 2370.11] loss=1.89 avg=2.34\n",
            "[2465 | 2373.24] loss=2.14 avg=2.34\n",
            "[2466 | 2376.38] loss=2.06 avg=2.33\n",
            "[2467 | 2379.51] loss=2.15 avg=2.33\n",
            "[2468 | 2382.63] loss=2.36 avg=2.33\n",
            "[2469 | 2385.74] loss=2.38 avg=2.33\n",
            "[2470 | 2388.90] loss=2.70 avg=2.34\n",
            "[2471 | 2392.02] loss=2.12 avg=2.33\n",
            "[2472 | 2395.17] loss=2.05 avg=2.33\n",
            "[2473 | 2398.29] loss=2.71 avg=2.34\n",
            "[2474 | 2401.37] loss=2.52 avg=2.34\n",
            "[2475 | 2404.49] loss=2.44 avg=2.34\n",
            "[2476 | 2407.58] loss=2.18 avg=2.34\n",
            "[2477 | 2410.72] loss=2.19 avg=2.34\n",
            "[2478 | 2413.83] loss=2.47 avg=2.34\n",
            "[2479 | 2416.95] loss=2.56 avg=2.34\n",
            "[2480 | 2420.06] loss=2.48 avg=2.34\n",
            "[2481 | 2423.18] loss=2.78 avg=2.34\n",
            "[2482 | 2426.29] loss=2.95 avg=2.35\n",
            "[2483 | 2429.42] loss=2.68 avg=2.35\n",
            "[2484 | 2432.56] loss=2.01 avg=2.35\n",
            "[2485 | 2435.67] loss=2.55 avg=2.35\n",
            "[2486 | 2438.79] loss=2.49 avg=2.35\n",
            "[2487 | 2441.87] loss=2.03 avg=2.35\n",
            "[2488 | 2444.96] loss=2.73 avg=2.35\n",
            "[2489 | 2448.07] loss=2.36 avg=2.35\n",
            "[2490 | 2451.16] loss=2.32 avg=2.35\n",
            "[2491 | 2454.25] loss=1.90 avg=2.35\n",
            "[2492 | 2457.36] loss=2.34 avg=2.35\n",
            "[2493 | 2460.45] loss=2.76 avg=2.35\n",
            "[2494 | 2463.54] loss=2.46 avg=2.35\n",
            "[2495 | 2466.64] loss=2.06 avg=2.35\n",
            "[2496 | 2469.74] loss=2.21 avg=2.35\n",
            "[2497 | 2472.84] loss=2.32 avg=2.35\n",
            "[2498 | 2475.94] loss=2.33 avg=2.35\n",
            "[2499 | 2479.03] loss=1.80 avg=2.34\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "ukel: yep, i just do the installer. which version of ubuntu ?\n",
            "(1239226060) wols: debian based\n",
            "\n",
            "\n",
            "(1325671700) geej: hi all and how do i check aptitude for version\n",
            "(1325671700) geej: i need a good one\n",
            "(1325671700) geej: no gui\n",
            "(1325671700) kram: aptitude\n",
            "\n",
            "\n",
            "(1273907380) xe2: anyone?\n",
            "(1273907380) xe2: anyone can help me with that\n",
            "(1273907380) ikonia: you are in #flood, that is ubuntu support only\n",
            "\n",
            "\n",
            "(1183727180) w33: how to use the ubuntu virtualbox? i have an external harddrive that i cant boot on in vmware, do i have to reconfigure the host?\n",
            "(1183727240) w33: anyone ?\n",
            "(1183727420) bernz: why dont you just boot on the host and try to play the VM\n",
            "\n",
            "\n",
            "(1248129500) kjell: someone tell me how to change the default resolution for the panel from 1440x900 to 1280x1024?\n",
            "(1248129420) fx00k: i'd try sudo update-grub &&\n",
            "(1248129480) fx00k:  i found a guide to that on a wiki with a few links.   you should try it if you want the same display resolution (or at least the same horizontal resolution)\n",
            "(1248129540) fx00k:  it works without doing a reboot.\n",
            "\n",
            "\n",
            "(1262401120) fudge: Is there an alternative way to upgrade to 10.10 with Ubuntu?\n",
            "(1262401180) fudge: Does anyone know of a better way to upgrade?\n",
            "(1262401180) LjL: try to upgrade your OS\n",
            "(1262401240) LjL: you can upgrade it to a newer version later on then the 10.10 upgrade\n",
            "\n",
            "\n",
            "(1233297440) wp: what's the best way to add gtk-2.0 ?\n",
            "(1233297440) brian00:  I'm using gnome-gtk.\n",
            "(1233297500) brian00:  it's a newer gnome alternative instead.  I want it to have a GUI with gtk+\n",
            "(1233297560) brian00:  I got the 3.5 version in apt to add gtk+ (which is just a synaptic extension) and it looks great.\n",
            "(1233297560) wp: is there a way to do that without rewording the gtk+ config in ~/.gnome/Config/default.gtt to use it in the GUI configuration editor for that?\n",
            "(1233297520) brian00:  if using gnome i'm guessing.\n",
            "\n",
            "\n",
            "(1221366160) rp_: Hey, I need help setting up a VGA (x86, ATI) adapter to enable nvidia drivers. I want to enable the nvidia proprietary driver from radeon driver. Is there a way to do that (and it's not just nv driver?):  'sudo apt-get install nvidia-glx'\n",
            "(1221366220) rp_: hey, I need help setting up a VGA (x86, ATI) adapter to enable nvidia drivers. I want to enable the nvidia proprietary driver from radeon driver. Is there a way to do that (and it's not just nvidia driver?):  'sudo apt-get install nvidia-glx'?\n",
            "(1221366220) rp_: and how can I do this?\n",
            "(1221366280) xangua:  xangua:  'sudo apt-get install nvidia-glx'\n",
            "\n",
            "\n",
            "(1116884980) n00b: hi. can i do a vnc server\n",
            "(1116884980) n00b: i've already done it before\n",
            "(1116885000) n00b: so how do i install and get the correct package\n",
            "(1116885000) fyrestrtr: try running:    make -j <number>               1:1               'nvidia-glx/'      # 'nvidia-glx/2.0.log'                      13       \n",
            "\n",
            "[2500 | 2520.67] loss=2.35 avg=2.34\n",
            "[2501 | 2523.80] loss=2.30 avg=2.34\n",
            "[2502 | 2526.92] loss=2.31 avg=2.34\n",
            "[2503 | 2530.05] loss=1.90 avg=2.34\n",
            "[2504 | 2533.15] loss=2.20 avg=2.34\n",
            "[2505 | 2536.24] loss=2.27 avg=2.34\n",
            "[2506 | 2539.34] loss=2.71 avg=2.34\n",
            "[2507 | 2542.41] loss=1.85 avg=2.34\n",
            "[2508 | 2545.48] loss=2.47 avg=2.34\n",
            "[2509 | 2548.54] loss=2.63 avg=2.34\n",
            "[2510 | 2551.62] loss=2.32 avg=2.34\n",
            "[2511 | 2554.71] loss=2.54 avg=2.34\n",
            "[2512 | 2557.79] loss=2.13 avg=2.34\n",
            "[2513 | 2560.83] loss=2.35 avg=2.34\n",
            "[2514 | 2563.87] loss=2.20 avg=2.34\n",
            "[2515 | 2566.90] loss=2.06 avg=2.34\n",
            "[2516 | 2569.94] loss=2.65 avg=2.34\n",
            "[2517 | 2572.98] loss=2.29 avg=2.34\n",
            "[2518 | 2576.05] loss=2.37 avg=2.34\n",
            "[2519 | 2579.14] loss=2.47 avg=2.34\n",
            "[2520 | 2582.25] loss=2.79 avg=2.34\n",
            "[2521 | 2585.32] loss=2.93 avg=2.35\n",
            "[2522 | 2588.39] loss=2.70 avg=2.35\n",
            "[2523 | 2591.47] loss=2.27 avg=2.35\n",
            "[2524 | 2594.58] loss=2.39 avg=2.35\n",
            "[2525 | 2597.66] loss=2.58 avg=2.36\n",
            "[2526 | 2600.77] loss=2.43 avg=2.36\n",
            "[2527 | 2603.88] loss=2.73 avg=2.36\n",
            "[2528 | 2606.96] loss=2.85 avg=2.37\n",
            "[2529 | 2610.07] loss=2.18 avg=2.36\n",
            "[2530 | 2613.19] loss=2.64 avg=2.37\n",
            "[2531 | 2616.30] loss=2.04 avg=2.36\n",
            "[2532 | 2619.42] loss=2.02 avg=2.36\n",
            "[2533 | 2622.52] loss=2.30 avg=2.36\n",
            "[2534 | 2625.64] loss=2.19 avg=2.36\n",
            "[2535 | 2628.79] loss=2.31 avg=2.36\n",
            "[2536 | 2631.92] loss=2.24 avg=2.36\n",
            "[2537 | 2635.06] loss=2.82 avg=2.36\n",
            "[2538 | 2638.18] loss=2.39 avg=2.36\n",
            "[2539 | 2641.28] loss=2.36 avg=2.36\n",
            "[2540 | 2644.41] loss=2.21 avg=2.36\n",
            "[2541 | 2647.50] loss=2.20 avg=2.36\n",
            "[2542 | 2650.64] loss=2.08 avg=2.35\n",
            "[2543 | 2653.77] loss=2.79 avg=2.36\n",
            "[2544 | 2656.92] loss=2.37 avg=2.36\n",
            "[2545 | 2660.05] loss=1.95 avg=2.35\n",
            "[2546 | 2663.16] loss=2.04 avg=2.35\n",
            "[2547 | 2666.31] loss=2.23 avg=2.35\n",
            "[2548 | 2669.40] loss=2.54 avg=2.35\n",
            "[2549 | 2672.51] loss=2.19 avg=2.35\n",
            "[2550 | 2675.63] loss=2.47 avg=2.35\n",
            "[2551 | 2678.74] loss=2.06 avg=2.35\n",
            "[2552 | 2681.86] loss=2.21 avg=2.35\n",
            "[2553 | 2684.99] loss=2.96 avg=2.35\n",
            "[2554 | 2688.12] loss=1.97 avg=2.35\n",
            "[2555 | 2691.25] loss=2.34 avg=2.35\n",
            "[2556 | 2694.39] loss=2.45 avg=2.35\n",
            "[2557 | 2697.53] loss=2.54 avg=2.35\n",
            "[2558 | 2700.63] loss=1.86 avg=2.35\n",
            "[2559 | 2703.77] loss=2.46 avg=2.35\n",
            "[2560 | 2706.92] loss=2.40 avg=2.35\n",
            "[2561 | 2710.04] loss=2.32 avg=2.35\n",
            "[2562 | 2713.20] loss=2.12 avg=2.35\n",
            "[2563 | 2716.35] loss=2.38 avg=2.35\n",
            "[2564 | 2719.51] loss=2.22 avg=2.35\n",
            "[2565 | 2722.63] loss=2.75 avg=2.35\n",
            "[2566 | 2725.77] loss=2.16 avg=2.35\n",
            "[2567 | 2728.89] loss=1.94 avg=2.34\n",
            "[2568 | 2732.02] loss=2.40 avg=2.34\n",
            "[2569 | 2735.15] loss=2.75 avg=2.35\n",
            "[2570 | 2738.25] loss=2.20 avg=2.35\n",
            "[2571 | 2741.35] loss=2.25 avg=2.35\n",
            "[2572 | 2744.47] loss=2.50 avg=2.35\n",
            "[2573 | 2747.60] loss=2.10 avg=2.35\n",
            "[2574 | 2750.72] loss=2.15 avg=2.34\n",
            "[2575 | 2753.82] loss=2.29 avg=2.34\n",
            "[2576 | 2756.90] loss=2.05 avg=2.34\n",
            "[2577 | 2759.97] loss=2.25 avg=2.34\n",
            "[2578 | 2763.04] loss=2.13 avg=2.34\n",
            "[2579 | 2766.11] loss=2.99 avg=2.34\n",
            "[2580 | 2769.18] loss=2.04 avg=2.34\n",
            "[2581 | 2772.27] loss=2.19 avg=2.34\n",
            "[2582 | 2775.33] loss=2.44 avg=2.34\n",
            "[2583 | 2778.42] loss=2.43 avg=2.34\n",
            "[2584 | 2781.49] loss=2.48 avg=2.34\n",
            "[2585 | 2784.57] loss=2.23 avg=2.34\n",
            "[2586 | 2787.64] loss=2.36 avg=2.34\n",
            "[2587 | 2790.72] loss=2.55 avg=2.34\n",
            "[2588 | 2793.80] loss=1.89 avg=2.34\n",
            "[2589 | 2796.88] loss=2.07 avg=2.34\n",
            "[2590 | 2799.95] loss=1.95 avg=2.33\n",
            "[2591 | 2803.02] loss=2.35 avg=2.33\n",
            "[2592 | 2806.09] loss=2.25 avg=2.33\n",
            "[2593 | 2809.14] loss=2.12 avg=2.33\n",
            "[2594 | 2812.22] loss=2.25 avg=2.33\n",
            "[2595 | 2815.28] loss=2.74 avg=2.33\n",
            "[2596 | 2818.35] loss=1.86 avg=2.33\n",
            "[2597 | 2821.44] loss=1.70 avg=2.32\n",
            "[2598 | 2824.51] loss=2.05 avg=2.32\n",
            "[2599 | 2827.58] loss=2.28 avg=2.32\n",
            "[2600 | 2830.67] loss=2.48 avg=2.32\n",
            "[2601 | 2833.75] loss=2.65 avg=2.32\n",
            "[2602 | 2836.83] loss=2.19 avg=2.32\n",
            "[2603 | 2839.91] loss=2.23 avg=2.32\n",
            "[2604 | 2842.96] loss=2.22 avg=2.32\n",
            "[2605 | 2846.04] loss=2.60 avg=2.32\n",
            "[2606 | 2849.11] loss=2.60 avg=2.33\n",
            "[2607 | 2852.18] loss=2.29 avg=2.33\n",
            "[2608 | 2855.25] loss=2.08 avg=2.32\n",
            "[2609 | 2858.33] loss=2.53 avg=2.33\n",
            "[2610 | 2861.40] loss=1.81 avg=2.32\n",
            "[2611 | 2864.47] loss=2.00 avg=2.32\n",
            "[2612 | 2867.55] loss=2.45 avg=2.32\n",
            "[2613 | 2870.63] loss=2.35 avg=2.32\n",
            "[2614 | 2873.71] loss=1.92 avg=2.31\n",
            "[2615 | 2876.80] loss=2.47 avg=2.32\n",
            "[2616 | 2879.87] loss=2.05 avg=2.31\n",
            "[2617 | 2882.95] loss=2.26 avg=2.31\n",
            "[2618 | 2886.02] loss=2.27 avg=2.31\n",
            "[2619 | 2889.10] loss=3.05 avg=2.32\n",
            "[2620 | 2892.16] loss=1.94 avg=2.32\n",
            "[2621 | 2895.24] loss=2.14 avg=2.31\n",
            "[2622 | 2898.35] loss=2.27 avg=2.31\n",
            "[2623 | 2901.41] loss=1.87 avg=2.31\n",
            "[2624 | 2904.50] loss=2.56 avg=2.31\n",
            "[2625 | 2907.62] loss=2.80 avg=2.32\n",
            "[2626 | 2910.73] loss=2.69 avg=2.32\n",
            "[2627 | 2913.87] loss=2.00 avg=2.32\n",
            "[2628 | 2916.98] loss=2.06 avg=2.31\n",
            "[2629 | 2920.09] loss=2.42 avg=2.32\n",
            "[2630 | 2923.19] loss=2.33 avg=2.32\n",
            "[2631 | 2926.32] loss=2.24 avg=2.32\n",
            "[2632 | 2929.43] loss=2.77 avg=2.32\n",
            "[2633 | 2932.55] loss=2.44 avg=2.32\n",
            "[2634 | 2935.66] loss=2.42 avg=2.32\n",
            "[2635 | 2938.79] loss=2.04 avg=2.32\n",
            "[2636 | 2941.90] loss=2.37 avg=2.32\n",
            "[2637 | 2945.05] loss=2.31 avg=2.32\n",
            "[2638 | 2948.16] loss=2.36 avg=2.32\n",
            "[2639 | 2951.25] loss=2.25 avg=2.32\n",
            "[2640 | 2954.38] loss=2.45 avg=2.32\n",
            "[2641 | 2957.51] loss=2.25 avg=2.32\n",
            "[2642 | 2960.62] loss=2.24 avg=2.32\n",
            "[2643 | 2963.71] loss=1.93 avg=2.32\n",
            "[2644 | 2966.83] loss=2.01 avg=2.31\n",
            "[2645 | 2969.92] loss=2.65 avg=2.32\n",
            "[2646 | 2973.02] loss=2.20 avg=2.31\n",
            "[2647 | 2976.14] loss=2.26 avg=2.31\n",
            "[2648 | 2979.26] loss=2.32 avg=2.31\n",
            "[2649 | 2982.37] loss=2.82 avg=2.32\n",
            "[2650 | 2985.49] loss=2.68 avg=2.32\n",
            "[2651 | 2988.61] loss=2.19 avg=2.32\n",
            "[2652 | 2991.76] loss=2.50 avg=2.32\n",
            "[2653 | 2994.86] loss=2.36 avg=2.32\n",
            "[2654 | 2997.98] loss=2.57 avg=2.33\n",
            "[2655 | 3001.11] loss=2.35 avg=2.33\n",
            "[2656 | 3004.21] loss=2.33 avg=2.33\n",
            "[2657 | 3007.33] loss=2.02 avg=2.32\n",
            "[2658 | 3010.45] loss=2.32 avg=2.32\n",
            "[2659 | 3013.57] loss=2.24 avg=2.32\n",
            "[2660 | 3016.68] loss=2.28 avg=2.32\n",
            "[2661 | 3019.77] loss=2.54 avg=2.32\n",
            "[2662 | 3022.90] loss=2.50 avg=2.33\n",
            "[2663 | 3026.04] loss=2.48 avg=2.33\n",
            "[2664 | 3029.17] loss=2.38 avg=2.33\n",
            "[2665 | 3032.30] loss=2.09 avg=2.33\n",
            "[2666 | 3035.44] loss=2.07 avg=2.32\n",
            "[2667 | 3038.56] loss=2.13 avg=2.32\n",
            "[2668 | 3041.66] loss=2.28 avg=2.32\n",
            "[2669 | 3044.81] loss=2.29 avg=2.32\n",
            "[2670 | 3047.92] loss=2.11 avg=2.32\n",
            "[2671 | 3051.06] loss=2.14 avg=2.32\n",
            "[2672 | 3054.19] loss=2.27 avg=2.32\n",
            "[2673 | 3057.33] loss=2.38 avg=2.32\n",
            "[2674 | 3060.44] loss=2.06 avg=2.31\n",
            "[2675 | 3063.54] loss=1.97 avg=2.31\n",
            "[2676 | 3066.61] loss=2.76 avg=2.31\n",
            "[2677 | 3069.71] loss=2.43 avg=2.32\n",
            "[2678 | 3072.81] loss=2.22 avg=2.31\n",
            "[2679 | 3075.89] loss=2.19 avg=2.31\n",
            "[2680 | 3078.98] loss=2.15 avg=2.31\n",
            "[2681 | 3082.08] loss=2.19 avg=2.31\n",
            "[2682 | 3085.17] loss=2.70 avg=2.31\n",
            "[2683 | 3088.26] loss=2.47 avg=2.32\n",
            "[2684 | 3091.34] loss=2.01 avg=2.31\n",
            "[2685 | 3094.44] loss=2.17 avg=2.31\n",
            "[2686 | 3097.52] loss=2.49 avg=2.31\n",
            "[2687 | 3100.61] loss=2.75 avg=2.32\n",
            "[2688 | 3103.68] loss=2.78 avg=2.32\n",
            "[2689 | 3106.78] loss=2.16 avg=2.32\n",
            "[2690 | 3109.85] loss=2.28 avg=2.32\n",
            "[2691 | 3112.94] loss=2.72 avg=2.32\n",
            "[2692 | 3116.01] loss=2.26 avg=2.32\n",
            "[2693 | 3119.09] loss=2.82 avg=2.33\n",
            "[2694 | 3122.19] loss=1.53 avg=2.32\n",
            "[2695 | 3125.28] loss=2.58 avg=2.32\n",
            "[2696 | 3128.37] loss=2.19 avg=2.32\n",
            "[2697 | 3131.46] loss=2.45 avg=2.32\n",
            "[2698 | 3134.53] loss=2.83 avg=2.33\n",
            "[2699 | 3137.60] loss=2.20 avg=2.33\n",
            "[2700 | 3140.71] loss=2.95 avg=2.33\n",
            "[2701 | 3143.78] loss=2.30 avg=2.33\n",
            "[2702 | 3146.87] loss=2.59 avg=2.34\n",
            "[2703 | 3149.98] loss=2.27 avg=2.33\n",
            "[2704 | 3153.05] loss=2.42 avg=2.34\n",
            "[2705 | 3156.14] loss=2.06 avg=2.33\n",
            "[2706 | 3159.23] loss=2.62 avg=2.34\n",
            "[2707 | 3162.33] loss=2.75 avg=2.34\n",
            "[2708 | 3165.43] loss=2.32 avg=2.34\n",
            "[2709 | 3168.51] loss=2.67 avg=2.34\n",
            "[2710 | 3171.59] loss=2.44 avg=2.34\n",
            "[2711 | 3174.70] loss=2.19 avg=2.34\n",
            "[2712 | 3177.79] loss=2.59 avg=2.35\n",
            "[2713 | 3180.90] loss=2.85 avg=2.35\n",
            "[2714 | 3184.05] loss=2.91 avg=2.36\n",
            "[2715 | 3187.17] loss=2.36 avg=2.36\n",
            "[2716 | 3190.28] loss=2.18 avg=2.35\n",
            "[2717 | 3193.41] loss=2.56 avg=2.36\n",
            "[2718 | 3196.53] loss=2.07 avg=2.35\n",
            "[2719 | 3199.65] loss=1.77 avg=2.35\n",
            "[2720 | 3202.75] loss=1.75 avg=2.34\n",
            "[2721 | 3205.87] loss=2.13 avg=2.34\n",
            "[2722 | 3209.01] loss=2.27 avg=2.34\n",
            "[2723 | 3212.14] loss=1.85 avg=2.33\n",
            "[2724 | 3215.28] loss=2.76 avg=2.34\n",
            "[2725 | 3218.40] loss=2.22 avg=2.34\n",
            "[2726 | 3221.53] loss=2.36 avg=2.34\n",
            "[2727 | 3224.64] loss=1.83 avg=2.33\n",
            "[2728 | 3227.77] loss=2.31 avg=2.33\n",
            "[2729 | 3230.91] loss=2.60 avg=2.33\n",
            "[2730 | 3234.05] loss=1.99 avg=2.33\n",
            "[2731 | 3237.17] loss=2.28 avg=2.33\n",
            "[2732 | 3240.29] loss=2.49 avg=2.33\n",
            "[2733 | 3243.38] loss=2.17 avg=2.33\n",
            "[2734 | 3246.50] loss=2.43 avg=2.33\n",
            "[2735 | 3249.65] loss=2.42 avg=2.33\n",
            "[2736 | 3252.75] loss=2.86 avg=2.34\n",
            "[2737 | 3255.88] loss=2.13 avg=2.34\n",
            "[2738 | 3259.03] loss=2.30 avg=2.34\n",
            "[2739 | 3262.13] loss=2.56 avg=2.34\n",
            "[2740 | 3265.25] loss=2.82 avg=2.34\n",
            "[2741 | 3268.38] loss=2.93 avg=2.35\n",
            "[2742 | 3271.51] loss=2.82 avg=2.35\n",
            "[2743 | 3274.66] loss=2.13 avg=2.35\n",
            "[2744 | 3277.80] loss=2.19 avg=2.35\n",
            "[2745 | 3280.95] loss=2.05 avg=2.35\n",
            "[2746 | 3284.08] loss=2.21 avg=2.34\n",
            "[2747 | 3287.22] loss=2.28 avg=2.34\n",
            "[2748 | 3290.34] loss=2.39 avg=2.34\n",
            "[2749 | 3293.47] loss=2.52 avg=2.35\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ".\n",
            "(1285888960) bazhang: sorry\n",
            "(1285888920) bazhang: sorry, you're out of line.\n",
            "(1285888980) bazhang: that works.\n",
            "(1285888980) bazhang: if you want to add a new user, add them to your group.\n",
            "(1285889100) B1gXRp: ok\n",
            "(1285889100) bazhang: what does it mean from /about/user to /msguser?\n",
            "(1285889160) B1GXRp:   I'm just getting the message. 'help wanted and a user is already in the group'\n",
            "\n",
            "\n",
            "(1244161280) achowd: ive created a new account for ubuntu 6.04 using the live cd\n",
            "(1244161280) achowd: it doesn't let me edit a user's profile\n",
            "(1244161280) achowd: what will i do\n",
            "(1244161280) achowd: i created a new account for ubuntu 6.06 using the live cd\n",
            "(1244161280) achowd: i deleted everything i did prior to the installation of ubuntu\n",
            "(1244161280) achowd: and cant seem to edit my user's profile\n",
            "(1244161280) achowd: how do i do that or edit my user's existing one?\n",
            "(1244161280) achowd: ive created a new account for ubuntu 6.04 using the live cd\n",
            "(1244161280) achowd: i got the whole system so i want to create a new user using the live cd\n",
            "(1244161280) achow: what should i do?\n",
            "(1244161280) achowd: im trying to create a dual boot computer\n",
            "(1244161280) achow: i cant get a user profile set up on the other machine... that's it\n",
            "(1244161280) Astaroth:  you can go through the useradd method\n",
            "\n",
            "\n",
            "(1290157800) sjvv: hi all\n",
            "(1290157800) sjv: i need a script with ubuntu and windows to be used at the same time\n",
            "(1290157800) sjv: i want to create an ubuntu vista ct of all files\n",
            "(1290157800) sjv: for instance\n",
            "(1290157800) sjv: when i go to the directory\n",
            "(1290157860) jrib: that's the command line, if you want to add that to existing existing scripts.\n",
            "(1290157860) sjv: i need some kind of virtual box\n",
            "(1290157980) tonyyarusso: try VBox\n",
            "\n",
            "\n",
            "(1243533800) cllin: i want to connect to my windows machine\n",
            "(1243533560) nubbs: do you have a windows harddrive?\n",
            "(1243533620) nubbs: hm.\n",
            "(1243534140) nubbs: yeah\n",
            "(1243534200) nubbs: you have a .iso or a .run file?\n",
            "(1243534260) nubbs: so you want to run that image file?\n",
            "(1243534260) nubbs: you can create a .zip file or .run from your windows machine, just make sure you have it properly installed.\n",
            "(1243534320) nubbs: yeah, then you may get a weird one\n",
            "(1243534320) nubbs: try 'vmd' , it lets you copy that file to that directory .\n",
            "(1243534380) nubbs: sorry vmd\n",
            "(1243534380) nubbs: vmd can be used in Ubuntu or in any OS\n",
            "\n",
            "\n",
            "(1150207400) the_guru: is it possible to run a script from the CLI or is it very difficult / impossible?\n",
            "(1150207520) fde:  it's difficult\n",
            "(1150251640) fde:  you can create multiple scripts.\n",
            "(1150251600) the_guru: thanks\n",
            "\n",
            "\n",
            "(1159095760) goth: I've been reading about it for awhile.. I haven't run into any problems.. I am going to try installing Ubuntu tomorrow.. and the new packages I did during install..\n",
            "(1159095820) goth: I want it to be in a drive..  it should be able to use any OS.. or not.. my main OS is windows\n",
            "(1159095820) erUSUL: is it\n",
            "\n",
            "[2750 | 3335.63] loss=2.07 avg=2.34\n",
            "[2751 | 3338.75] loss=2.24 avg=2.34\n",
            "[2752 | 3341.83] loss=2.18 avg=2.34\n",
            "[2753 | 3344.95] loss=2.73 avg=2.34\n",
            "[2754 | 3348.02] loss=2.57 avg=2.35\n",
            "[2755 | 3351.08] loss=2.33 avg=2.35\n",
            "[2756 | 3354.15] loss=2.63 avg=2.35\n",
            "[2757 | 3357.20] loss=2.70 avg=2.35\n",
            "[2758 | 3360.28] loss=2.39 avg=2.35\n",
            "[2759 | 3363.35] loss=2.50 avg=2.35\n",
            "[2760 | 3366.40] loss=2.84 avg=2.36\n",
            "[2761 | 3369.47] loss=1.91 avg=2.36\n",
            "[2762 | 3372.55] loss=2.56 avg=2.36\n",
            "[2763 | 3375.63] loss=2.27 avg=2.36\n",
            "[2764 | 3378.69] loss=2.22 avg=2.35\n",
            "[2765 | 3381.80] loss=2.64 avg=2.36\n",
            "[2766 | 3384.89] loss=2.27 avg=2.36\n",
            "[2767 | 3387.97] loss=2.17 avg=2.35\n",
            "[2768 | 3391.06] loss=2.55 avg=2.36\n",
            "[2769 | 3394.12] loss=2.28 avg=2.36\n",
            "[2770 | 3397.20] loss=3.15 avg=2.36\n",
            "[2771 | 3400.27] loss=2.36 avg=2.36\n",
            "[2772 | 3403.37] loss=2.34 avg=2.36\n",
            "[2773 | 3406.45] loss=2.59 avg=2.37\n",
            "[2774 | 3409.54] loss=2.42 avg=2.37\n",
            "[2775 | 3412.62] loss=2.30 avg=2.37\n",
            "[2776 | 3415.70] loss=2.04 avg=2.36\n",
            "[2777 | 3418.77] loss=2.24 avg=2.36\n",
            "[2778 | 3421.85] loss=2.57 avg=2.36\n",
            "[2779 | 3424.94] loss=2.25 avg=2.36\n",
            "[2780 | 3428.00] loss=2.25 avg=2.36\n",
            "[2781 | 3431.07] loss=2.12 avg=2.36\n",
            "[2782 | 3434.14] loss=2.11 avg=2.36\n",
            "[2783 | 3437.23] loss=2.35 avg=2.36\n",
            "[2784 | 3440.33] loss=2.14 avg=2.35\n",
            "[2785 | 3443.41] loss=2.61 avg=2.36\n",
            "[2786 | 3446.50] loss=2.30 avg=2.36\n",
            "[2787 | 3449.60] loss=2.03 avg=2.35\n",
            "[2788 | 3452.67] loss=2.82 avg=2.36\n",
            "[2789 | 3455.75] loss=2.11 avg=2.36\n",
            "[2790 | 3458.84] loss=2.29 avg=2.35\n",
            "[2791 | 3461.91] loss=2.20 avg=2.35\n",
            "[2792 | 3464.98] loss=2.56 avg=2.35\n",
            "[2793 | 3468.06] loss=2.18 avg=2.35\n",
            "[2794 | 3471.14] loss=1.96 avg=2.35\n",
            "[2795 | 3474.22] loss=2.53 avg=2.35\n",
            "[2796 | 3477.30] loss=2.26 avg=2.35\n",
            "[2797 | 3480.38] loss=2.20 avg=2.35\n",
            "[2798 | 3483.45] loss=2.09 avg=2.35\n",
            "[2799 | 3486.52] loss=2.08 avg=2.34\n",
            "[2800 | 3489.64] loss=2.73 avg=2.35\n",
            "[2801 | 3492.75] loss=2.05 avg=2.34\n",
            "[2802 | 3495.90] loss=2.43 avg=2.35\n",
            "[2803 | 3499.02] loss=2.27 avg=2.34\n",
            "[2804 | 3502.13] loss=2.30 avg=2.34\n",
            "[2805 | 3505.23] loss=2.53 avg=2.35\n",
            "[2806 | 3508.37] loss=2.56 avg=2.35\n",
            "[2807 | 3511.47] loss=2.67 avg=2.35\n",
            "[2808 | 3514.61] loss=2.10 avg=2.35\n",
            "[2809 | 3517.72] loss=2.12 avg=2.35\n",
            "[2810 | 3520.84] loss=2.77 avg=2.35\n",
            "[2811 | 3523.98] loss=2.65 avg=2.35\n",
            "[2812 | 3527.13] loss=2.62 avg=2.36\n",
            "[2813 | 3530.24] loss=2.18 avg=2.35\n",
            "[2814 | 3533.35] loss=2.16 avg=2.35\n",
            "[2815 | 3536.45] loss=1.99 avg=2.35\n",
            "[2816 | 3539.56] loss=2.55 avg=2.35\n",
            "[2817 | 3542.69] loss=2.38 avg=2.35\n",
            "[2818 | 3545.81] loss=2.20 avg=2.35\n",
            "[2819 | 3548.92] loss=2.41 avg=2.35\n",
            "[2820 | 3551.99] loss=2.62 avg=2.35\n",
            "[2821 | 3555.11] loss=2.00 avg=2.35\n",
            "[2822 | 3558.20] loss=2.33 avg=2.35\n",
            "[2823 | 3561.30] loss=1.99 avg=2.35\n",
            "[2824 | 3564.41] loss=2.12 avg=2.34\n",
            "[2825 | 3567.53] loss=2.30 avg=2.34\n",
            "[2826 | 3570.66] loss=2.57 avg=2.35\n",
            "[2827 | 3573.76] loss=2.53 avg=2.35\n",
            "[2828 | 3576.85] loss=2.61 avg=2.35\n",
            "[2829 | 3579.95] loss=2.28 avg=2.35\n",
            "[2830 | 3583.05] loss=2.82 avg=2.35\n",
            "[2831 | 3586.13] loss=2.46 avg=2.35\n",
            "[2832 | 3589.22] loss=2.37 avg=2.35\n",
            "[2833 | 3592.34] loss=2.61 avg=2.36\n",
            "[2834 | 3595.43] loss=2.27 avg=2.36\n",
            "[2835 | 3598.56] loss=2.36 avg=2.36\n",
            "[2836 | 3601.66] loss=2.11 avg=2.35\n",
            "[2837 | 3604.78] loss=2.43 avg=2.35\n",
            "[2838 | 3607.87] loss=2.45 avg=2.36\n",
            "[2839 | 3610.96] loss=2.38 avg=2.36\n",
            "[2840 | 3614.06] loss=3.00 avg=2.36\n",
            "[2841 | 3617.17] loss=1.80 avg=2.36\n",
            "[2842 | 3620.27] loss=2.75 avg=2.36\n",
            "[2843 | 3623.35] loss=2.35 avg=2.36\n",
            "[2844 | 3626.46] loss=2.41 avg=2.36\n",
            "[2845 | 3629.58] loss=2.02 avg=2.36\n",
            "[2846 | 3632.66] loss=2.72 avg=2.36\n",
            "[2847 | 3635.77] loss=2.41 avg=2.36\n",
            "[2848 | 3638.89] loss=2.07 avg=2.36\n",
            "[2849 | 3641.96] loss=2.42 avg=2.36\n",
            "[2850 | 3645.04] loss=2.31 avg=2.36\n",
            "[2851 | 3648.12] loss=2.18 avg=2.36\n",
            "[2852 | 3651.20] loss=2.05 avg=2.35\n",
            "[2853 | 3654.32] loss=2.36 avg=2.35\n",
            "[2854 | 3657.43] loss=2.63 avg=2.36\n",
            "[2855 | 3660.56] loss=2.20 avg=2.36\n",
            "[2856 | 3663.65] loss=2.35 avg=2.36\n",
            "[2857 | 3666.76] loss=2.13 avg=2.35\n",
            "[2858 | 3669.89] loss=2.18 avg=2.35\n",
            "[2859 | 3673.00] loss=2.26 avg=2.35\n",
            "[2860 | 3676.14] loss=2.15 avg=2.35\n",
            "[2861 | 3679.26] loss=2.26 avg=2.35\n",
            "[2862 | 3682.38] loss=2.49 avg=2.35\n",
            "[2863 | 3685.50] loss=2.30 avg=2.35\n",
            "[2864 | 3688.61] loss=2.32 avg=2.35\n",
            "[2865 | 3691.73] loss=2.74 avg=2.35\n",
            "[2866 | 3694.85] loss=2.51 avg=2.35\n",
            "[2867 | 3697.97] loss=2.25 avg=2.35\n",
            "[2868 | 3701.07] loss=2.60 avg=2.36\n",
            "[2869 | 3704.16] loss=2.59 avg=2.36\n",
            "[2870 | 3707.27] loss=2.10 avg=2.35\n",
            "[2871 | 3710.37] loss=2.80 avg=2.36\n",
            "[2872 | 3713.47] loss=2.21 avg=2.36\n",
            "[2873 | 3716.59] loss=1.91 avg=2.35\n",
            "[2874 | 3719.71] loss=2.38 avg=2.35\n",
            "[2875 | 3722.82] loss=2.64 avg=2.36\n",
            "[2876 | 3725.93] loss=2.28 avg=2.36\n",
            "[2877 | 3729.03] loss=2.09 avg=2.35\n",
            "[2878 | 3732.11] loss=2.05 avg=2.35\n",
            "[2879 | 3735.19] loss=1.94 avg=2.35\n",
            "[2880 | 3738.29] loss=2.35 avg=2.35\n",
            "[2881 | 3741.39] loss=2.11 avg=2.34\n",
            "[2882 | 3744.48] loss=2.03 avg=2.34\n",
            "[2883 | 3747.56] loss=1.77 avg=2.33\n",
            "[2884 | 3750.65] loss=2.14 avg=2.33\n",
            "[2885 | 3753.73] loss=2.34 avg=2.33\n",
            "[2886 | 3756.80] loss=2.19 avg=2.33\n",
            "[2887 | 3759.90] loss=2.32 avg=2.33\n",
            "[2888 | 3762.99] loss=1.91 avg=2.33\n",
            "[2889 | 3766.08] loss=2.35 avg=2.33\n",
            "[2890 | 3769.16] loss=2.22 avg=2.33\n",
            "[2891 | 3772.23] loss=2.10 avg=2.32\n",
            "[2892 | 3775.30] loss=2.46 avg=2.33\n",
            "[2893 | 3778.39] loss=1.99 avg=2.32\n",
            "[2894 | 3781.50] loss=2.33 avg=2.32\n",
            "[2895 | 3784.60] loss=2.24 avg=2.32\n",
            "[2896 | 3787.70] loss=2.81 avg=2.33\n",
            "[2897 | 3790.79] loss=1.92 avg=2.32\n",
            "[2898 | 3793.87] loss=2.08 avg=2.32\n",
            "[2899 | 3796.96] loss=1.60 avg=2.31\n",
            "[2900 | 3800.05] loss=2.25 avg=2.31\n",
            "[2901 | 3803.16] loss=2.66 avg=2.32\n",
            "[2902 | 3806.25] loss=2.78 avg=2.32\n",
            "[2903 | 3809.33] loss=2.51 avg=2.32\n",
            "[2904 | 3812.43] loss=2.85 avg=2.33\n",
            "[2905 | 3815.51] loss=1.86 avg=2.32\n",
            "[2906 | 3818.62] loss=2.82 avg=2.33\n",
            "[2907 | 3821.72] loss=2.77 avg=2.33\n",
            "[2908 | 3824.81] loss=2.25 avg=2.33\n",
            "[2909 | 3827.88] loss=2.48 avg=2.33\n",
            "[2910 | 3830.98] loss=2.01 avg=2.33\n",
            "[2911 | 3834.07] loss=2.34 avg=2.33\n",
            "[2912 | 3837.17] loss=2.32 avg=2.33\n",
            "[2913 | 3840.26] loss=2.33 avg=2.33\n",
            "[2914 | 3843.36] loss=2.23 avg=2.33\n",
            "[2915 | 3846.45] loss=2.50 avg=2.33\n",
            "[2916 | 3849.56] loss=2.42 avg=2.33\n",
            "[2917 | 3852.66] loss=1.99 avg=2.33\n",
            "[2918 | 3855.76] loss=2.21 avg=2.33\n",
            "[2919 | 3858.85] loss=2.25 avg=2.33\n",
            "[2920 | 3861.95] loss=2.21 avg=2.32\n",
            "[2921 | 3865.05] loss=2.58 avg=2.33\n",
            "[2922 | 3868.14] loss=2.60 avg=2.33\n",
            "[2923 | 3871.25] loss=2.20 avg=2.33\n",
            "[2924 | 3874.35] loss=2.22 avg=2.33\n",
            "[2925 | 3877.45] loss=2.13 avg=2.33\n",
            "[2926 | 3880.52] loss=2.20 avg=2.32\n",
            "[2927 | 3883.61] loss=2.52 avg=2.33\n",
            "[2928 | 3886.71] loss=2.63 avg=2.33\n",
            "[2929 | 3889.80] loss=2.39 avg=2.33\n",
            "[2930 | 3892.88] loss=2.21 avg=2.33\n",
            "[2931 | 3895.99] loss=2.38 avg=2.33\n",
            "[2932 | 3899.12] loss=2.07 avg=2.33\n",
            "[2933 | 3902.26] loss=2.25 avg=2.33\n",
            "[2934 | 3905.41] loss=2.33 avg=2.33\n",
            "[2935 | 3908.55] loss=2.15 avg=2.32\n",
            "[2936 | 3911.66] loss=2.29 avg=2.32\n",
            "[2937 | 3914.76] loss=2.19 avg=2.32\n",
            "[2938 | 3917.88] loss=2.58 avg=2.33\n",
            "[2939 | 3921.01] loss=2.15 avg=2.32\n",
            "[2940 | 3924.14] loss=2.25 avg=2.32\n",
            "[2941 | 3927.26] loss=2.06 avg=2.32\n",
            "[2942 | 3930.37] loss=2.00 avg=2.32\n",
            "[2943 | 3933.50] loss=1.98 avg=2.31\n",
            "[2944 | 3936.66] loss=2.35 avg=2.31\n",
            "[2945 | 3939.77] loss=1.98 avg=2.31\n",
            "[2946 | 3942.90] loss=2.39 avg=2.31\n",
            "[2947 | 3946.02] loss=1.82 avg=2.31\n",
            "[2948 | 3949.11] loss=2.63 avg=2.31\n",
            "[2949 | 3952.26] loss=2.19 avg=2.31\n",
            "[2950 | 3955.37] loss=2.17 avg=2.31\n",
            "[2951 | 3958.51] loss=2.32 avg=2.31\n",
            "[2952 | 3961.63] loss=2.24 avg=2.31\n",
            "[2953 | 3964.75] loss=1.82 avg=2.30\n",
            "[2954 | 3967.85] loss=1.91 avg=2.30\n",
            "[2955 | 3970.95] loss=2.09 avg=2.30\n",
            "[2956 | 3974.07] loss=2.18 avg=2.29\n",
            "[2957 | 3977.20] loss=2.47 avg=2.30\n",
            "[2958 | 3980.33] loss=2.03 avg=2.29\n",
            "[2959 | 3983.45] loss=2.02 avg=2.29\n",
            "[2960 | 3986.59] loss=2.34 avg=2.29\n",
            "[2961 | 3989.72] loss=2.28 avg=2.29\n",
            "[2962 | 3992.84] loss=2.38 avg=2.29\n",
            "[2963 | 3995.97] loss=2.17 avg=2.29\n",
            "[2964 | 3999.11] loss=2.27 avg=2.29\n",
            "[2965 | 4002.23] loss=2.96 avg=2.30\n",
            "[2966 | 4005.33] loss=2.33 avg=2.30\n",
            "[2967 | 4008.45] loss=2.45 avg=2.30\n",
            "[2968 | 4011.56] loss=2.52 avg=2.30\n",
            "[2969 | 4014.68] loss=2.07 avg=2.30\n",
            "[2970 | 4017.81] loss=2.74 avg=2.30\n",
            "[2971 | 4020.94] loss=2.39 avg=2.30\n",
            "[2972 | 4024.04] loss=1.99 avg=2.30\n",
            "[2973 | 4027.17] loss=2.57 avg=2.30\n",
            "[2974 | 4030.32] loss=2.89 avg=2.31\n",
            "[2975 | 4033.40] loss=3.08 avg=2.32\n",
            "[2976 | 4036.52] loss=2.50 avg=2.32\n",
            "[2977 | 4039.65] loss=2.21 avg=2.32\n",
            "[2978 | 4042.75] loss=2.52 avg=2.32\n",
            "[2979 | 4045.90] loss=2.01 avg=2.32\n",
            "[2980 | 4049.01] loss=2.47 avg=2.32\n",
            "[2981 | 4052.11] loss=2.07 avg=2.32\n",
            "[2982 | 4055.22] loss=2.44 avg=2.32\n",
            "[2983 | 4058.34] loss=2.53 avg=2.32\n",
            "[2984 | 4061.44] loss=2.19 avg=2.32\n",
            "[2985 | 4064.57] loss=2.58 avg=2.32\n",
            "[2986 | 4067.70] loss=2.08 avg=2.32\n",
            "[2987 | 4070.85] loss=2.50 avg=2.32\n",
            "[2988 | 4073.94] loss=2.30 avg=2.32\n",
            "[2989 | 4077.07] loss=2.84 avg=2.33\n",
            "[2990 | 4080.18] loss=2.38 avg=2.33\n",
            "[2991 | 4083.28] loss=2.70 avg=2.33\n",
            "[2992 | 4086.39] loss=2.37 avg=2.33\n",
            "[2993 | 4089.50] loss=2.88 avg=2.34\n",
            "[2994 | 4092.56] loss=1.80 avg=2.33\n",
            "[2995 | 4095.66] loss=2.26 avg=2.33\n",
            "[2996 | 4098.78] loss=2.31 avg=2.33\n",
            "[2997 | 4101.85] loss=2.42 avg=2.33\n",
            "[2998 | 4104.93] loss=2.25 avg=2.33\n",
            "[2999 | 4108.01] loss=2.00 avg=2.33\n",
            "Saving checkpoint/run1/model-3000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "1575952740) jimmyg:  it was already installed as long ago as last year so i'm quite confident it's working\n",
            "(1147121400) jimmyg: my xorg hangs on the first boot (I'm not on the latest one, and it didn't work, so I thought that was probably not the issue, though it could be)\n",
            "\n",
            "\n",
            "(1283628480) kyph: does anyone know if Ubuntu 6.10 does not work? I wanted to try out KDE and it works with GNOME, but I can't get any apps to play on it.\n",
            "(1283628480) Dr_willis:  you could try dpkg as that may help with that.\n",
            "(1283628480) kyph: i've tried that already; no work\n",
            "(1283628540) kyph: it's been worked for a few hours. i thought you could have something installed there, but nope\n",
            "(1283628600) kyph: what can I try? I'm new, so might not know the answer.\n",
            "(1283628600) kyph: I can try that by starting up synaptic and running 'aptitude install beryl'?\n",
            "(1283628720) kyph: i'll try apt-get install beryl, but i just downloaded it, it doesn't work ;/\n",
            "\n",
            "\n",
            "(1199486740) erUSUL: What's the latest version of a software like Firefox or Chrome? And, are there any ways to check the current version? Because, for example, I do want to know if the current version is 4.0 yet and whether it won't work in my current versions?\n",
            "(1199486800) erUSUL: What's the latest version of a software like Firefox or Chrome? And, are there any ways to check the current version? Because, for example, I do want to know if the current version is 4.0 yet and whether it won't work in my current versions?\n",
            "(1199486800) ActionParsnip: I have the latest (current?) version of Firefox\n",
            "(1199486860) erUSUL: thanks.\n",
            "\n",
            "\n",
            "(1211642640) nikid: Is Ubuntu a good OS?\n",
            "(1211642640) nikid: I prefer Xubuntu\n",
            "(1211642700) nikid: I think Xubuntu is a bit better than Windows\n",
            "(1211642700) wylie: yes\n",
            "\n",
            "\n",
            "(1179758980) chad: hello\n",
            "(1179759040) chad: does anyone know how to configure gdm to use the ubuntu theme? I only have one theme\n",
            "\n",
            "\n",
            "(1145171310) LjL: yes\n",
            "(1145171380) LjL: but he'll be better looking and I'd like him to know what 'normal' looks like\n",
            "(1145171380) LjL: what's  the difference between a normal theme and one used in a karmic or gentoo package?\n",
            "\n",
            "\n",
            "(1156839440) eirash: when a new update comes out, I'm worried what programs are not in there and when I try to install from updates site, there's no option to download the files from sources?\n",
            "(1156839440) Dr_Willis:  packages which require some downloading software may not be there.   ive used 'apt-get install' too many times and ive only need one package .  (for example, k2b, kde or xmms)\n",
            "(1156839600) ActionParsnip: kxmms is from the apt-get repos.\n",
            "\n",
            "\n",
            "(1111446940) jade: why does the terminal sometimes ask a password for the root account?\n",
            "(1111447060) gordonjcp: the password?\n",
            "(1111447120) Jade: sorry. Thanks.\n",
            "\n",
            "\n",
            "(1141359380) ianm: anyone know the exact error message i got from the site that will open up the page it asks me for to download the firmware? i had the same error message, but didn't click ok or anything like.\n",
            "(1141359380) ianm: it was a previous release.\n",
            "(1141359920) ActionParsnip:  I had that, but it wasn't for that version.\n",
            "\n",
            "\n",
            "(1202558280) ogra: what's up with those files? does windows work with them?\n",
            "(1202558340) ogra: that's ok, as long as you only need the packages that are necessary for the running\n",
            "(1202544500) ogra: for example, xmms plays fine over windows (but the sound files\n",
            "\n",
            "[3000 | 4157.48] loss=2.43 avg=2.33\n",
            "[3001 | 4160.49] loss=2.12 avg=2.33\n",
            "[3002 | 4163.54] loss=2.42 avg=2.33\n",
            "[3003 | 4166.58] loss=2.19 avg=2.32\n",
            "[3004 | 4169.62] loss=2.19 avg=2.32\n",
            "[3005 | 4172.66] loss=2.23 avg=2.32\n",
            "[3006 | 4175.69] loss=2.91 avg=2.33\n",
            "[3007 | 4178.73] loss=2.47 avg=2.33\n",
            "[3008 | 4181.76] loss=2.58 avg=2.33\n",
            "[3009 | 4184.81] loss=2.22 avg=2.33\n",
            "[3010 | 4187.85] loss=2.17 avg=2.33\n",
            "[3011 | 4190.89] loss=2.11 avg=2.33\n",
            "[3012 | 4193.93] loss=2.29 avg=2.33\n",
            "[3013 | 4196.96] loss=2.33 avg=2.33\n",
            "[3014 | 4200.00] loss=2.47 avg=2.33\n",
            "[3015 | 4203.05] loss=1.99 avg=2.32\n",
            "[3016 | 4206.09] loss=2.77 avg=2.33\n",
            "[3017 | 4209.14] loss=2.52 avg=2.33\n",
            "[3018 | 4212.19] loss=1.88 avg=2.33\n",
            "[3019 | 4215.25] loss=2.39 avg=2.33\n",
            "[3020 | 4218.31] loss=2.33 avg=2.33\n",
            "[3021 | 4221.36] loss=2.61 avg=2.33\n",
            "[3022 | 4224.42] loss=1.65 avg=2.32\n",
            "[3023 | 4227.48] loss=2.70 avg=2.33\n",
            "[3024 | 4230.52] loss=2.14 avg=2.33\n",
            "[3025 | 4233.59] loss=2.03 avg=2.32\n",
            "[3026 | 4236.63] loss=2.47 avg=2.32\n",
            "[3027 | 4239.67] loss=2.56 avg=2.33\n",
            "[3028 | 4242.74] loss=2.67 avg=2.33\n",
            "[3029 | 4245.81] loss=2.34 avg=2.33\n",
            "[3030 | 4248.87] loss=2.00 avg=2.33\n",
            "[3031 | 4251.93] loss=2.19 avg=2.33\n",
            "[3032 | 4254.98] loss=2.72 avg=2.33\n",
            "[3033 | 4258.03] loss=2.51 avg=2.33\n",
            "[3034 | 4261.08] loss=2.60 avg=2.33\n",
            "[3035 | 4264.13] loss=2.42 avg=2.33\n",
            "[3036 | 4267.18] loss=2.00 avg=2.33\n",
            "[3037 | 4270.21] loss=2.67 avg=2.33\n",
            "[3038 | 4273.25] loss=2.98 avg=2.34\n",
            "[3039 | 4276.31] loss=2.63 avg=2.34\n",
            "[3040 | 4279.35] loss=2.56 avg=2.35\n",
            "[3041 | 4282.41] loss=2.47 avg=2.35\n",
            "[3042 | 4285.45] loss=2.19 avg=2.35\n",
            "[3043 | 4288.48] loss=2.05 avg=2.34\n",
            "[3044 | 4291.53] loss=2.13 avg=2.34\n",
            "[3045 | 4294.59] loss=2.15 avg=2.34\n",
            "[3046 | 4297.65] loss=2.60 avg=2.34\n",
            "[3047 | 4300.76] loss=2.62 avg=2.34\n",
            "[3048 | 4303.87] loss=2.53 avg=2.35\n",
            "[3049 | 4306.98] loss=2.45 avg=2.35\n",
            "[3050 | 4310.07] loss=2.13 avg=2.34\n",
            "[3051 | 4313.18] loss=2.72 avg=2.35\n",
            "[3052 | 4316.26] loss=1.94 avg=2.34\n",
            "[3053 | 4319.35] loss=2.16 avg=2.34\n",
            "[3054 | 4322.43] loss=2.38 avg=2.34\n",
            "[3055 | 4325.53] loss=2.02 avg=2.34\n",
            "[3056 | 4328.63] loss=2.53 avg=2.34\n",
            "[3057 | 4331.75] loss=2.13 avg=2.34\n",
            "[3058 | 4334.84] loss=2.08 avg=2.34\n",
            "[3059 | 4337.93] loss=2.47 avg=2.34\n",
            "[3060 | 4341.01] loss=2.78 avg=2.34\n",
            "[3061 | 4344.12] loss=2.81 avg=2.35\n",
            "[3062 | 4347.22] loss=2.18 avg=2.35\n",
            "[3063 | 4350.31] loss=2.37 avg=2.35\n",
            "[3064 | 4353.41] loss=2.08 avg=2.34\n",
            "[3065 | 4356.48] loss=2.56 avg=2.35\n",
            "[3066 | 4359.58] loss=2.17 avg=2.34\n",
            "[3067 | 4362.67] loss=2.42 avg=2.34\n",
            "[3068 | 4365.80] loss=1.70 avg=2.34\n",
            "[3069 | 4368.92] loss=2.12 avg=2.34\n",
            "[3070 | 4372.00] loss=2.17 avg=2.33\n",
            "[3071 | 4375.10] loss=1.84 avg=2.33\n",
            "[3072 | 4378.18] loss=2.32 avg=2.33\n",
            "[3073 | 4381.29] loss=2.22 avg=2.33\n",
            "[3074 | 4384.40] loss=1.79 avg=2.32\n",
            "[3075 | 4387.53] loss=2.12 avg=2.32\n",
            "[3076 | 4390.67] loss=2.37 avg=2.32\n",
            "[3077 | 4393.80] loss=2.71 avg=2.33\n",
            "[3078 | 4396.95] loss=2.31 avg=2.32\n",
            "[3079 | 4400.10] loss=2.01 avg=2.32\n",
            "[3080 | 4403.24] loss=2.27 avg=2.32\n",
            "[3081 | 4406.41] loss=2.39 avg=2.32\n",
            "[3082 | 4409.54] loss=2.08 avg=2.32\n",
            "[3083 | 4412.68] loss=2.19 avg=2.32\n",
            "[3084 | 4415.81] loss=2.16 avg=2.32\n",
            "[3085 | 4418.95] loss=2.28 avg=2.32\n",
            "[3086 | 4422.07] loss=2.20 avg=2.32\n",
            "[3087 | 4425.19] loss=2.47 avg=2.32\n",
            "[3088 | 4428.29] loss=2.44 avg=2.32\n",
            "[3089 | 4431.42] loss=2.29 avg=2.32\n",
            "[3090 | 4434.53] loss=2.12 avg=2.32\n",
            "[3091 | 4437.67] loss=2.48 avg=2.32\n",
            "[3092 | 4440.78] loss=1.87 avg=2.31\n",
            "[3093 | 4443.89] loss=1.98 avg=2.31\n",
            "[3094 | 4447.00] loss=2.51 avg=2.31\n",
            "[3095 | 4450.15] loss=1.86 avg=2.31\n",
            "[3096 | 4453.26] loss=1.96 avg=2.30\n",
            "[3097 | 4456.35] loss=2.36 avg=2.30\n",
            "[3098 | 4459.46] loss=2.34 avg=2.30\n",
            "[3099 | 4462.58] loss=2.22 avg=2.30\n",
            "[3100 | 4465.69] loss=2.04 avg=2.30\n",
            "[3101 | 4468.83] loss=2.15 avg=2.30\n",
            "[3102 | 4471.96] loss=2.74 avg=2.30\n",
            "[3103 | 4475.04] loss=1.89 avg=2.30\n",
            "[3104 | 4478.14] loss=2.56 avg=2.30\n",
            "[3105 | 4481.25] loss=2.33 avg=2.30\n",
            "[3106 | 4484.36] loss=2.77 avg=2.31\n",
            "[3107 | 4487.49] loss=2.83 avg=2.31\n",
            "[3108 | 4490.59] loss=2.00 avg=2.31\n",
            "[3109 | 4493.68] loss=2.42 avg=2.31\n",
            "[3110 | 4496.81] loss=2.61 avg=2.31\n",
            "[3111 | 4499.89] loss=2.77 avg=2.32\n",
            "[3112 | 4503.01] loss=2.39 avg=2.32\n",
            "[3113 | 4506.06] loss=2.62 avg=2.32\n",
            "[3114 | 4509.14] loss=2.45 avg=2.32\n",
            "[3115 | 4512.22] loss=2.24 avg=2.32\n",
            "[3116 | 4515.30] loss=2.42 avg=2.32\n",
            "[3117 | 4518.37] loss=2.79 avg=2.33\n",
            "[3118 | 4521.46] loss=2.90 avg=2.33\n",
            "[3119 | 4524.52] loss=2.42 avg=2.33\n",
            "[3120 | 4527.58] loss=2.29 avg=2.33\n",
            "[3121 | 4530.68] loss=2.12 avg=2.33\n",
            "[3122 | 4533.77] loss=2.36 avg=2.33\n",
            "[3123 | 4536.83] loss=2.61 avg=2.33\n",
            "[3124 | 4539.92] loss=1.86 avg=2.33\n",
            "[3125 | 4543.00] loss=2.18 avg=2.33\n",
            "[3126 | 4546.07] loss=2.59 avg=2.33\n",
            "[3127 | 4549.15] loss=2.28 avg=2.33\n",
            "[3128 | 4552.23] loss=2.46 avg=2.33\n",
            "[3129 | 4555.31] loss=2.70 avg=2.34\n",
            "[3130 | 4558.39] loss=2.14 avg=2.33\n",
            "[3131 | 4561.46] loss=2.85 avg=2.34\n",
            "[3132 | 4564.53] loss=1.93 avg=2.33\n",
            "[3133 | 4567.61] loss=2.56 avg=2.34\n",
            "[3134 | 4570.70] loss=2.69 avg=2.34\n",
            "[3135 | 4573.76] loss=2.52 avg=2.34\n",
            "[3136 | 4576.85] loss=2.14 avg=2.34\n",
            "[3137 | 4579.91] loss=2.60 avg=2.34\n",
            "[3138 | 4582.99] loss=2.43 avg=2.34\n",
            "[3139 | 4586.03] loss=1.90 avg=2.34\n",
            "[3140 | 4589.11] loss=2.10 avg=2.34\n",
            "[3141 | 4592.21] loss=2.76 avg=2.34\n",
            "[3142 | 4595.29] loss=2.52 avg=2.34\n",
            "[3143 | 4598.37] loss=2.58 avg=2.35\n",
            "[3144 | 4601.47] loss=2.86 avg=2.35\n",
            "[3145 | 4604.55] loss=2.45 avg=2.35\n",
            "[3146 | 4607.62] loss=2.02 avg=2.35\n",
            "[3147 | 4610.71] loss=2.57 avg=2.35\n",
            "[3148 | 4613.78] loss=2.43 avg=2.35\n",
            "[3149 | 4616.84] loss=2.00 avg=2.35\n",
            "[3150 | 4619.92] loss=2.68 avg=2.35\n",
            "[3151 | 4623.00] loss=2.27 avg=2.35\n",
            "[3152 | 4626.09] loss=2.54 avg=2.35\n",
            "[3153 | 4629.15] loss=2.10 avg=2.35\n",
            "[3154 | 4632.21] loss=2.12 avg=2.35\n",
            "[3155 | 4635.29] loss=2.26 avg=2.35\n",
            "[3156 | 4638.35] loss=2.78 avg=2.35\n",
            "[3157 | 4641.50] loss=2.43 avg=2.35\n",
            "[3158 | 4644.64] loss=2.37 avg=2.35\n",
            "[3159 | 4647.76] loss=1.97 avg=2.35\n",
            "[3160 | 4650.85] loss=2.70 avg=2.35\n",
            "[3161 | 4653.99] loss=2.28 avg=2.35\n",
            "[3162 | 4657.08] loss=2.40 avg=2.35\n",
            "[3163 | 4660.18] loss=2.93 avg=2.36\n",
            "[3164 | 4663.26] loss=2.27 avg=2.36\n",
            "[3165 | 4666.35] loss=2.60 avg=2.36\n",
            "[3166 | 4669.45] loss=2.18 avg=2.36\n",
            "[3167 | 4672.60] loss=2.40 avg=2.36\n",
            "[3168 | 4675.69] loss=2.28 avg=2.36\n",
            "[3169 | 4678.81] loss=2.21 avg=2.36\n",
            "[3170 | 4681.95] loss=2.73 avg=2.36\n",
            "[3171 | 4685.05] loss=2.20 avg=2.36\n",
            "[3172 | 4688.18] loss=2.18 avg=2.36\n",
            "[3173 | 4691.33] loss=1.87 avg=2.35\n",
            "[3174 | 4694.46] loss=2.29 avg=2.35\n",
            "[3175 | 4697.58] loss=2.11 avg=2.35\n",
            "[3176 | 4700.70] loss=2.65 avg=2.35\n",
            "[3177 | 4703.83] loss=2.87 avg=2.36\n",
            "[3178 | 4706.95] loss=2.34 avg=2.36\n",
            "[3179 | 4710.07] loss=2.27 avg=2.35\n",
            "[3180 | 4713.19] loss=2.28 avg=2.35\n",
            "[3181 | 4716.31] loss=2.07 avg=2.35\n",
            "[3182 | 4719.40] loss=2.15 avg=2.35\n",
            "[3183 | 4722.51] loss=2.63 avg=2.35\n",
            "[3184 | 4725.63] loss=2.46 avg=2.35\n",
            "[3185 | 4728.73] loss=2.55 avg=2.36\n",
            "[3186 | 4731.84] loss=2.47 avg=2.36\n",
            "[3187 | 4734.97] loss=2.31 avg=2.36\n",
            "[3188 | 4738.06] loss=2.50 avg=2.36\n",
            "[3189 | 4741.19] loss=2.67 avg=2.36\n",
            "[3190 | 4744.28] loss=2.12 avg=2.36\n",
            "[3191 | 4747.37] loss=1.77 avg=2.35\n",
            "[3192 | 4750.47] loss=2.35 avg=2.35\n",
            "[3193 | 4753.58] loss=1.57 avg=2.34\n",
            "[3194 | 4756.67] loss=2.63 avg=2.35\n",
            "[3195 | 4759.79] loss=2.11 avg=2.34\n",
            "[3196 | 4762.92] loss=1.91 avg=2.34\n",
            "[3197 | 4766.06] loss=2.50 avg=2.34\n",
            "[3198 | 4769.19] loss=2.54 avg=2.34\n",
            "[3199 | 4772.30] loss=2.29 avg=2.34\n",
            "[3200 | 4775.40] loss=2.18 avg=2.34\n",
            "[3201 | 4778.49] loss=2.06 avg=2.34\n",
            "[3202 | 4781.57] loss=1.89 avg=2.33\n",
            "[3203 | 4784.63] loss=2.43 avg=2.34\n",
            "[3204 | 4787.75] loss=2.79 avg=2.34\n",
            "[3205 | 4790.84] loss=2.34 avg=2.34\n",
            "[3206 | 4793.92] loss=2.73 avg=2.34\n",
            "[3207 | 4797.02] loss=2.15 avg=2.34\n",
            "[3208 | 4800.13] loss=2.47 avg=2.34\n",
            "[3209 | 4803.21] loss=2.28 avg=2.34\n",
            "[3210 | 4806.31] loss=2.51 avg=2.34\n",
            "[3211 | 4809.40] loss=2.15 avg=2.34\n",
            "[3212 | 4812.52] loss=1.90 avg=2.34\n",
            "[3213 | 4815.63] loss=2.14 avg=2.34\n",
            "[3214 | 4818.74] loss=1.96 avg=2.33\n",
            "[3215 | 4821.84] loss=2.58 avg=2.33\n",
            "[3216 | 4824.94] loss=2.27 avg=2.33\n",
            "[3217 | 4828.06] loss=2.76 avg=2.34\n",
            "[3218 | 4831.18] loss=2.39 avg=2.34\n",
            "[3219 | 4834.30] loss=2.43 avg=2.34\n",
            "[3220 | 4837.37] loss=2.87 avg=2.34\n",
            "[3221 | 4840.48] loss=2.45 avg=2.35\n",
            "[3222 | 4843.58] loss=2.74 avg=2.35\n",
            "[3223 | 4846.67] loss=2.04 avg=2.35\n",
            "[3224 | 4849.78] loss=2.22 avg=2.35\n",
            "[3225 | 4852.87] loss=2.39 avg=2.35\n",
            "[3226 | 4855.93] loss=1.96 avg=2.34\n",
            "[3227 | 4858.98] loss=2.68 avg=2.35\n",
            "[3228 | 4862.04] loss=2.29 avg=2.35\n",
            "[3229 | 4865.11] loss=2.56 avg=2.35\n",
            "[3230 | 4868.16] loss=2.46 avg=2.35\n",
            "[3231 | 4871.23] loss=2.16 avg=2.35\n",
            "[3232 | 4874.25] loss=2.04 avg=2.34\n",
            "[3233 | 4877.27] loss=2.23 avg=2.34\n",
            "[3234 | 4880.28] loss=2.14 avg=2.34\n",
            "[3235 | 4883.33] loss=2.87 avg=2.35\n",
            "[3236 | 4886.36] loss=2.13 avg=2.34\n",
            "[3237 | 4889.40] loss=1.94 avg=2.34\n",
            "[3238 | 4892.43] loss=2.34 avg=2.34\n",
            "[3239 | 4895.46] loss=2.53 avg=2.34\n",
            "[3240 | 4898.48] loss=2.78 avg=2.35\n",
            "[3241 | 4901.52] loss=2.72 avg=2.35\n",
            "[3242 | 4904.56] loss=2.69 avg=2.35\n",
            "[3243 | 4907.61] loss=2.46 avg=2.35\n",
            "[3244 | 4910.65] loss=2.37 avg=2.35\n",
            "[3245 | 4913.70] loss=2.26 avg=2.35\n",
            "[3246 | 4916.74] loss=1.88 avg=2.35\n",
            "[3247 | 4919.80] loss=2.37 avg=2.35\n",
            "[3248 | 4922.85] loss=2.80 avg=2.35\n",
            "[3249 | 4925.89] loss=2.19 avg=2.35\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " Kamakud: I think I have that problem, yes?\n",
            "(1148336560) Mankethejazorp: hmm, I don't think so. It is a weirdness. I've done everything I should have done. Just wondering if there is anybody who can help with it.\n",
            "(1149007860) jrib: what are you saying?  You can fix it by fixing the kernel, right?\n",
            "(1149007860) Mankethejorp: No.\n",
            "(1149007860) jrib: I'm talking to you, not sure if there's anybody I can get on IRC with a solution for you problem.\n",
            "\n",
            "\n",
            "(1171625100) bhavesh: Does anyone help me? When I try to install kde on 10.10, it says a package of 'kubuntu-desktop-1.4.0-16.10.el'\n",
            "(1171625100) bhavesh: Anyone know how to fix this?\n",
            "(1171625100) bluybo: what errors are you getting from synaptic? have you tried compiling them again from source?\n",
            "(1171625160) bluybo: if you have a recent installation with kubuntu-desktop-1.4.0-16.10.el, it'll have problems, and the apt-get should fix them\n",
            "(1171625220) bluybo: that's the first problem that will happen, but you can see why your updating is a mess!  The problem may arise with some other package, the same thing happened with the gnome applets that caused gnome...\n",
            "(1171625220) bhavesh: i'm trying not to update gnome-session-daemon as much :S\n",
            "(1171625280) bluybo: you may also need to use synaptic update to fix the deb and not make the update to the repo repository - the same for gnome-session-daemon\n",
            "(1171625280) bhavesh: :P :(\n",
            "(1171625280) bluybo: it happens with any version of ubuntu\n",
            "(1171625340) bluybo: not on my experience, anyway - i don't even install it for daily use\n",
            "(1171625340) bluybo: it happens for you\n",
            "\n",
            "\n",
            "(1178012640) the_guy_from_scuba: if i want to install a png, you're already a human being too\n",
            "(1178012640) the_guy_from_scuba: and it's so stupid to even install it?\n",
            "(1178012640) the_guy_from_scuba: is there a way to create a png in ubuntu and use it for my ubuntu desktop\n",
            "(1178012640) the_guy_from_scuba: ?\n",
            "(1178012640) the_guy_from_scuba: ?\n",
            "(1178012640) the_guy_from_scuba: so why can't he install a png instead\n",
            "(1178012640) the_guy_from_scuba: and it's so stupid to even do something like\n",
            "(1178012640) the_guy_from_scuba: install a png with ubuntu\n",
            "(1178012640) jrib: can someone please tell me how to do it? I'm sure I've got no clue how to do it at all, since I do it by hand, so I want to help. :-)\n",
            "(1268788060) jrib: try running gtkpod in a terminal window, then copy the output and paste on pastebin.\n",
            "(1268788060) the_guy_from_scuba: you'll need root\n",
            "(1268788060) the_guy_from_scuba: gtkpod\n",
            "(1268786120) the_guy_from_scuba: I can't use gtkpod\n",
            "(1268786120) the_guy_from_scuba: why can't I get the x/audio working\n",
            "\n",
            "\n",
            "(1309134440) L4C0n: hey there. I'm trying to get my Ubuntu partition partition on my hard drive, but it won't let me. What can possibly be causing the problem? I have only a NTFS partition. Anyone can help me to get it working?\n",
            "(1309134440) L4C0n: I have NTFS, no windows, no linux.\n",
            "(1309134440) L4C0n: I'm trying to get Ubuntu partition on my hard drive, but it won't let me. What can possibly be causing the problem? I have NTFS to work under Ubuntu.\n",
            "(\n",
            "\n",
            "[3250 | 4967.21] loss=2.17 avg=2.35\n",
            "[3251 | 4970.29] loss=2.24 avg=2.35\n",
            "[3252 | 4973.33] loss=2.11 avg=2.35\n",
            "[3253 | 4976.37] loss=2.21 avg=2.34\n",
            "[3254 | 4979.41] loss=2.36 avg=2.35\n",
            "[3255 | 4982.45] loss=2.25 avg=2.34\n",
            "[3256 | 4985.49] loss=2.43 avg=2.35\n",
            "[3257 | 4988.53] loss=2.41 avg=2.35\n",
            "[3258 | 4991.59] loss=2.25 avg=2.34\n",
            "[3259 | 4994.65] loss=2.53 avg=2.35\n",
            "[3260 | 4997.72] loss=2.63 avg=2.35\n",
            "[3261 | 5000.79] loss=2.01 avg=2.35\n",
            "[3262 | 5003.84] loss=2.45 avg=2.35\n",
            "[3263 | 5006.87] loss=2.16 avg=2.35\n",
            "[3264 | 5009.91] loss=2.46 avg=2.35\n",
            "[3265 | 5012.98] loss=2.54 avg=2.35\n",
            "[3266 | 5016.01] loss=2.44 avg=2.35\n",
            "[3267 | 5019.08] loss=2.32 avg=2.35\n",
            "[3268 | 5022.16] loss=2.31 avg=2.35\n",
            "[3269 | 5025.27] loss=2.38 avg=2.35\n",
            "[3270 | 5028.36] loss=2.09 avg=2.35\n",
            "[3271 | 5031.45] loss=2.28 avg=2.35\n",
            "[3272 | 5034.54] loss=2.22 avg=2.34\n",
            "[3273 | 5037.63] loss=2.24 avg=2.34\n",
            "[3274 | 5040.70] loss=2.31 avg=2.34\n",
            "[3275 | 5043.82] loss=2.51 avg=2.34\n",
            "[3276 | 5046.90] loss=2.35 avg=2.34\n",
            "[3277 | 5049.98] loss=2.48 avg=2.35\n",
            "[3278 | 5053.06] loss=2.07 avg=2.34\n",
            "[3279 | 5056.17] loss=2.46 avg=2.34\n",
            "[3280 | 5059.28] loss=2.59 avg=2.35\n",
            "[3281 | 5062.37] loss=2.31 avg=2.35\n",
            "[3282 | 5065.48] loss=2.30 avg=2.35\n",
            "[3283 | 5068.55] loss=1.99 avg=2.34\n",
            "[3284 | 5071.65] loss=1.80 avg=2.34\n",
            "[3285 | 5074.74] loss=1.76 avg=2.33\n",
            "[3286 | 5077.83] loss=2.60 avg=2.33\n",
            "[3287 | 5080.92] loss=2.63 avg=2.34\n",
            "[3288 | 5084.02] loss=1.83 avg=2.33\n",
            "[3289 | 5087.11] loss=2.27 avg=2.33\n",
            "[3290 | 5090.20] loss=2.21 avg=2.33\n",
            "[3291 | 5093.29] loss=2.43 avg=2.33\n",
            "[3292 | 5096.39] loss=2.27 avg=2.33\n",
            "[3293 | 5099.50] loss=2.53 avg=2.33\n",
            "[3294 | 5102.60] loss=2.14 avg=2.33\n",
            "[3295 | 5105.71] loss=2.03 avg=2.33\n",
            "[3296 | 5108.77] loss=1.98 avg=2.32\n",
            "[3297 | 5111.87] loss=1.93 avg=2.32\n",
            "[3298 | 5114.95] loss=2.19 avg=2.32\n",
            "[3299 | 5118.07] loss=2.22 avg=2.32\n",
            "[3300 | 5121.15] loss=2.97 avg=2.32\n",
            "[3301 | 5124.25] loss=2.02 avg=2.32\n",
            "[3302 | 5127.33] loss=2.02 avg=2.32\n",
            "[3303 | 5130.42] loss=2.53 avg=2.32\n",
            "[3304 | 5133.53] loss=2.19 avg=2.32\n",
            "[3305 | 5136.59] loss=2.27 avg=2.32\n",
            "[3306 | 5139.69] loss=2.14 avg=2.32\n",
            "[3307 | 5142.76] loss=2.33 avg=2.32\n",
            "[3308 | 5145.84] loss=2.31 avg=2.32\n",
            "[3309 | 5148.92] loss=2.54 avg=2.32\n",
            "[3310 | 5152.00] loss=2.73 avg=2.32\n",
            "[3311 | 5155.07] loss=2.16 avg=2.32\n",
            "[3312 | 5158.16] loss=1.93 avg=2.32\n",
            "[3313 | 5161.23] loss=2.24 avg=2.32\n",
            "[3314 | 5164.32] loss=2.13 avg=2.32\n",
            "[3315 | 5167.42] loss=2.17 avg=2.31\n",
            "[3316 | 5170.51] loss=2.17 avg=2.31\n",
            "[3317 | 5173.58] loss=2.20 avg=2.31\n",
            "[3318 | 5176.68] loss=2.27 avg=2.31\n",
            "[3319 | 5179.76] loss=2.07 avg=2.31\n",
            "[3320 | 5182.85] loss=1.55 avg=2.30\n",
            "[3321 | 5185.92] loss=2.70 avg=2.30\n",
            "[3322 | 5189.01] loss=2.42 avg=2.31\n",
            "[3323 | 5192.12] loss=2.10 avg=2.30\n",
            "[3324 | 5195.19] loss=2.01 avg=2.30\n",
            "[3325 | 5198.26] loss=2.23 avg=2.30\n",
            "[3326 | 5201.32] loss=2.71 avg=2.30\n",
            "[3327 | 5204.39] loss=2.34 avg=2.30\n",
            "[3328 | 5207.46] loss=2.38 avg=2.31\n",
            "[3329 | 5210.53] loss=2.09 avg=2.30\n",
            "[3330 | 5213.60] loss=2.65 avg=2.31\n",
            "[3331 | 5216.67] loss=2.59 avg=2.31\n",
            "[3332 | 5219.73] loss=2.30 avg=2.31\n",
            "[3333 | 5222.79] loss=2.61 avg=2.31\n",
            "[3334 | 5225.87] loss=2.47 avg=2.31\n",
            "[3335 | 5228.94] loss=2.27 avg=2.31\n",
            "[3336 | 5231.99] loss=2.53 avg=2.32\n",
            "[3337 | 5235.05] loss=2.17 avg=2.31\n",
            "[3338 | 5238.10] loss=2.55 avg=2.32\n",
            "[3339 | 5241.17] loss=2.05 avg=2.31\n",
            "[3340 | 5244.21] loss=2.43 avg=2.32\n",
            "[3341 | 5247.26] loss=2.07 avg=2.31\n",
            "[3342 | 5250.33] loss=2.27 avg=2.31\n",
            "[3343 | 5253.40] loss=2.60 avg=2.32\n",
            "[3344 | 5256.47] loss=2.31 avg=2.32\n",
            "[3345 | 5259.54] loss=2.82 avg=2.32\n",
            "[3346 | 5262.61] loss=2.14 avg=2.32\n",
            "[3347 | 5265.67] loss=2.25 avg=2.32\n",
            "[3348 | 5268.75] loss=2.50 avg=2.32\n",
            "[3349 | 5271.82] loss=2.08 avg=2.32\n",
            "[3350 | 5274.90] loss=2.07 avg=2.31\n",
            "[3351 | 5277.95] loss=2.49 avg=2.32\n",
            "[3352 | 5281.01] loss=2.15 avg=2.31\n",
            "[3353 | 5284.07] loss=2.82 avg=2.32\n",
            "[3354 | 5287.14] loss=2.20 avg=2.32\n",
            "[3355 | 5290.20] loss=2.16 avg=2.32\n",
            "[3356 | 5293.27] loss=2.00 avg=2.31\n",
            "[3357 | 5296.32] loss=2.23 avg=2.31\n",
            "[3358 | 5299.39] loss=2.56 avg=2.32\n",
            "[3359 | 5302.45] loss=2.03 avg=2.31\n",
            "[3360 | 5305.51] loss=2.64 avg=2.32\n",
            "[3361 | 5308.58] loss=2.28 avg=2.32\n",
            "[3362 | 5311.64] loss=2.75 avg=2.32\n",
            "[3363 | 5314.71] loss=2.47 avg=2.32\n",
            "[3364 | 5317.78] loss=2.42 avg=2.32\n",
            "[3365 | 5320.83] loss=2.11 avg=2.32\n",
            "[3366 | 5323.91] loss=2.01 avg=2.32\n",
            "[3367 | 5326.99] loss=2.47 avg=2.32\n",
            "[3368 | 5330.05] loss=2.01 avg=2.32\n",
            "[3369 | 5333.13] loss=2.58 avg=2.32\n",
            "[3370 | 5336.21] loss=2.02 avg=2.31\n",
            "[3371 | 5339.30] loss=1.97 avg=2.31\n",
            "[3372 | 5342.38] loss=2.15 avg=2.31\n",
            "[3373 | 5345.48] loss=2.59 avg=2.31\n",
            "[3374 | 5348.58] loss=2.20 avg=2.31\n",
            "[3375 | 5351.65] loss=2.43 avg=2.31\n",
            "[3376 | 5354.77] loss=1.95 avg=2.31\n",
            "[3377 | 5357.86] loss=2.08 avg=2.31\n",
            "[3378 | 5360.94] loss=2.40 avg=2.31\n",
            "[3379 | 5364.07] loss=2.43 avg=2.31\n",
            "[3380 | 5367.16] loss=2.37 avg=2.31\n",
            "[3381 | 5370.26] loss=2.36 avg=2.31\n",
            "[3382 | 5373.33] loss=2.23 avg=2.31\n",
            "[3383 | 5376.44] loss=2.40 avg=2.31\n",
            "[3384 | 5379.54] loss=2.27 avg=2.31\n",
            "[3385 | 5382.64] loss=2.02 avg=2.31\n",
            "[3386 | 5385.73] loss=1.95 avg=2.30\n",
            "[3387 | 5388.82] loss=2.26 avg=2.30\n",
            "[3388 | 5391.92] loss=2.34 avg=2.30\n",
            "[3389 | 5395.01] loss=2.66 avg=2.31\n",
            "[3390 | 5398.12] loss=2.19 avg=2.31\n",
            "[3391 | 5401.21] loss=2.98 avg=2.31\n",
            "[3392 | 5404.31] loss=2.04 avg=2.31\n",
            "[3393 | 5407.42] loss=2.06 avg=2.31\n",
            "[3394 | 5410.51] loss=2.53 avg=2.31\n",
            "[3395 | 5413.63] loss=2.50 avg=2.31\n",
            "[3396 | 5416.73] loss=2.20 avg=2.31\n",
            "[3397 | 5419.85] loss=2.02 avg=2.31\n",
            "[3398 | 5422.95] loss=2.39 avg=2.31\n",
            "[3399 | 5426.03] loss=2.35 avg=2.31\n",
            "[3400 | 5429.15] loss=2.20 avg=2.31\n",
            "[3401 | 5432.28] loss=2.11 avg=2.31\n",
            "[3402 | 5435.42] loss=2.24 avg=2.31\n",
            "[3403 | 5438.48] loss=2.10 avg=2.30\n",
            "[3404 | 5441.58] loss=1.97 avg=2.30\n",
            "[3405 | 5444.70] loss=2.23 avg=2.30\n",
            "[3406 | 5447.81] loss=2.53 avg=2.30\n",
            "[3407 | 5450.92] loss=2.54 avg=2.30\n",
            "[3408 | 5454.03] loss=2.43 avg=2.30\n",
            "[3409 | 5457.14] loss=2.33 avg=2.31\n",
            "[3410 | 5460.24] loss=2.79 avg=2.31\n",
            "[3411 | 5463.35] loss=2.03 avg=2.31\n",
            "[3412 | 5466.46] loss=2.59 avg=2.31\n",
            "[3413 | 5469.57] loss=2.46 avg=2.31\n",
            "[3414 | 5472.69] loss=2.33 avg=2.31\n",
            "[3415 | 5475.77] loss=2.18 avg=2.31\n",
            "[3416 | 5478.88] loss=2.28 avg=2.31\n",
            "[3417 | 5481.99] loss=2.14 avg=2.31\n",
            "[3418 | 5485.07] loss=2.47 avg=2.31\n",
            "[3419 | 5488.16] loss=2.38 avg=2.31\n",
            "[3420 | 5491.24] loss=2.77 avg=2.32\n",
            "[3421 | 5494.33] loss=2.52 avg=2.32\n",
            "[3422 | 5497.43] loss=2.38 avg=2.32\n",
            "[3423 | 5500.49] loss=2.11 avg=2.32\n",
            "[3424 | 5503.57] loss=2.24 avg=2.32\n",
            "[3425 | 5506.68] loss=2.01 avg=2.31\n",
            "[3426 | 5509.78] loss=2.42 avg=2.31\n",
            "[3427 | 5512.87] loss=2.10 avg=2.31\n",
            "[3428 | 5515.93] loss=2.45 avg=2.31\n",
            "[3429 | 5518.98] loss=2.07 avg=2.31\n",
            "[3430 | 5522.03] loss=1.92 avg=2.31\n",
            "[3431 | 5525.08] loss=2.16 avg=2.30\n",
            "[3432 | 5528.14] loss=1.86 avg=2.30\n",
            "[3433 | 5531.20] loss=2.20 avg=2.30\n",
            "[3434 | 5534.24] loss=2.13 avg=2.30\n",
            "[3435 | 5537.29] loss=2.41 avg=2.30\n",
            "[3436 | 5540.34] loss=2.63 avg=2.30\n",
            "[3437 | 5543.39] loss=2.43 avg=2.30\n",
            "[3438 | 5546.44] loss=2.65 avg=2.31\n",
            "[3439 | 5549.50] loss=2.94 avg=2.31\n",
            "[3440 | 5552.52] loss=2.14 avg=2.31\n",
            "[3441 | 5555.58] loss=2.03 avg=2.31\n",
            "[3442 | 5558.64] loss=2.02 avg=2.31\n",
            "[3443 | 5561.71] loss=2.29 avg=2.31\n",
            "[3444 | 5564.77] loss=2.50 avg=2.31\n",
            "[3445 | 5567.80] loss=2.14 avg=2.31\n",
            "[3446 | 5570.86] loss=2.24 avg=2.31\n",
            "[3447 | 5573.91] loss=2.13 avg=2.30\n",
            "[3448 | 5576.96] loss=1.91 avg=2.30\n",
            "[3449 | 5580.03] loss=2.41 avg=2.30\n",
            "[3450 | 5583.07] loss=2.02 avg=2.30\n",
            "[3451 | 5586.12] loss=1.91 avg=2.29\n",
            "[3452 | 5589.16] loss=2.20 avg=2.29\n",
            "[3453 | 5592.21] loss=2.26 avg=2.29\n",
            "[3454 | 5595.27] loss=2.71 avg=2.30\n",
            "[3455 | 5598.34] loss=2.53 avg=2.30\n",
            "[3456 | 5601.38] loss=2.24 avg=2.30\n",
            "[3457 | 5604.44] loss=2.20 avg=2.30\n",
            "[3458 | 5607.49] loss=2.73 avg=2.30\n",
            "[3459 | 5610.54] loss=2.17 avg=2.30\n",
            "[3460 | 5613.61] loss=1.90 avg=2.30\n",
            "[3461 | 5616.67] loss=2.65 avg=2.30\n",
            "[3462 | 5619.74] loss=2.28 avg=2.30\n",
            "[3463 | 5622.78] loss=2.59 avg=2.30\n",
            "[3464 | 5625.83] loss=2.06 avg=2.30\n",
            "[3465 | 5628.91] loss=2.72 avg=2.30\n",
            "[3466 | 5631.99] loss=2.16 avg=2.30\n",
            "[3467 | 5635.05] loss=2.14 avg=2.30\n",
            "[3468 | 5638.11] loss=2.40 avg=2.30\n",
            "[3469 | 5641.18] loss=2.71 avg=2.31\n",
            "[3470 | 5644.23] loss=1.92 avg=2.30\n",
            "[3471 | 5647.29] loss=2.23 avg=2.30\n",
            "[3472 | 5650.34] loss=2.42 avg=2.30\n",
            "[3473 | 5653.42] loss=2.80 avg=2.31\n",
            "[3474 | 5656.47] loss=2.48 avg=2.31\n",
            "[3475 | 5659.54] loss=2.21 avg=2.31\n",
            "[3476 | 5662.64] loss=2.85 avg=2.31\n",
            "[3477 | 5665.74] loss=2.66 avg=2.32\n",
            "[3478 | 5668.84] loss=2.18 avg=2.32\n",
            "[3479 | 5671.95] loss=2.45 avg=2.32\n",
            "[3480 | 5675.04] loss=1.97 avg=2.31\n",
            "[3481 | 5678.14] loss=2.27 avg=2.31\n",
            "[3482 | 5681.23] loss=2.60 avg=2.32\n",
            "[3483 | 5684.33] loss=2.85 avg=2.32\n",
            "[3484 | 5687.43] loss=2.43 avg=2.32\n",
            "[3485 | 5690.51] loss=2.08 avg=2.32\n",
            "[3486 | 5693.61] loss=2.40 avg=2.32\n",
            "[3487 | 5696.70] loss=2.62 avg=2.32\n",
            "[3488 | 5699.77] loss=2.11 avg=2.32\n",
            "[3489 | 5702.85] loss=2.06 avg=2.32\n",
            "[3490 | 5705.98] loss=2.07 avg=2.32\n",
            "[3491 | 5709.09] loss=2.50 avg=2.32\n",
            "[3492 | 5712.20] loss=2.23 avg=2.32\n",
            "[3493 | 5715.29] loss=2.40 avg=2.32\n",
            "[3494 | 5718.38] loss=2.33 avg=2.32\n",
            "[3495 | 5721.45] loss=2.43 avg=2.32\n",
            "[3496 | 5724.56] loss=2.29 avg=2.32\n",
            "[3497 | 5727.66] loss=1.98 avg=2.32\n",
            "[3498 | 5730.77] loss=2.34 avg=2.32\n",
            "[3499 | 5733.89] loss=2.07 avg=2.31\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ".) What do I do?  Please! \n",
            "(1163825800) k1ck:  you can set up a proxy ip address in windows\n",
            "(1163825800) k1ck:  you can set up a proxy ip address in windows\n",
            "(1163825800) fyrestrtr:  I am able to use the internet on linux, right now i dont know where i can put the web proxy..\n",
            "(1163825800) fyrestrtr:  I will try again..\n",
            "(1163825800) mrh:  you don't want to do this..\n",
            "(1163825800) k1ck:  not sure how it will work.. try some places first..\n",
            "(1163825800) mrh:  my proxy is working with Windows to connect from, but now i cant use my pc...\n",
            "(1163825800) fyrestrtr:  can you just use a proxy from the pc?\n",
            "(1163825800) k1ck:  yes\n",
            "(1163825800) mrh:  not sure the right answer.. and if you want to know the answer you can google your pc and its IP.. you have your problem solved :)\n",
            "(1163825800) k1ck:  you can do it from windows if you set up it as a proxy\n",
            "(1163825860) mrh:  i don't know, i dont use windows :\n",
            "(1163825860) k1ck:  can you set up a proxy from the pc instead of from windows?\n",
            "(1163825920) mrh:  i want a proxy, not one from a pc\n",
            "(1163825920) mrh:  and i dont really need a pc... it just needs a proxy\n",
            "(1163825920) k1ck:  you can just set it up from a browser\n",
            "(1163826040) mrh:  i don't use w3c\n",
            "(1163881320) fyrestrtr:  how's that possible when your pc cant open the internet...\n",
            "(1163881320) mrh:  i just set up it from the pc.. i dont mean directly, but it works fine in my pc. but in my pc it can't do what i want..\n",
            "(1163881320) k1ck:  you can set up a proxy on windows, if you set up it as a proxy and then you have a proxy ip\n",
            "(1163881380) mrh:  what do I need then that i can set the settings to the pc/network for the pc to open the pc\n",
            "\n",
            "\n",
            "(1189855400) nalioth_: how do I convert a torrent to HTML?\n",
            "(1189855400) nalioth_: theres a command or two\n",
            "(1189855400) nalioth_: but I dont know what it is\n",
            "(1189855400) nalioth_: its not in the wiki\n",
            "(1189855400) nalioth_: theres a command\n",
            "(1189855400) nalioth_: yes\n",
            "(1189855400) nalioth_: I just used\n",
            "(1189855400) nalioth_: a few years ago\n",
            "(1189855400) nalioth_: a command in a terminal\n",
            "(1189855520) nalioth_: I dont know what it is\n",
            "(1189855580) nalioth_: how do I find the folder in Ubuntu I could use the command?\n",
            "(1189855580) nalioth_: and then its not in my menu?\n",
            "(1189855580) nalioth_: where is it?\n",
            "(1189855580) nalioth_: I have all my documents in there, so I can move with the command?\n",
            "(1189855580) nalioth_: i can move with the command with the command?\n",
            "(1189855580) nalioth_: that what the command is called\n",
            "(1189855640) nalioth_: I dont see it on my computer?\n",
            "(1189855640) nalioth_: any ideas where it is?\n",
            "(1189855640) nalioth_: but i cannot find it\n",
            "(1189855640) nalioth_: so I don't know who to use: I would try the command in a text editor or some other editor, if you have to do it with a program\n",
            "(1189855640) nalioth_: i thought so\n",
            "(1189855640) nalioth_: what are you using a command\n",
            "(1189855640) yaw:  i am using a command... a command to move. which is the command we used many years ago.\n",
            "\n",
            "\n",
            "[3500 | 5775.45] loss=2.04 avg=2.31\n",
            "Saving checkpoint/run1/model-3501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RogywTGfW_8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clean up old saves\n",
        "files = []\n",
        "iterations = []\n",
        "for p, d, f in os.walk('/content/gpt-2/checkpoint/'):\n",
        "    for file in f:\n",
        "        r = re.findall('model-(\\d+).', file)\n",
        "        if r:\n",
        "            iterations.append(int(r[0]))\n",
        "            files.append(f'{p}/{file}')\n",
        "\n",
        "\n",
        "last_saved = str(max(iterations))\n",
        "for f in files:\n",
        "  if last_saved not in f:\n",
        "    os.remove(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrzdyV_WVdhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/gpt-2/checkpoint/run1/* /content/gpt-2/models/345M/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rsd0hrChXVVp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bada7e94-a49e-415f-ceba-59bf8b755505"
      },
      "source": [
        "!python3 src/generate_unconditional_samples.py --top_k 40 --temperature 0.95 --model_name 345M"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From src/generate_unconditional_samples.py:49: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-10-13 04:06:06.270504: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-10-13 04:06:06.296601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:06:06.297377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-10-13 04:06:06.304503: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-13 04:06:06.317636: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-10-13 04:06:06.323096: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-10-13 04:06:06.333812: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-10-13 04:06:06.353651: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-10-13 04:06:06.364970: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-10-13 04:06:06.387644: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-10-13 04:06:06.387836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:06:06.388903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:06:06.389692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-10-13 04:06:06.400992: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-10-13 04:06:06.401449: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2bbb640 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-10-13 04:06:06.401490: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-10-13 04:06:06.470650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:06:06.471774: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x85ed6c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-10-13 04:06:06.471810: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-10-13 04:06:06.472031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:06:06.472722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-10-13 04:06:06.472799: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-13 04:06:06.472831: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-10-13 04:06:06.472867: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-10-13 04:06:06.472899: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-10-13 04:06:06.472930: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-10-13 04:06:06.472956: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-10-13 04:06:06.472996: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-10-13 04:06:06.473092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:06:06.473904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:06:06.474627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-10-13 04:06:06.474697: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-13 04:06:06.476345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-10-13 04:06:06.476380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-10-13 04:06:06.476394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-10-13 04:06:06.476588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:06:06.477460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:06:06.478152: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-10-13 04:06:06.478203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From src/generate_unconditional_samples.py:51: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From src/generate_unconditional_samples.py:60: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "2019-10-13 04:06:19.134130: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "======================================== SAMPLE 1 ========================================\n",
            "(1302579780) kjell: !x\n",
            "(1302579780) kjell: !ask | hooligan\n",
            "(1302579780) kjell: what are you trying to do in there?\n",
            "(1302579780) hooligan: I am trying to help someone. I am a newbie in this channel.\n",
            "(1302589860) hooligan: I would like to know where I can get info for some of this, but right now I am on #archlinux-offtopic :)\n",
            "(1302589860) chmod 644 /dev/null || echo $GLOBAL | sudo tee -a /etc/apt/sources.list /etc/apt/sources.list.d/*\n",
            "(1302589980) kjell: just don't add a new line to /etc/apt/sources.list and try again\n",
            "(1302589980) kjell: just add another line to it\n",
            "(1302589980) hooligan: Alright, is that a terminal command to run it?\n",
            "(1302590040) kjell: don't include anything in there.\n",
            "(1302590040) kjell: just add another line that will be in /etc/apt/sources.list\n",
            "(1302590040) hooligan: OK.\n",
            "(1302590040) chmod 644 /dev/null || echo $GLOBAL | sudo tee -a /etc/apt/sources.list\n",
            "(1302590040) kjell: add the 'exit' and the path of the command.\n",
            "(1302590100) hooligan: Okay!\n",
            "(1302590100) chmod 644 /dev/null || echo $GLOBAL | sudo tee -a /etc/apt/sources.list.d/*\n",
            "(1302590100) kjell: ok, then add a new line.\n",
            "(1302590100) chmod 644 /dev/null || echo $GLOBAL | sudo tee -a /etc/apt/sources.list.d/*\n",
            "(1302590020) chmod 644 /dev/null || echo $GLOBAL | sudo tee -a /etc/apt/sources.list.d/*\n",
            "(1302590020) kjell: you can add the full path of the command to the end and the command's full path there\n",
            "(1302590080) chmod 644 /dev/null || echo $GLOBAL | sudo tee -a /etc/apt/sources.list.d/*\n",
            "(1302590080) hooligan: I have it! Thanks :)\n",
            "(1302590080) kjell: yes, you can\n",
            "(1302590240) kjell: yes, you can\n",
            "\n",
            "\n",
            "(1306567180) K-S-L|: ok I'm trying Ubuntu and I was wondering what drivers I would want.\n",
            "(1306567180) K-S-L|: I've got the Geforce 6M\n",
            "(1306567180) K-S-L|: and Geforce is the driver I was looking into.\n",
            "(1306567240) LjL: you might want to look into geforce-graphics if you are really new.\n",
            "(1306567240) K-S-L|: i mean that I have to change the driver on my Radeon 6M\n",
            "(1306567300) K-S-L|: but I can't find the driver I used\n",
            "(1306567300) LjL: I can't find it, I didn't see your question.\n",
            "(1306567360) K-S-L|: I don't have access to anything.\n",
            "(1306567360) K-S-L|: I am asking people where it is.\n",
            "(1306567360) K-S-L|: It says nothing about the driver I can download.\n",
            "(1306567360) LjL: yeah, you don't. What's the problem.\n",
            "(1306567360) K-S-L|: its a geforce 6m\n",
            "(1306567420) K-S-L|: I installed the latest ubuntu and that driver is fine\n",
            "(1306567420) LjL: have you used it in a while?\n",
            "(1306567480) K-S-L|: my bad... i really needed to find out how to find out, so I didn't have to download, install, and setup the driver.\n",
            "(1306567840) K-S-L|: I mean the fact that I can't see it is very important, as it could save my system. or it\n",
            "======================================== SAMPLE 2 ========================================\n",
            "(1205039360) omar_: how do i fix this????\n",
            "(1205039360) omar_: what do i fix it ?\n",
            "(1205039360) omar_: so do i use the apt-cache to install\n",
            "\n",
            "\n",
            "(1134344080) jordan: what does this mean: 'i`e: 'sudo dpkg-reconfigure xserver-xorg' ?\n",
            "(1134344080) jordan: I dont have an X environment!\n",
            "(1134344080) jordan: what do you mean by that?\n",
            "(1134344140) jordan:  what do i do ?\n",
            "(1134344140) sjovan: sudo gedit /etc/apt/sources.list\n",
            "(1134344200) sjovan: it will tell you what you need to install\n",
            "(113434220) sjovan: sudo add-apt-reconfigure xserver-xorg\n",
            "(113434220) sjovan: you can then go to the packages.ubuntu.com and you should be able to remove those\n",
            "(113434280) sjovan: you can also try sudo apt-get remove ...\n",
            "(113434280) sjovan: it will install the new version\n",
            "(113434340) sjovan: the -t\n",
            "(1134344100) sjovan: you did add the sudo?\n",
            "\n",
            "\n",
            "(1278849280) dorothyp: does anyone know how to fix this xorg.conf?\n",
            "(1278849280) dorothyp: i have a sound problem\n",
            "(1278849280) dorothyp: no sound at all...\n",
            "(1278849340) dorothyp: do i need to do anything else?\n",
            "(1278849340) dorothyp: i can try xorg and fglrx\n",
            "(1278849340) dorothyp: I guess i need to re-install everything.\n",
            "(1278849400) dorothyp: and i don't get any sound at all with any of the cards in my computer\n",
            "(1278849940) dorothyp: i've been to my local linux and linux for music and audio and all hardware are great. nothing here..\n",
            "(1278850060) dorothyp: is there anything i should have done?\n",
            "(1278850060) dorothyp: oh sorry i need to fix my comp at work\n",
            "(1278852760) dorothyp: is there some kind of compiz theme program i can use to change the color of my monitors?\n",
            "(1278577520) dorothyp: is there any kind of compiz theme program i can use to change the color of my monitors?\n",
            "(1278577580) dorothyp: I don't want to do it now...but i really want it now.\n",
            "(1278857640) dorothyp: well this is why i hate ubuntu, is there some kind of compiz theme program i can use to change the color of my monitors?\n",
            "(1278857640) dorothyp: I don't want to do it now...but i really want it now\n",
            "(1278857640) dorothyp: yes.\n",
            "(1278857700) dorothyp: yes i know.\n",
            "(1278892820) dorothyp: i know.\n",
            "(1278892820) dorothyp: i know but it's not easy\n",
            "(1278892820) dorothyp: because i've installed linux\n",
            "(1278892880) dorothyp: i've made it a bit hard.\n",
            "(1278892880) dorothyp: ok\n",
            "(1278892880) dorothyp: but i'm having bad luck. is it possible to install a package and then reinstall the whole system?\n",
            "(1278892880) dr_willis:  if its not possible...then its not possible.\n",
            "\n",
            "\n",
            "(1115682440) grawity: hey all, is there a way to get the latest firmware for the usb mouse ?\n",
            "(1115682500) ActionParsnip1: http://tinyurl.com/6af7xg\n",
            "(1115682500) grawity: is on the forums\n",
            "(1115682000) ActionParsnip1: i dunno, i just do not like having a mouse all day long on these servers\n",
            "(1115682060) grawity: does it matter if I install this firmware then plug it in, or not ?\n",
            "(1115682100) ActionParsnip1: http://tinyurl.com/6af8\n",
            "======================================== SAMPLE 3 ========================================\n",
            "(1123136500) jrib: not a fan\n",
            "(1123136500) jrib: just want to know, I just purchased a new PC and it's going to be a very fun new computer\n",
            "(1123136500) jrib: it has 4 hours of RAM on it's motherboard, that's what the motherboard is used for\n",
            "(1123136560) jrib: yeah its going to be a fun new computer for sure, the only problem is, the pc comes with no real drivers and no built in drivers you can find to make it run on it, not even if you install all the drivers that the manufacturer made for it\n",
            "(1123136620) jrib: you should get something that would let you boot off of it and see if anyone can work with it instead\n",
            "(1123136680) jrib: no, but it should be able to\n",
            "\n",
            "\n",
            "(1099769200) Danko-H: !info irc\n",
            "(1099769200) Danko-H: !info irc\n",
            "(1099769200) Danko-H: !info irc\n",
            "(1099769260) Danko-H: !info irc\n",
            "(1099769260) Danko-H: !info irc\n",
            "(1099769320) Danko-H: !info irc\n",
            "(1099769320) Danko-H: !info irc\n",
            "(1099769320) Danko-H: !info irc\n",
            "(1099769320) Danko-H: oops\n",
            "(1099769320) Danko-H: !info irc\n",
            "(1099769320) Danko-H: !info irc\n",
            "(1099769380) Danko-H: !info irc\n",
            "(1099769380) Danko-H: !info irc\n",
            "(1099769380) Danko-H: !info irc\n",
            "(1099769500) Danko-H: !info irc\n",
            "(1099769500) Danko-H: !info irc\n",
            "(1099769500) Danko-H: !info irc\n",
            "(1099769500) Juk: please keep the language PG, and no one else can hear you\n",
            "\n",
            "\n",
            "(1114406560) Dr_Willis:  i can try this.. but when i do a network update on my computer.. i get only what is listed in 'Network > Preferences > Update Manager' -   network manager.  i have the update manager running and can ping to the net if i go to the Network screen for instance.. i run the update manager and do that thingy.. not sure how to fix this though...\n",
            "(1114406560) Dr_Willis:  i always use network manager as its a window manager and i always wanted to test it on my new computer.\n",
            "(1114406680) Dr_Willis:  thats not good.. i have no clue then\n",
            "(1114406680) Dr_Willis:  and i did  a fresh install of the live cd.\n",
            "(1114406680) Dr_Willis:  and i have a little ppt in my laptop.\n",
            "(1114406680) Dr_Willis: a live cd is not for linux only\n",
            "(1114406900) Dr_Willis:  it may be possible to have you type the names of the users and stuff when it happens...\n",
            "(1114406960) Dr_Willis:  i will also show you a list some of my friends have used it to manage their computers.. they like to put a small irc to /usr/bin/ and also put a .bin file on a partition for it.. or  install it and then use apt-get install install it\n",
            "(1114406960) Dr_Willis:  in short.  :)\n",
            "(1114406960) Dr_Willis:  it is hard to say.  theres usually only 1 person using it.\n",
            "(1114406960) Dr_Willis:  so its harder than many of us use it.\n",
            "(1114406140) Dr_Willis:  i have gone here as well.\n",
            "(1114406140) Dr_Willis:  i am afraid i do not have to use those terms for linux and i will not be helping you with your questions.\n",
            "(1114406140) Dr_Willis:  i have only ever used 'network explorer'  as it sounds alot better.. theres a lot of people that i cant seem to connect to on the way..\n",
            "(1114406200) Dr_Willis:  i\n",
            "======================================== SAMPLE 4 ========================================\n",
            "Sudan, your problem is that you are running a kernel with a lot of ram\n",
            "(1296313780) Sudan: when you are booting, the hard disk has a little green button. This will bring up a menu that you can select options to choose the partition or you can choose how many hard drives you want to put between the disk and the disk\n",
            "(1296313780) Sudan: or you could go in the terminal and type 'mount' and type 'lvm' and press enter then it will say it can do it\n",
            "(1296313780) Sudan: the thing about it is you are only allowed to have 1 hd and you will only be able to boot two windows\n",
            "(1296313780) Sudan: it will always be a partition that you can put between the disk and the disk\n",
            "(1296313920) Sudan: how many partitions will it be. if you want to do so, you need to tell it to use the other one\n",
            "\n",
            "\n",
            "(1257182760) dsusnip: hello\n",
            "(1257182760) dsusnip: i'm trying to install a new version of firefox. i want to install it on both the 64bit amd64 versions. any solutions please?\n",
            "(1257182820) dsusnip: hello\n",
            "(1257182820) dsusnip: i'd like to install ubuntu on both the 64bit and the 64bit versions\n",
            "(1257182880) dsusnip: how do i find the binary's\n",
            "(1257182880) dsusnip: but it will go thru an installation\n",
            "(1257182880) D-nix:  make sure you have all of the required stuff installed.  if you have it installed, you'll just have to find a suitable shell. I recommend that you try and find the binary itself\n",
            "(1257182880) dsusnip: yeah i get it now\n",
            "(1257182880) dsusnip: what is wrong with that?\n",
            "(1257182880) dsusnip: i already had the 64bit version of firefox\n",
            "(1257182880) D-nix:  make sure you have all of the required stuff installed.  if you have it installed, you'll just have to find a suitable shell.\n",
            "(1257182940) D-nix:  it's still the same problem.  you'll see.\n",
            "\n",
            "\n",
            "(114695540) fyrestrtr: hi, I'm looking for help in creating a new partition with Ubuntu.\n",
            "(114695600) fyrestrtr: yes.\n",
            "(114695660) fyrestrtr: just install windows with ubuntu and create your partition\n",
            "(114695660) fyrestrtr: i am using ubuntu right now\n",
            "(114695660) fyrestrtr: I am having some problems with the partition creation\n",
            "(114695660) fyrestrtr: because the partition was created before installing windows and it is now on it\n",
            "(114695660) fyrestrtr: i have installed ubuntu to another system so I did not use windows.\n",
            "(114695660) fyrestrtr: so you have no idea how to solve it?\n",
            "(114695660) fyrestrtr: it's not on it?\n",
            "(114695660) fyrestrtr: yes.\n",
            "(114695660) fyrestrtr: well yeah!\n",
            "(114695660) fyrestrtr: so I have no clue how to solve it\n",
            "(114695660) fyrestrtr: but I do have that I think.\n",
            "(114695660) fyrestrtr: so I think he is lying.\n",
            "(114695660) fyrestrtr: yes or no?\n",
            "(114695660) fyrestrtr: yes or no?\n",
            "(114695660) fyrestrtr: yeah!\n",
            "(114695660) fyrestrtr: nope.\n",
            "(114695660) fyrestrtr: he is lying.\n",
            "(114695660) fyrestrtr: and you, nigguz\n",
            "(114695660) FYrestrtr: I haven't installed ubuntu yet.\n",
            "(114695660) FYrestrtr: but if you find anything useful in that file\n",
            "(114695660) FYrestrtr: yeah, but I'm just asking.\n",
            "(114695660) fyrestrtr: I\n",
            "======================================== SAMPLE 5 ========================================\n",
            "SORRY I JUST FOUND NEWLY INSTALLED GIMP!!!\n",
            "(1279159380) ajm: i'm afraid I don't know what you are talking about\n",
            "(1279159440) ajm: you didn't see those words 'newly installed'?\n",
            "(1279159500) ajm: how do you get a word to be installed by command?\n",
            "(1279159500) ajm: ok!\n",
            "(1279159560) ajm: well you can use sudo and change the /etc/init.d/gdm to /etc/init.d/gdm, then then you have to do the job\n",
            "(1279159560) ajm: you did 'sudo dpkg-reconfigure kdm' but it will fix it\n",
            "(1279159560) ajm: then you need the right commands for it\n",
            "(1279159620) ajm: what? 'sudo dpkg-reconfigure kdm'?\n",
            "(1279159680) ajm: you have to do it manually\n",
            "(1279159680) ajm: you need to find the right command, and it will give you the right commands to do it, then you can just use one of the commands you got from the command line\n",
            "(1279159680) ajm: if you want one of the commands, they are all in /etc/init.d/gdm\n",
            "(1279159680) ajm: the one you need is 'sudo dpkg-reconfigure kdm'\n",
            "(1279159680) ajm: then when you run 'sudo dpkg-reconfigure kdm' you get the name of your new command\n",
            "(1279159680) ajm: and for those commands, you are going to need the 'sudo' command\n",
            "(129476660) ajm: what do you want to install anyway?\n",
            "(129476660) ajm: I mean 'sudo apt-cache search vlc' or something\n",
            "(129476660) ajm: 'sudo apt-cache search mp3' or something\n",
            "(129476660) ajm: are you using an iso?\n",
            "(129476660) ajm: like the one you can install\n",
            "(129476720) ajm: you don't have to install anything else\n",
            "(129533600) ajm: not sure what you're asking, but yes it is possible\n",
            "(129533600) ajm: if you dont have gstreamer and all of the compiz stuff installed.\n",
            "(129533600) ajm: you might have to install it\n",
            "\n",
            "\n",
            "(1307839480) wenderer: yeah, its weird\n",
            "(1307839480) wenderer: but when i run compiz it all i get at all is black\n",
            "(1307839480) wenderer: i get all the options in compiz in the preferences\n",
            "(1307839540) wenderer: yea, I know its just that it's hard to explain in gaim\n",
            "(1307839460) wenderer: yeah it is, that's why I was wondering about how it works when I first installed it\n",
            "(1307839540) wenderer: yea i do agree it is kinda weird, but this is a newbie and still learning\n",
            "(1307839460) wenderer: yeah, and a lot of people said that all it is for is gaming\n",
            "(1307839460) wenderer: so, i know it works good for me\n",
            "(1307839460) wenderer: but i still dont know how to play games on it, or how to play games on it\n",
            "(1307839520) wenderer: ya i think it's weird\n",
            "(1307839520) wenderer: hehe\n",
            "(1307839460) wenderer: yeah\n",
            "(1307839520) wenderer: im just an idiot\n",
            "(1307839460) wenderer: I wish I had some idea...\n",
            "(1307839460) wenderer: but i don't believe its a matter of being a newb\n",
            "(1307839580) wenderer: its strange\n",
            "(1307839580) wenderer: i feel like it takes a lot of effort to fix myself to get into this.\n",
            "(1307839580) wenderer: it's only a matter of time, it took me like 15 months to get into windows and i'm just here to try to figure out how to solve my problem, and i didn't even finish the first part of that one\n",
            "(1307839640\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D6k_meRXVMs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eafceefe-1cf7-4380-d791-578402fde133"
      },
      "source": [
        "!python3 src/interactive_conditional_samples.py --model_name='345M'  --nsamples=2 --top_k=100 --temperature=1"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:52: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-10-13 04:10:26.173469: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-10-13 04:10:26.196431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:10:26.197379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-10-13 04:10:26.197806: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-13 04:10:26.199571: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-10-13 04:10:26.200942: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-10-13 04:10:26.201342: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-10-13 04:10:26.203163: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-10-13 04:10:26.204501: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-10-13 04:10:26.208487: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-10-13 04:10:26.208645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:10:26.209454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:10:26.210235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-10-13 04:10:26.216440: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-10-13 04:10:26.216736: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x22e5640 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-10-13 04:10:26.216778: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-10-13 04:10:26.275862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:10:26.277455: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7cdd880 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-10-13 04:10:26.278148: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-10-13 04:10:26.278469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:10:26.279577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-10-13 04:10:26.279684: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-13 04:10:26.279746: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-10-13 04:10:26.279813: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-10-13 04:10:26.279869: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-10-13 04:10:26.279916: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-10-13 04:10:26.279961: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-10-13 04:10:26.280007: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-10-13 04:10:26.280143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:10:26.280902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:10:26.281646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-10-13 04:10:26.281729: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-13 04:10:26.283668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-10-13 04:10:26.283705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-10-13 04:10:26.283723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-10-13 04:10:26.283900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:10:26.284757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:10:26.285558: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-10-13 04:10:26.285621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:53: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:55: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:63: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Model prompt >>> (137378602389) Ioannis Agathocleous: hi\n",
            "2019-10-13 04:11:30.978075: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "======================================== SAMPLE 1 ========================================\n",
            "\n",
            "(1373786020) Ioannis Agathocleous: can someone help me?\n",
            "(1373786020) Ioannis Agathocleous: i need help\n",
            "(1373786020) Ioannis Agathocleous: ok\n",
            "(1373786080) Ioannis Agathocleous: you tell me: how i could get ubuntu in my laptop\n",
            "(1373786080) Ioannis Agathocleous: do u know how\n",
            "(1373786080) Ioannis Agathocleous: maybe please\n",
            "(1373786080) Ioannis Agathocleous: i need help\n",
            "(1373786080) Ioannis Agathocleous: i think that my computer is no use\n",
            "(1373786080) Ioannis Agathocleous: thanks\n",
            "(1373786140) Ioannis Agathocleous: i still want to try with you\n",
            "(1373786140) Ioannis Agathocleous: you can tell me the details if you want\n",
            "(1373786260) Ioannis Agathocleous: help will you?\n",
            "(1373786260) Ioannis Agathocleous: thanks\n",
            "(1373786320) Ioannis Agathocleous: ok\n",
            "(1373787360) Ioannis Agathocleous: i do not know how\n",
            "(1373787360) Ioannis Agathocleous: are you at the ubuntu website\n",
            "\n",
            "\n",
            "(1225576040) kosh: hey, i created my new partition ext3 in windows, i created an empty one in ubuntu, what do i need from windows?\n",
            "(1225576160) kosh: omg i installed it in windows, now every time i boot, i see everything but windows, and no windows anymore, why?\n",
            "(1225576160) taniwn: then you need to select your windows partition in BIOS, boot in windows, and use sudo.\n",
            "(1225576220) luis: why exactly the linux partition if i only have ubuntu on the first to the last slot of my ubuntu server, when i select my windows one, all contents of windows are listed, when i boot, ubuntu appears with everything back to normal\n",
            "(1225566440) taniwn: if you just added the windows part, you've done the right thing. If you keep\n",
            "======================================== SAMPLE 2 ========================================\n",
            ", is there a chance to convert a docetical ISO into a usb drive ?\n",
            "(1373786020) Ioannis Agathocleous: i know that there is no USB program\n",
            "(1373786080) Ioannis Agathocleous: what do i need to download from ubuntu\n",
            "(1373786080) ompaul: unzip -> d\n",
            "(1373786140) ompaul: do not paste... http://paste.ubuntu-nl.org/2344\n",
            "(1373786200) ompaul: read that man.\n",
            "(1373786200) ompaul: read that man\n",
            "(1373786560) ompaul: no, no\n",
            "(1373786560) ompaul: you can't\n",
            "(1373786560) ompaul: no, you don't\n",
            "(1373786620) ompaul: if u don't install linux through desktop then you don't have to install anything else\n",
            "(1373786680) ompaul: it is impossible .. i am talking about ubuntu boot\n",
            "(1373786680) ompaul: you can try the latest ubuntu\n",
            "(1373786960) ompaul: they do it\n",
            "(1373786960) ompaul: but i am saying no\n",
            "(1373787160) ompaul: the first thing is to use it's name in here\n",
            "(1373787160) ompaul: read that man.\n",
            "(1373822580) ompaul: there is no way to copy iso /\n",
            "(1373822700) ompaul: when you install ubuntu you can't run gnome , you get the gnome options for it from the system menu\n",
            "(1373822700) ompaul: there is not only one way\n",
            "(1373861220) ompaul: gparted has it's options, i didn't mention it there, sorry, i was just teasing ;)\n",
            "(1373861280) ompaul: install it as root if u dont know and use it as a tool to copy the iso\n",
            "(1373861280) ompaul: read that man\n",
            "(1373861340) ompaul: it only has umount\n",
            "(1373861340) ompaul: see http://paste.ubuntu-nl.org/233\n",
            "(1373861500) Ioannis Agathocle\n",
            "================================================================================\n",
            "Model prompt >>> (1349728992965) Svilen Todorov: lol if its fine you wont be able to be a middle aged man without pay\n",
            "======================================== SAMPLE 1 ========================================\n",
            " to take care of that stupid thing he is making a joke about\n",
            "(13497290040) Svilen Todorov: so the option 'noobishness' is a good one to always use\n",
            "(13497290040) Svilen Todorov: or ya can never use it then\n",
            "(13497290040) Svilen Todorov: !gah | sf\n",
            "(13497290160) D_Wolf:  I don't ever use the 'noobishness' option.\n",
            "\n",
            "\n",
            "(1156465300) daniels: Hi! i have problems with my ubuntu. I only have 2 HDs. It takes a long time to load 1g or 2g ram for cd and .exe\n",
            "(1156465360) daniels: Hello.\n",
            "(1156465360) daniels: Hi.\n",
            "(1156455180) daniels: I will try with a program. can you help me ?\n",
            "(1156455240) daniels: hi I can you help me?\n",
            "(1156455240) daniels: is there a way to make ubuntu as thin as you can :P\n",
            "\n",
            "\n",
            "(1202484820) amarok: which version of linux u stick i run a windows\n",
            "(1202484880) amarok:  should i take lubuntu\n",
            "(1202484880) amarok: or debian or linux ?\n",
            "(1202484880) amarok: well, i want to check it is right to use debian\n",
            "(1202484940) Amaranth: that's what I've done..\n",
            "(1202484940) amaranth: if you're unsure you really can check with your manufacturer ..\n",
            "(1202484940) amaranth: do you think there are better linux distros to do support\n",
            "(1202485000) amaranth: nope I know what ubuntu is.. (and it's good..!)\n",
            "\n",
            "\n",
            "(1101552560) rj80lk: !ask | stuart_\n",
            "(1101552560) stuart_: Is there another way to ask a channel?\n",
            "(1101552620) rj80lk: the channel bot is the official channel.\n",
            "(1101552620) wols_: Is there another channel for Debian/Ubuntu?\n",
            "(11\n",
            "======================================== SAMPLE 2 ========================================\n",
            " attention\n",
            "\n",
            "\n",
            "(1162412300) s0o: is there any idea what can I do in ubuntu that  they can never do in windows with their  'x' on keyboard layout?\n",
            "(1162412300) s0o: there are more than 30000 apps like that that can do some kinda\n",
            "(1162412680) s0o: im trying to get a 'more 'n more' program for my keyboard for what i want to do with my ubuntu system\n",
            "(1162412680) [charity]: use the same system\n",
            "(1162412680) [charity]: and go here\n",
            "(1162412740) [charity]: I use peter: not sure how to do this, but I think vim or gvim is a fairly simple program\n",
            "(1162412860) [charity]: maybe you use something like vim or something in ubuntu, I use vim like vim in windows, I'd probably recommend vim\n",
            "(1162412920) [charity]: so be bold, are you doing anything else?\n",
            "(1162412920) [charity]: you might need to install emacs or nano or something\n",
            "(1162412980) [charity]: but that would keep you from doin it in ubuntu :)\n",
            "(1162412980) [charity]: you could try to compile in g++\n",
            "(1162412980) [charity]: or maybe you're just a little dumb :|\n",
            "\n",
            "\n",
            "(1252927300) dikus: what ubuntu can do ? :)\n",
            "(1252927360) dikus: ok thump thump thump dikus\n",
            "(1252927480) dikus: thump thump thump\n",
            "(1252927480) dikus: i want my new Ubuntu to show up in my system\n",
            "(1252927480) dikus: thump thump\n",
            "(1252927480) dikus: dikus: wtf\n",
            "(1252927600) dikus: ok thump thump\n",
            "(1252927600) dikus: thump thump...\n",
            "(1252927560) dikus: so, where can download the drivers for my laptop then ?\n",
            "(1252927560) dikus: the problem ?\n",
            "(1252927560) dikus: what programs can do that the best computers can\n",
            "\n",
            "================================================================================\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 5480, in get_controller\n",
            "    yield g\n",
            "  File \"src/interactive_conditional_samples.py\", line 68, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"src/interactive_conditional_samples.py\", line 86, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 138, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 471, in _Fire\n",
            "    target=component.__name__)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 675, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"src/interactive_conditional_samples.py\", line 83, in interact_model\n",
            "    print(\"=\" * 80)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1633, in __exit__\n",
            "    close_thread.start()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 851, in start\n",
            "    self._started.wait()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 551, in wait\n",
            "    signaled = self._cond.wait(timeout)\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 295, in wait\n",
            "    waiter.acquire()\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D518LBF8XVCQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bf919c62-d3e2-48d1-bda6-8658bb6af3fc"
      },
      "source": [
        "!python3 src/interactive_conditional_samples.py --model_name='345M'  --nsamples=2 --top_k=100 --temperature=1"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:52: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-10-13 04:16:02.144586: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-10-13 04:16:02.164689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:16:02.165484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-10-13 04:16:02.165823: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-13 04:16:02.167246: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-10-13 04:16:02.168637: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-10-13 04:16:02.168993: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-10-13 04:16:02.170722: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-10-13 04:16:02.172025: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-10-13 04:16:02.175765: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-10-13 04:16:02.175957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:16:02.176829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:16:02.177528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-10-13 04:16:02.183182: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-10-13 04:16:02.183498: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2a1b640 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-10-13 04:16:02.183535: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-10-13 04:16:02.251213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:16:02.252096: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x8413880 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-10-13 04:16:02.252134: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-10-13 04:16:02.252387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:16:02.253089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-10-13 04:16:02.253181: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-13 04:16:02.253232: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-10-13 04:16:02.253280: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-10-13 04:16:02.253392: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-10-13 04:16:02.253441: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-10-13 04:16:02.253484: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-10-13 04:16:02.253528: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-10-13 04:16:02.253656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:16:02.254461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:16:02.255136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-10-13 04:16:02.255224: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-13 04:16:02.256779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-10-13 04:16:02.256816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-10-13 04:16:02.256834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-10-13 04:16:02.257007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:16:02.257925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:16:02.258637: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-10-13 04:16:02.258697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:53: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:55: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:63: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Model prompt >>> Theodore McCarrick is the most senior Catholic figure to be dismissed from the priesthood in modern times. US Church officials said allegations he had sexually assaulted a teenager five decades ago were credible. Mr McCarrick, 88, had previously resigned but said he had \"no recollection\" of the alleged abuse. \"No bishop, no matter how influential, is above the law of the Church,\" Cardinal Daniel DiNardo, president of the United States Conference of Catholic Bishops said in a statement. \"For all those McCarrick abused, I pray this judgment will be one small step, among many, toward healing.\" The alleged abuses may have taken place too long ago for criminal charges to be filed because of the statute of limitations. Mr McCarrick was the archbishop of Washington DC from 2001 to 2006. Since his resignation last year from the College of Cardinals, he has been living in seclusion in a monastery in Kansas. He was the first person to resign as a cardinal since 1927. He is among hundreds of members of the clergy accused of sexually abusing children over several decades and his dismissal comes days before the Vatican hosts a summit on preventing child abuse. The Vatican said Pope Francis had ruled Mr McCarrick's expulsion from the clergy as definitive, and would not allow any further appeals against the decision. \n",
            "2019-10-13 04:17:01.197274: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "======================================== SAMPLE 1 ========================================\n",
            " Archdiocese of Massachusetts said it did not know the specifics of the abuse and was in the process of sending its own lawyers to Mr McCarrick when the allegation arose. Archdiocese of Massachusetts said it has had no contacts with any other Catholic diocese. The Archdiocese of Massachusetts spokesman said it might refer it to other jurisdictions if necessary. <wish1->\n",
            "\n",
            "\n",
            "(1312256540) Zomg_: what do you mean by 'hard disk' is more than software and hardware?\n",
            "(1312256600) Zomg_: hardware disk, software disk, or hardware cache cache ?\n",
            "(1312256600) Zomg_: hardware cache, software cache??\n",
            "(1312256660) Zomg_: ssi, ram\n",
            "(1312256660) Zomg_: ssi, I can't say I use a lot of software\n",
            "(1312256660) Zomg_: I can say I use a lot of software\n",
            "(1312256660) Zomg_: as in my use I love it\n",
            "(1312256660) Zomg_: I love the whole partition\n",
            "(1312256660) Zomg_: is my machine as big as a small screen?\n",
            "(1312256720) Zomg_: 'no, I don't use my machines like one; I use lots in a day, and you guys have found them for a while to be that much more annoying', I want to know it's possible, I guess\n",
            "\n",
            "\n",
            "(1172684740) mabar: sudo hdparm -z is the right way to get security updates.\n",
            "(1172684740) mabar: (and it does, I've checked it.)\n",
            "(1172684740) mabar: (remember --disable-common is just to install that app for the common users, not the superusers).\n",
            "(1172684740) mabar: (the --disable-common is just really handy.)\n",
            "(1172684740) phefferra: it's the app you want to install\n",
            "(1172685020) mabar: you're right. I'm just curious as to why people are asking it\n",
            "(1172685020) mabar: is there any particular system they'd be searching for?\n",
            "(1172685020) m\n",
            "======================================== SAMPLE 2 ========================================\n",
            " Catholic Church leaders said claims that Mr McCarrick had perpetrated the same abuses at the behest of his archbishop of DC were believed to be false. US Church officials said allegations Mr McCarrick had sexually assaulted a teenager five decades ago were credible. US Church officials said allegations Mr McCarrick had sexually abused a teenager five decades ago were believed to be baseless. US Church officials said this was not true. Mr McCarrick was the archbishop of Washington DC from 2001 to 2006. Since his resignation last year from the College of Cardinals, he has been living in seclusion in a monastery in Kansas. He was the first person to resign as a cardinal since 1927. He is among hundreds of members of the clergy accused of sexually abusing children over several decades and his dismissal comes days before the Vatican hosts a summit on preventing child abuse. The Vatican said Pope Francis had ruled Mr McCarrick's expulsion from the clergy as definitive, and would not allow any further appeals against the decision. Father McCarrick was the archbishop of Washington DC from 2001 to 2006. Since his resignation last year from the College of Cardinals, he has been living in seclusion in a monastery in Kansas. He was the first person to resign as a cardinal since 1927. He is among hundreds of members of the clergy accused of sexually abusing children over several decades and his dismissal comes days before the Vatican hosts a summit on preventing child abuse. The Vatican said Pope Francis had ruled Mr McCarrick's expulsion from the clergy as definite, and would not allow any further appeals against the decision. US Church officials said accusations Mr McCarrick had perpetrated the same abuses at the behest of his archbishop of DC were believed to be false. US Church officials said this was not true. Pope Francis has ruled that the Church rejects Cardinal Daniel DiNardo as a priest but it works in the right of the Church\n",
            "(1130463640) AO1: im saying...that he is responsible for the mistakes and abuse\n",
            "(1130463700) AO1: he got you\n",
            "(1130463700) AO1: i remember...\n",
            "(1130463760) AO1: hey\n",
            "(1130463820) xajah: you mean the current administration of it, or are you suggesting that it is not the correct practice?\n",
            "(1130463880) AO1: are you being serious in #ubuntu-offtopic? or just asking about nics please.\n",
            "(113046\n",
            "================================================================================\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 5480, in get_controller\n",
            "    yield g\n",
            "  File \"src/interactive_conditional_samples.py\", line 68, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"src/interactive_conditional_samples.py\", line 86, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 138, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 471, in _Fire\n",
            "    target=component.__name__)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 675, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"src/interactive_conditional_samples.py\", line 83, in interact_model\n",
            "    print(\"=\" * 80)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1633, in __exit__\n",
            "    close_thread.start()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 851, in start\n",
            "    self._started.wait()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 551, in wait\n",
            "    signaled = self._cond.wait(timeout)\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 295, in wait\n",
            "    waiter.acquire()\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcEmb7HLXU3g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6c7e8b6a-a213-43d6-8fc0-6bb70fd57a58"
      },
      "source": [
        "!python3 src/interactive_conditional_samples.py  --model_name='345M'  --nsamples=10 --top_k=40 --temperature=.80 --length=1"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:52: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-10-13 04:17:59.439217: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-10-13 04:17:59.459852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:17:59.460706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-10-13 04:17:59.461149: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-13 04:17:59.463123: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-10-13 04:17:59.464950: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-10-13 04:17:59.465517: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-10-13 04:17:59.467778: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-10-13 04:17:59.469254: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-10-13 04:17:59.473662: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-10-13 04:17:59.473967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:17:59.474908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:17:59.475705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-10-13 04:17:59.483111: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-10-13 04:17:59.483665: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x19ab640 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-10-13 04:17:59.483722: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-10-13 04:17:59.536165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:17:59.537049: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x73a3880 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-10-13 04:17:59.537094: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-10-13 04:17:59.537353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:17:59.538174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-10-13 04:17:59.538315: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-13 04:17:59.538395: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-10-13 04:17:59.538449: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-10-13 04:17:59.538529: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-10-13 04:17:59.538577: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-10-13 04:17:59.538626: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-10-13 04:17:59.538694: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-10-13 04:17:59.538872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:17:59.539725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:17:59.540490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-10-13 04:17:59.540597: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-13 04:17:59.542356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-10-13 04:17:59.542393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-10-13 04:17:59.542412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-10-13 04:17:59.542650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:17:59.543527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:17:59.544312: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-10-13 04:17:59.544374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:53: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:55: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:63: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Model prompt >>> The 2008 Summer Olympics torch relay was run from March 24 until August 8, 2008, prior to the 2008 Summer Olympics, with the theme of “one world, one dream”. Plans for the relay were announced on April 26, 2007, in Beijing, China. The relay, also called by the organizers as the “Journey of Harmony”, lasted 129 days and carried the torch 137,000 km (85,000 mi) – the longest distance of any Olympic torch relay since the tradition was started ahead of the 1936 Summer Olympics. After being lit at the birthplace of the Olympic Games in Olympia, Greece on March 24, the torch traveled to the Panathinaiko Stadium in Athens, and then to Beijing, arriving on March 31. From Beijing, the torch was following a route passing through six continents. The torch has visited cities along the Silk Road, symbolizing ancient links between China and the rest of the world. The relay also included an ascent with the flame to the top of Mount Everest on the border of Nepal and Tibet, China from the Chinese side, which was closed specially for the event. Q: What was the length of the race? A: 137,000 km Q: Was it larger than previous ones? A: No Q: Where did the race begin? A: Olympia, Greece Q: Where did they go after? A: Athens Q: How many days was the race? A: seven Q: Did they visit any notable landmarks? A: Panathinaiko Stadium Q: And did they climb any mountains?\n",
            "2019-10-13 04:18:32.136603: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "======================================== SAMPLE 1 ========================================\n",
            " A\n",
            "======================================== SAMPLE 2 ========================================\n",
            " A\n",
            "======================================== SAMPLE 3 ========================================\n",
            " A\n",
            "======================================== SAMPLE 4 ========================================\n",
            "\n",
            "\n",
            "======================================== SAMPLE 5 ========================================\n",
            " A\n",
            "======================================== SAMPLE 6 ========================================\n",
            " A\n",
            "======================================== SAMPLE 7 ========================================\n",
            " A\n",
            "======================================== SAMPLE 8 ========================================\n",
            " A\n",
            "======================================== SAMPLE 9 ========================================\n",
            " A\n",
            "======================================== SAMPLE 10 ========================================\n",
            " A\n",
            "================================================================================\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 5287, in get_controller\n",
            "    yield default\n",
            "  File \"src/interactive_conditional_samples.py\", line 68, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"src/interactive_conditional_samples.py\", line 86, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 138, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 471, in _Fire\n",
            "    target=component.__name__)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 675, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"src/interactive_conditional_samples.py\", line 83, in interact_model\n",
            "    print(\"=\" * 80)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1607, in __exit__\n",
            "    exec_tb)\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 5296, in get_controller\n",
            "    self.stack.pop()\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AJV9ONUTQGF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4fb9c4be-bc26-4a29-ff34-75cc277f695e"
      },
      "source": [
        "#Translation\n",
        "!python3 src/interactive_conditional_samples.py --model_name='345M'  --nsamples=3 --temperature=1"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:52: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-10-13 04:20:18.008951: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-10-13 04:20:18.029017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:20:18.029763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-10-13 04:20:18.030095: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-13 04:20:18.031616: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-10-13 04:20:18.033329: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-10-13 04:20:18.033814: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-10-13 04:20:18.036015: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-10-13 04:20:18.037853: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-10-13 04:20:18.042673: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-10-13 04:20:18.042873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:20:18.043836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:20:18.044606: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-10-13 04:20:18.050647: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-10-13 04:20:18.051001: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2b59640 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-10-13 04:20:18.051039: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-10-13 04:20:18.110700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:20:18.111572: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x8553880 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-10-13 04:20:18.111607: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-10-13 04:20:18.111849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:20:18.112551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-10-13 04:20:18.112650: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-13 04:20:18.112710: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-10-13 04:20:18.112766: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-10-13 04:20:18.112822: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-10-13 04:20:18.112873: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-10-13 04:20:18.112921: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-10-13 04:20:18.112980: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-10-13 04:20:18.113127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:20:18.113942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:20:18.114635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-10-13 04:20:18.114723: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-10-13 04:20:18.116343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-10-13 04:20:18.116381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-10-13 04:20:18.116399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-10-13 04:20:18.116581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:20:18.117376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-10-13 04:20:18.118068: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-10-13 04:20:18.118126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:53: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:55: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:63: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Model prompt >>> Good morning. = Buenos días. I am lost. Where is the restroom? = Estoy perdido. ¿Dónde está el baño? How much does it cost? = ¿Cuánto cuesta? How do you say maybe in Spanish? = ¿Cómo se dice maybe en Español? Would you speak slower, please. = Por favor, habla mas despacio. Where is the book store? = ¿Dónde está la librería? At last a feminist comedian who makes jokes about men. = Por fin un cómico feminista que hace chistes sobre hombres.   How old are you? =   \n",
            "2019-10-13 04:20:36.269683: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "======================================== SAMPLE 1 ========================================\n",
            " Be honest.\n",
            "(1149704940) ku_: you're asking me a whole other first\n",
            "(1149705060) rabbit: hi men.. = Okay, dude\n",
            "(1149705060) ku_: ywz    and you can do that in every partition..to recursivelly.. again, be honest with me if you don't :D\n",
            "(1149705060) rabbit:  necesidad espaol. into my partition\n",
            "(1149410520) rabbit: nani más safar\n",
            "(1149410520) ku_: http://db.fastoccasion.com/db/events/2016/December/09597939/Post-21437-mns-sets-record-shrink.aspx#postmedia_wrappers\n",
            "(1149410580) ku_: happenbot.com man\n",
            "(1149410580) ku_: also why  thanonly davaint_tests.\n",
            "(1149410580) rabbit: i have received my coins again\n",
            "(1149410580) ku_: must be connected for sudo to see your MXs\n",
            "\n",
            "\n",
            "(1199695200) quaq|NY: -> more basic!\n",
            "(1199695 200) quaq|NY: tuxista :p\n",
            "(1199695200) quaq|NY: brain\n",
            "(1199695200) bait: rip\n",
            "(1199695200) quaq|NY: if it all goes with AMP how come it only does it with ubuntu\n",
            "(1199695200) quaq|NY: Getta\n",
            "(1199695200) bait: turquoise adds shit\n",
            "(1199695200) bait: theres different windows version of ubuntu with different compression rings\n",
            "(1199695200) oy1: and theres only mone xmms\n",
            "(1199695200) oy1: i hope not\n",
            "(1199695200) oy1: not really\n",
            "(11996986060) oy1: let me install a pcmplt\n",
            "(11996986060) oy1: nvm\n",
            "(11996986060) oy1: let me put it\n",
            "(1199697120) oy1: M4 or Moz, please.\n",
            "(1199697120) oy1: what sound card ?\n",
            "(1199697180\n",
            "======================================== SAMPLE 2 ========================================\n",
            " Pronto oño several Els, o fazer y su Hombres\n",
            "\n",
            "(119160240) simpaper: pluv seta pluv seta\n",
            "(119160240) simpaper: just gogogada  because it'll be easier\n",
            "(119160240) simpaper: 31 pushes\n",
            "(119160240) simpaper: multi-lingual\n",
            "(119160240) simpaper: more or less\n",
            "(119160240) simpaper: a lot of idiots in here active\n",
            "(119160240) simpaper: should take you a bit of time\n",
            "(119160240) simpaper: ./ .. som there but I just get you about a week for future reference\n",
            "(119160240) simpaper: hi guys --> whats about volum\n",
            "(119160240) simpaper: what do you mean\n",
            "(119160360) simpaper: or Automatic Installer\n",
            "(119160360) simpaper: why its called that?\n",
            "(119160360) smu: the installer uses a .deb package rather than your regular one....the .re settings are saved underground\n",
            "\n",
            "\n",
            "(1206513660) jamesg: well it has only v, is it?\n",
            "(1206513720) lstarnes: - xenix should look to inheritance\n",
            "(1206513720) lstarnes: it should have an  (S)SYSTEM 251poll\n",
            "(1203106000) lstarnes: - the shell is fine\n",
            "(1203107060) lstarnes: - you might check the Loading Script Directories\n",
            "(1203107100) lstarnes: - try to find it it is located lately\n",
            "(120311780) lstarnes: - act of normalization\n",
            "(120312720) lstarnes: - you might add a trailing '&' to all of your entries too\n",
            "(120312840) lstarnes: - and, :P\n",
            "(120312900) lstarnes: - keep on updating\n",
            "(120312960) lstarnes: - not every day :-)\n",
            "(128321760) lstarnes: - I have no reason to fetch md5 files and I cant even say to use non free - system does not include any software to d/l multiple disks\n",
            "(128321760\n",
            "======================================== SAMPLE 3 ========================================\n",
            " ¿Cómion? I dont like you as a man. = ¿Ella? Please stop injecting me in. = : but man, yes your not ideal. =  Women say something less painful about [you]. = Hello. = And is there a way to search your computer? = Hobgang: worse than the box drive ;-)\n",
            "(1134904840) Hobgang: who has the stuff, jipe?\n",
            "(1134904960) Hobgang: when you say, 'who has the stuff', what have you meant?\n",
            "(1134904960) Hobgang: i dunno, ever heard of it before?\n",
            "(1134905020) Hobgang: random, flash?? flash??\n",
            "(1134905020) Hobgang: oh, I'm curious, ah ok\n",
            "(1134905020) Hobgang: is there a way to search the box drive cache of the firmware device, like the flash??\n",
            "(1134905020) Hobgang: how do you load it? or fix that dns apache-firefox and firefox-name?\n",
            "(1134905020) Hobgang: 'sudo dns-server and type do-root dns-adapter'.\n",
            "(1134905020) Hobgang: wait,  what's one more thing?\n",
            "(1134905020) Hobgang: i can debug, and i can debug the boot ati dd.i hope you seen it happened\n",
            "(1134905760) Hobgang: i mean, when you say that you have the stuff, what would that mean. = You have those, why would you need swap for caching? = ;-)\n",
            "(1134905760) Hobgang: when i get fetch, it's when you fetch that i need it\n",
            "(1134905860) Hobgang: so, jipe, you could just use apt-get every time so pack it in a chroot just to get rid of the stuff from ubuntu\n",
            "(1134956160) Hobgang: okay\n",
            "(1134956160) Hobgang: did you read from the ubuntu.com and got some doc\n",
            "(1134956160) Hobgang: huh? was that ubottu guy there? = did he read.\n",
            "(1134956160) Hobgang: momf*cking help\n",
            "\n",
            "\n",
            "(1305867160) Swanton: what if ub\n",
            "================================================================================\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 5480, in get_controller\n",
            "    yield g\n",
            "  File \"src/interactive_conditional_samples.py\", line 68, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"src/interactive_conditional_samples.py\", line 86, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 138, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 471, in _Fire\n",
            "    target=component.__name__)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 675, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"src/interactive_conditional_samples.py\", line 83, in interact_model\n",
            "    print(\"=\" * 80)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1633, in __exit__\n",
            "    close_thread.start()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 851, in start\n",
            "    self._started.wait()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 551, in wait\n",
            "    signaled = self._cond.wait(timeout)\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 295, in wait\n",
            "    waiter.acquire()\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHklZVjKTPP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMCqVC80TPI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPdTGR66TOYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TBe6AubTOSI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrMxHuLVTOJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtvGb2kkTN3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}